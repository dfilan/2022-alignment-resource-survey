Run,Program Version,User,Time Started (UTC),Time Finished (UTC),Minutes Spent,Position,Points,How involved in AI alignment are you? Please select all that apply. (6kkcmyw),Which of the following resources have you used to understand AI alignment research? (nwudv6g),resources,verbal_rating_scale,prob_rating_scale,How involved in AI alignment are you? Please select all that apply. (g4mdi50),involvement,"For how many years have you been interested in AI alignment research? Please enter an integer, rounded to the nearest year. (bu4zyb5)",category,Which of the following resources have you spent more than 30 minutes engaging with? (t0y9zda),"Overall, how useful have you found {resource}? (okdtv5m)",resource,"How likely would you be to recommend {resource} as an AI alignment resource to a friend getting into AI alignment, who hadn't already read widely in the space? (xf22i7s)",How likely would you be to recommend {resource} as an AI alignment resource to a friend who is paid to do AI alignment research? (b51q4lo),"If you'd like to go into detail about your answers about {resource}, you can do so here. (83afusr)","Please specify which researchers you're thinking of. Also, if you'd like to go into detail about your answers about {resource}, you can do so here. (bj70ff1)",Is there anything else you'd like to tell us? (bty6mmy),Where did you hear about this survey? (auqgbpq),"For how many years have you been paid to work on technical AI alignment research? Please enter an integer, rounded to the nearest year. (ub5lnfb)",fbclid
7388237,43,,2022-05-30 09:44:32,,0.93,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7388617,43,,2022-05-30 13:15:37,2022-05-30 13:19:41,4.05,t6togir,0,,,"[""Human Compatible (book)"",""The Alignment Problem (book)"",""AGI Safety Fundamentals Course"",""the AI Alignment Newsletter"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the Value Learning sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""the ML Safety newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""talks by AI alignment researchers"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""the FLI podcast"",""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",3,I spend some of my time solving technical problems related to AI alignment,"Human Compatible (book),The Alignment Problem (book),AGI Safety Fundamentals Course,the AI Alignment Newsletter,the ARCHES agenda by Andrew Critch and David Krueger,the Value Learning sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,the ML Safety newsletter,Concrete Problems in AI Safety by Amodei et al,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),talks by AI alignment researchers,Superintelligence (book),AXRP - the AI X-risk Research Podcast,the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,the FLI podcast,the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al",2 | 3 | 3 | 3 | 2 | 3 | 2 | 1 | 1 | 3 | 2 | 1 | 3 | 3 | 1 | 1 | 4 | 1,Unsolved Problems in ML Safety by Hendrycks et al,3 | 4 | 3 | 2 | 0 | 2 | 0 | 2 | 1 | 1 | 1 | 0 | 0 | 2 | 0 | 2 | 3 | 2,0 | 0 | 3 | 4 | 2 | 2 | 1 | 1 | 0 | 3 | 2 | 0 | 3 | 2 | 1 | 2 | 1 | 1, |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | ,"","","",,
7388618,43,,2022-05-30 13:15:40,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7389274,43,,2022-05-30 19:07:06,,19.85,now6hjo,0,,,"[""AI Safety Camp"",""Rob Miles videos"",""conversations with AI alignment researchers at conferences"",""AXRP - the AI X-risk Research Podcast"",""AGI Safety Fundamentals Course"",""the Iterated Amplification sequence on the Alignment Forum"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Superintelligence (book)"",""the Value Learning sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"AI Safety Camp,Rob Miles videos,conversations with AI alignment researchers at conferences,AXRP - the AI X-risk Research Podcast,AGI Safety Fundamentals Course,the Iterated Amplification sequence on the Alignment Forum,the ARCHES agenda by Andrew Critch and David Krueger,Superintelligence (book),the Value Learning sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum",0 | 1 | 4 | 1 | 0 | 2 | 0 | 2 | 1 | 1,Concrete Problems in AI Safety by Amodei et al,1 | 2 | 4 | 1 | 4 | 2 | 0 | 2 | 1 | 1,0 | 1 | 4 | 1 | 1 | 2 | 0 | 2 | 1 | 1," |  |  | I'd want to make sure professional AI alignment researchers were familiar with the content in AI safety fundamentals | On one hand I dunno if Iterated Amplification makes any sense (and not sure Paul even endorses it anymore?), on the other hand hand it seems like people should know about it because it's one of the major conversation threads. | I have a vague belief that there's stuff in Arches that is useful, but also a vague belief that much of it is written up elsewhere more readably. (It may be good for people who like reading academic papers) | Superintelligence was the first book I read. I think I was familiar with most of it. I've read it all the way through whereas I've only engaged with Arches kinda second-hand, so my higher ratings may not actually make sense. | I honestly don't remember much about it, not sure if it faded into background obvious facts that I don't think about anymore. | I think I'd include this mostly from historical importance.","",,,,
7389276,43,,2022-05-30 19:07:25,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7389296,43,,2022-05-30 19:18:20,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7389302,43,,2022-05-30 19:19:52,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7389408,43,,2022-05-30 19:58:13,2022-05-30 20:12:14,14.02,t6togir,0,,,"[""Human Compatible (book)"",""the AI Alignment Newsletter"",""Life 3.0 (book)"",""Concrete Problems in AI Safety by Amodei et al"",""The Alignment Problem (book)"",""AXRP - the AI X-risk Research Podcast"",""the annual AI Alignment Literature Review and Charity Comparison"",""Superintelligence (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""talks by AI alignment researchers"",""AI Safety Camp"",""the Embedded Agency sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""Rob Miles videos"",""conversations with AI alignment researchers at conferences"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the FLI podcast"",""AGI Safety Fundamentals Course"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",1,I am paid to work on technical AI alignment research,"Human Compatible (book),the AI Alignment Newsletter,Life 3.0 (book),Concrete Problems in AI Safety by Amodei et al,The Alignment Problem (book),AXRP - the AI X-risk Research Podcast,the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),the ARCHES agenda by Andrew Critch and David Krueger,talks by AI alignment researchers,AI Safety Camp,the Embedded Agency sequence on the Alignment Forum,the 80,000 Hours podcast,Rob Miles videos,conversations with AI alignment researchers at conferences,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the FLI podcast,AGI Safety Fundamentals Course,Unsolved Problems in ML Safety by Hendrycks et al",2 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 3 | 4 | 1 | 4 | 3 | 3 | 1 | 2 | 4 | 1,Unsolved Problems in ML Safety by Hendrycks et al,3 | 2 | 1 | 1 | 3 | 1 | 1 | 1 | 1 | 2 | 2 | 1 | 4 | 4 | 2 | 2 | 2 | 4 | 2,1 | 3 | 1 | 3 | 1 | 3 | 1 | 2 | 3 | 4 | 1 | 2 | 3 | 3 | 4 | 2 | 2 | 2 | 2,"Good intro resource, but not super helpful for experienced people | Very good way to keep up with the field, also quite good to hear about whats happening in the field if you're new | I like the concrete intro story, but I'm not sure how helpful the rest is | This is a pretty important paper, both for seeing what kind of safety work was being done around 2017, and the model of writing a paper of problems for people to solve. I think given the focus on RL and simple gridworlds the actual content isn't super useful today | I think this isa very good intro to AI safety, although it's not very x-risk-y | I like that this is a technical podcast and actually explains technical research agendas | Decent for seeing what has been happening over the psat year | I read this book in 2016 and then proceeded to not care about alignment for about 3 years. But I think it probably is pretty important thing to read (or at least know the ideas in) if you want to do alignment work | I read this as my first big intro to AI safety and thought it was very good for giving an overview of research in the field | I found this very good for getting into the field. But I think that people who haven't read widely probably won't get in, and people who are doing paid alignment reasearch are too advanced. I think AI safety camp is well suited to people between these levels |  | Very good, I think the AI safety episodes are very helpful for finding out how people got into the field | Probably the best AI safety communication/media out there |  |  | Very good course. Good for networking and also for forcing me to talk and think about things I had already read | Good to have a follow up to ""Concrete Problems"" but I would have liked more specific research directions","Very good, I think people talking about their work is a good way to find out what people believe | Obviously talking with people is super helpful. Not sure how usefull/efficient if you're not up to speed with the field.","",Lightcone Slack,1,
7389789,43,,2022-05-30 22:53:10,,0.20,t0y9zda,0,,,,,,"I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,,,,,,,,,,4,
7390267,43,,2022-05-31 03:58:52,2022-05-31 04:09:21,10.48,t6togir,0,,,"[""talks by AI alignment researchers"",""Human Compatible (book)"",""Concrete Problems in AI Safety by Amodei et al"",""Unsolved Problems in ML Safety by Hendrycks et al"",""AGI Safety Fundamentals Course"",""the Iterated Amplification sequence on the Alignment Forum"",""AI Safety Camp"",""Superintelligence (book)"",""The Alignment Problem (book)"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"talks by AI alignment researchers,Human Compatible (book),Concrete Problems in AI Safety by Amodei et al,Unsolved Problems in ML Safety by Hendrycks et al,AGI Safety Fundamentals Course,the Iterated Amplification sequence on the Alignment Forum,AI Safety Camp,Superintelligence (book),The Alignment Problem (book),the 80,000 Hours podcast,conversations with AI alignment researchers at conferences",1 | 4 | 2 | 1 | 4 | 1 | 3 | 3 | 2 | 2 | 4,conversations with AI alignment researchers at conferences,2 | 4 | 1 | 1 | 4 | 0 | 2 | 1 | 4 | 2 | 3,2 | 3 | 2 | 2 | 2 | 2 | 3 | 2 | 4 | 1 | 4," | Don’t remember specifics | Don’t remember specifics |  |  | Applied to AI Safety Camp, so I’m talking about the usefulness of diving into one of the researchers’ agendas to write a proposal. | Concerned that the transhumanist vibe and dire predictions are off-putting. The ideas are certainly more out-there than those in Human Compatible by Russell. Personally, I’m not a big fan of transhumanism, so I’m uncomfortable that such a controversial concept is conflated with AI safety, which I think should not be controversial | Haven’t finished reading it myself | ", | ,"",Habiba’s post on 80k alumni Slack,0,
7390791,43,,2022-05-31 10:05:41,2022-05-31 10:08:03,2.37,t6togir,0,,,"[""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast",3,"the 80,000 Hours podcast",3,0,"",,"",80000 hours slack,,
7390884,43,,2022-05-31 10:52:46,2022-05-31 11:01:22,8.60,t6togir,0,,,"[""the FLI podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the ARCHES agenda by Andrew Critch and David Krueger"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the ML Safety newsletter"",""conversations with AI alignment researchers at conferences"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""the annual AI Alignment Literature Review and Charity Comparison"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum"",""Life 3.0 (book)"",""the AI Alignment Newsletter"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",1,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the FLI podcast,Unsolved Problems in ML Safety by Hendrycks et al,the ARCHES agenda by Andrew Critch and David Krueger,AGI Safety Fundamentals Course,the 80,000 Hours podcast,the ML Safety newsletter,conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),AXRP - the AI X-risk Research Podcast,Rob Miles videos,the annual AI Alignment Literature Review and Charity Comparison,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,Life 3.0 (book),the AI Alignment Newsletter,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum",1 | 1 | 2 | 1 | 1 | 4 | 0 | 3 | 2 | 4 | 1 | 3 | 2 | 4 | 1 | 2 | 3 | 1 | 2 | 3 | 1 | 2,the Iterated Amplification sequence on the Alignment Forum,0 | 0 | 1 | 0 | 2 | 4 | 0 | 4 | 2 | 1 | 0 | 3 | 2 | 2 | 0 | 0 | 1 | 3 | 0 | 1 | 3 | 0,0 | 0 | 0 | 1 | 2 | 4 | 0 | 4 | 0 | 4 | 0 | 0 | 1 | 4 | 2 | 2 | 3 | 1 | 2 | 3 | 1 | 2," | The guest list is good, but the interviewer is just really frustratingly bad at making the interviews productive. |  |  | The curriculum confusing and often way less persuasive than it should, but it's still really valuable for a field to have a clear set of shared knowledge, and this seems like the closest thing we have. |  |  |  |  | Good guest list, but badly produced, meandering episodes. |  |  | Doesn't seem relevant to most current important work, but still gets at questions that people are likely to keep having. | I read this a couple of years before I got interested in alignment and it did nothing to convince me. That said, it's still clear and interesting. |  |  |  |  |  | ", | , | , | Constellation Slack,1,
7394585,43,,2022-06-01 13:46:14,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7396022,43,,2022-06-01 23:45:44,,3.32,8d2bvx6,0,,,"[""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the annual AI Alignment Literature Review and Charity Comparison"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",1,I am paid to work on technical AI alignment research,"the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,the 80,000 Hours podcast,the annual AI Alignment Literature Review and Charity Comparison,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda"")",,,,,,,,,0,
7396026,43,,2022-06-01 23:46:54,2022-06-01 23:48:40,1.77,t6togir,0,,,"[""the ML Safety newsletter"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic) | I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the ML Safety newsletter,Unsolved Problems in ML Safety by Hendrycks et al",3 | 3,Unsolved Problems in ML Safety by Hendrycks et al,2 | 4,4 | 4, | ,,"","",4,
7396029,43,,2022-06-01 23:50:24,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7396030,43,,2022-06-01 23:50:29,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7396033,43,,2022-06-01 23:51:54,,2427.08,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7396034,43,,2022-06-01 23:52:56,2022-06-02 00:00:18,7.37,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""AXRP - the AI X-risk Research Podcast"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",6,I am paid to work on technical AI alignment research,"the Embedded Agency sequence on the Alignment Forum,the FLI podcast,Superintelligence (book),AXRP - the AI X-risk Research Podcast,Rob Miles videos,AGI Safety Fundamentals Course,the AI Alignment Newsletter,the 80,000 Hours podcast | the Embedded Agency sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast,Superintelligence (book),AGI Safety Fundamentals Course,the 80,000 Hours podcast,the AI Alignment Newsletter,the FLI podcast",1 | 1 | 3 | 3 | 1 | 1 | 3 | 3 | 4 | 3 | 1,the FLI podcast,0 | 0 | 0 | 3 | 0 | 0 | 0 | 3 | 1 | 2 | 0,1 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 2 | 0, |  | It was hugely useful because it was THE THING that got me interested in AI alignment. These days there are a lot better resources around though. | I think the Christiano and Ngo episodes are great introductions to the alignment problem and serve as interesting overviews for people who have more experience. The Wentworth one is also great. |  |  | It was hugely useful because it was THE THING that got me interested in AI alignment. These days there are a lot better resources around though. | I think the Christiano and Ngo episodes are great introductions to the alignment problem and serve as interesting overviews for people who have more experience. The Wentworth one is also great. |  |  | ,,The risks from learned optimizers paper is a great intro to inner alignment issues. I think having a more accessible version of it would be good.,CHAI slack,0,
7396041,43,,2022-06-01 23:55:14,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7396133,43,,2022-06-02 00:47:03,,0.53,t0y9zda,0,,,,,,I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,,,,,,,,,,0,
7396168,43,,2022-06-02 01:05:30,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7396279,43,,2022-06-02 02:06:12,,1.78,8d2bvx6,0,,,"[""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""Life 3.0 (book)"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"talks by AI alignment researchers,conversations with AI alignment researchers at conferences,Life 3.0 (book),the AI Alignment Newsletter",,,,,,,,,1,
7396583,43,,2022-06-02 05:11:46,,2.65,8d2bvx6,0,,,"[""the Machine Learning for Alignment Bootcamp"",""The Alignment Problem (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""the AI Alignment Newsletter"",""AIRCS workshops"",""AI Safety Camp"",""AGI Safety Fundamentals Course"",""conversations with AI alignment researchers at conferences"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""the ML Safety newsletter"",""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos"",""Concrete Problems in AI Safety by Amodei et al"",""the Iterated Amplification sequence on the Alignment Forum"",""Human Compatible (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the FLI podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the Value Learning sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the Machine Learning for Alignment Bootcamp,The Alignment Problem (book),the annual AI Alignment Literature Review and Charity Comparison,the AI Alignment Newsletter,AIRCS workshops,AI Safety Camp,AGI Safety Fundamentals Course,conversations with AI alignment researchers at conferences,AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,the ML Safety newsletter,the Embedded Agency sequence on the Alignment Forum,Rob Miles videos,Concrete Problems in AI Safety by Amodei et al,the Iterated Amplification sequence on the Alignment Forum,Human Compatible (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the FLI podcast,the ARCHES agenda by Andrew Critch and David Krueger,talks by AI alignment researchers,Superintelligence (book),the Value Learning sequence on the Alignment Forum",,,,,,,,,4,
7398077,43,,2022-06-02 15:19:18,,0.80,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7401899,43,,2022-06-03 15:52:54,2022-06-03 16:02:59,10.08,t6togir,0,,,"[""the ML Safety newsletter"",""conversations with AI alignment researchers at conferences"",""Concrete Problems in AI Safety by Amodei et al"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am paid to work on technical AI alignment research","[""I am interested in AI alignment research"",""I am paid to work on technical AI alignment research""]",2,I am paid to work on technical AI alignment research,"the ML Safety newsletter,conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al,Unsolved Problems in ML Safety by Hendrycks et al",2 | 1 | 1 | 1,Unsolved Problems in ML Safety by Hendrycks et al,2 | 1 | 3 | 3,3 | 3 | 0 | 1,It doesn't serve as introductory materials moreso as a summarization and a nice way to keep up with what is going on in the literature/research with regards to AI Safety. | It does serve as a nice high level overview of the problems.  I just didn't find very much of it actually actionable personally. | This also gives a nice overview of the space and tries to orient researchers early on in their careers.  For more seasoned researchers I'd recommend skimming it as they might already know quite a few of the ideas.,This one is tough because it depends on the researcher.  Some are more approachable then others.  You'd have to get more familiar with the work to be able to talk to some and other researchers are more approachable for complete newbies.,"",AI-X Risk Slack,1,
7401998,43,,2022-06-03 16:33:52,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7402110,43,,2022-06-03 17:25:27,,1463.95,now6hjo,0,,,"[""The Alignment Problem (book)"",""the 80,000 Hours podcast"",""Human Compatible (book)"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"The Alignment Problem (book),the 80,000 Hours podcast,Human Compatible (book),Superintelligence (book)",4 | 3,Human Compatible (book),4 | 3,1 | 0,"I feel like anyone paid to do alignment research would already have read the book | 80k podcast isn’t only alignment-focused, so it’s not that applicable to professionals",,,,,
7402111,43,,2022-06-03 17:25:28,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7409803,43,yrmasik,2022-06-06 15:17:21,,0.55,u166gt1,0,,,,,,I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,,,,,,,,,,,,
7410138,43,,2022-06-06 17:17:14,,0.08,u166gt1,0,,,,,,I spend some of my time doing AI alignment field/community-building,"[""I spend some of my time doing AI alignment field/community-building""]",,,,,,,,,,,,,
7410184,43,,2022-06-06 17:32:41,2022-06-06 17:45:28,12.77,t6togir,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""AI Safety Camp"",""the AI Alignment Newsletter"",""the Value Learning sequence on the Alignment Forum"",""Human Compatible (book)"",""the FLI podcast"",""Superintelligence (book)"",""the Embedded Agency sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",4,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,AI Safety Camp,the AI Alignment Newsletter,the Value Learning sequence on the Alignment Forum,Human Compatible (book),the FLI podcast,Superintelligence (book),the Embedded Agency sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),the 80,000 Hours podcast",3 | 2 | 3 | 1 | 2 | 2 | 2 | 1 | 2 | 4 | 2 | 3 | 3 | 3 | 4,"the 80,000 Hours podcast",4 | 3 | 0 | 1 | 3 | 4 | 2 | 1 | 2 | 1 | 1 | 2 | 0 | 4 | 4,2 | 2 | 3 | 3 | 1 | 2 | 2 | 1 | 3 | 4 | 2 | 4 | 3 | 3 | 3,"It's very concrete and easy to understand to people new to the area or who have never heard of it, but it is also something people will probably know if they have worked on AIS already. |  |  |  |  |  |  |  |  |  |  |  | ", | ,"","",,
7410218,43,,2022-06-06 17:44:05,,1.03,8d2bvx6,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""talks by AI alignment researchers"",""the Iterated Amplification sequence on the Alignment Forum"",""The Alignment Problem (book)"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""the 80,000 Hours podcast"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",3,I spend some of my time doing AI alignment field/community-building,"Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,talks by AI alignment researchers,the Iterated Amplification sequence on the Alignment Forum,The Alignment Problem (book),Superintelligence (book),AGI Safety Fundamentals Course,the FLI podcast,the 80,000 Hours podcast,Rob Miles videos",,,,,,,,,,
7410254,43,,2022-06-06 17:54:43,,0.37,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7410262,43,,2022-06-06 17:57:34,2022-06-06 18:07:47,10.22,t6togir,0,,,"[""talks by AI alignment researchers"",""The Alignment Problem (book)"",""Human Compatible (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""conversations with AI alignment researchers at conferences"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",0,I am paid to work on technical AI alignment research,"conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,The Alignment Problem (book),Superintelligence (book),Human Compatible (book),the 80,000 Hours podcast,the AI Alignment Newsletter,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course | talks by AI alignment researchers,The Alignment Problem (book),Human Compatible (book),Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,AXRP - the AI X-risk Research Podcast,Rob Miles videos,conversations with AI alignment researchers at conferences,the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book),the AI Alignment Newsletter,AGI Safety Fundamentals Course",4 | 4 | 2 | 3 | 3 | 3 | 2 | 3 | 4 | 2 | 2 | 3 | 4,AGI Safety Fundamentals Course,4 | 4 | 3 | 3 | 3 | 4 | 3 | 4 | 1 | 2 | 2 | 3 | 4,4 | 4 | 2 | 2 | 3 | 3 | 4 | 1 | 4 | 3 | 0 | 1 | 1," |  |  |  |  |  |  |  | I think everyone probably knows about it already.
Is a shame there is not much activity recently | ","Evan Hubinger
Vivek Hebbar
Lucius Bushnaq
Buck Schlegeris | Evan Hubinger
Vivek Hebbar
Lucius Bushnaq
Buck Schlegeris | ","",EA Ireland whatsapp,0,
7410267,43,,2022-06-06 17:58:38,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410294,43,,2022-06-06 18:08:52,2022-06-06 18:12:01,3.13,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the ML Safety newsletter"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",2,I spend some of my time solving technical problems related to AI alignment,"conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,AGI Safety Fundamentals Course,the 80,000 Hours podcast,the ML Safety newsletter,the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison",4 | 2 | 1 | 3 | 1 | 2 | 2 | 1 | 1 | 0,the annual AI Alignment Literature Review and Charity Comparison,4 | 3 | 1 | 4 | 0 | 2 | 2 | 0 | 1 | 0,4 | 1 | 0 | 1 | 0 | 2 | 1 | 0 | 1 | 0, |  |  |  |  |  |  | , | ,"",Twitter,,
7410322,43,,2022-06-06 18:16:01,2022-06-06 18:18:08,2.12,t6togir,0,,,"[""the 80,000 Hours podcast"",""Superintelligence (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",4,I am interested in AI alignment research,"the 80,000 Hours podcast,Superintelligence (book),Rob Miles videos",2 | 2 | 2,Rob Miles videos,2 | 2 | 4,0 | 0 | 1, |  | ,,"","",,
7410325,43,,2022-06-06 18:16:19,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410334,43,,2022-06-06 18:19:18,2022-06-06 18:25:57,6.65,t6togir,0,,,"[""the annual AI Alignment Literature Review and Charity Comparison"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"the annual AI Alignment Literature Review and Charity Comparison,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,Rob Miles videos,the 80,000 Hours podcast,the AI Alignment Newsletter",4 | 2 | 4 | 3 | 2 | 4,the AI Alignment Newsletter,0 | 4 | 4 | 4 | 2 | 2,3 | 2 | 3 | 0 | 2 | 4, |  |  |  |  | ,,"",Twitter,,
7410342,43,,2022-06-06 18:21:09,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410378,43,,2022-06-06 21:15:54,2022-06-07 00:15:48,179.90,t6togir,0,,,"[""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""conversations with AI alignment researchers at conferences"",""Human Compatible (book)"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),conversations with AI alignment researchers at conferences,Human Compatible (book),talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,the AI Alignment Newsletter",1 | 2 | 3 | 3 | 2 | 3 | 2 | 2 | 3,the AI Alignment Newsletter,0 | 2 | 4 | 2 | 3 | 4 | 2 | 2 | 2,0 | 2 | 0 | 4 | 1 | 3 | 2 | 1 | 3,I think it's a good introduction to AI alignment for a very particular kind of person - maybe a maths student who likes philosophy? - but can be off-putting to others. |  |  |  |  |  | , | ,"","",,
7410406,43,,2022-06-06 18:43:12,,146.30,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410416,43,,2022-06-06 18:47:09,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410448,43,,2022-06-06 19:01:02,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410496,43,,2022-06-06 19:16:12,2022-06-06 19:22:47,6.57,t6togir,0,,,"[""Superintelligence (book)"",""talks by AI alignment researchers"",""Rob Miles videos"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"Superintelligence (book),talks by AI alignment researchers,Rob Miles videos,the AI Alignment Newsletter",3 | 2 | 2 | 1,the AI Alignment Newsletter,3 | 2 | 3 | 1,0 | 2 | 1 | 4," |  | My primary reason for only getting a little personal use from the newsletter is that I have only been subscribed for a short time, but what I have seen was useful",Rohin Shah,"",EA Ireland group chat,,
7410569,43,,2022-06-06 19:40:27,,1.12,now6hjo,0,,,"[""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""Rob Miles videos"",""Human Compatible (book)"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",1,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Superintelligence (book),AGI Safety Fundamentals Course,talks by AI alignment researchers,Rob Miles videos,Human Compatible (book),the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast",0,Superintelligence (book),0,0,"",,,,,
7410621,43,,2022-06-06 19:57:56,2022-06-06 19:59:47,1.83,t6togir,0,,,"[""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",2,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"talks by AI alignment researchers,the 80,000 Hours podcast,Superintelligence (book)",3 | 3 | 4,Superintelligence (book),3 | 3 | 3,3 | 3 | 4, | ,"","",twitter,,
7410786,43,,2022-06-06 21:16:57,2022-06-06 21:27:27,10.50,t6togir,0,,,"[""Life 3.0 (book)"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""the Embedded Agency sequence on the Alignment Forum"",""The Alignment Problem (book)"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career | I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",6,I am trying to move into a technical AI alignment career,"Life 3.0 (book),AGI Safety Fundamentals Course,Rob Miles videos,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),the 80,000 Hours podcast,the Embedded Agency sequence on the Alignment Forum,The Alignment Problem (book),the FLI podcast",3 | 3 | 2 | 3 | 3 | 3 | 3 | 3 | 2 | 3 | 4 | 1 | 2 | 3,the FLI podcast,2 | 4 | 3 | 3 | 3 | 3 | 1 | 3 | 1 | 2 | 4 | 1 | 1 | 3,1 | 2 | 1 | 4 | 3 | 3 | 4 | 4 | 2 | 1 | 2 | 2 | 0 | 1, |  |  |  |  |  |  |  |  |  |  | , | seems expensive to have them talk to very inexperienced people.,"I am currently considering going into technical alignment, among other things. Whether I'm ""trying"" to get into the field is therefore ambiguous.",jamie bernardi twitter,,
7410829,43,,2022-06-06 21:35:30,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7410922,43,,2022-06-06 22:13:28,,0.65,8d2bvx6,0,,,"[""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I have heard of AI alignment,"[""I have heard of AI alignment""]",,I have heard of AI alignment,"the 80,000 Hours podcast",,,,,,,,,,
7411106,43,,2022-06-06 23:38:59,2022-06-06 23:42:21,3.37,t6togir,0,,,"[""Rob Miles videos"",""Life 3.0 (book)"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",5,I spend some of my time solving technical problems related to AI alignment,"Rob Miles videos,Life 3.0 (book),the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,Superintelligence (book),the 80,000 Hours podcast,talks by AI alignment researchers,AGI Safety Fundamentals Course",4 | 2 | 3 | 4 | 4 | 4 | 4 | 4,AGI Safety Fundamentals Course,4 | 2 | 1 | 0 | 4 | 4 | 2 | 4,4 | 1 | 4 | 4 | 4 | 2 | 4 | 0, |  |  |  |  | , | ,"",Local EA group (EA Ireland) groupchat,,
7411229,43,,2022-06-07 00:35:16,,1.67,now6hjo,0,,,"[""the 80,000 Hours podcast"",""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""the FLI podcast"",""Rob Miles videos"",""Life 3.0 (book)"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"the 80,000 Hours podcast,the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),the FLI podcast,Rob Miles videos,Life 3.0 (book),Superintelligence (book)",3,"the 80,000 Hours podcast",2,1,"",,,,,
7411387,43,,2022-06-07 01:57:36,,0.00,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7411717,43,,2022-06-07 04:59:12,,0.18,u166gt1,0,,,,,,I have heard of AI alignment,"[""I have heard of AI alignment""]",,,,,,,,,,,,,
7412066,43,,2022-06-07 09:40:35,2022-06-07 09:43:31,2.92,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""the AI Alignment Newsletter"",""Rob Miles videos"",""Superintelligence (book)"",""Human Compatible (book)"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""Concrete Problems in AI Safety by Amodei et al"",""the FLI podcast"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"AXRP - the AI X-risk Research Podcast,the AI Alignment Newsletter,Rob Miles videos,Superintelligence (book),Human Compatible (book),the 80,000 Hours podcast,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al,the FLI podcast,The Alignment Problem (book)",3 | 2 | 2 | 3 | 3 | 3 | 1 | 2 | 1 | 1 | 3,The Alignment Problem (book),3 | 3 | 4 | 3 | 4 | 2 | 1 | 2 | 0 | 1 | 4,2 | 4 | 2 | 1 | 1 | 3 | 3 | 4 | 2 | 1 | 2, |  |  |  |  |  |  |  | , | ,"","",,
7412073,43,,2022-06-07 09:43:32,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7412075,43,,2022-06-07 09:45:29,,3.03,now6hjo,0,,,"[""the 80,000 Hours podcast"",""AGI Safety Fundamentals Course"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""Human Compatible (book)"",""AXRP - the AI X-risk Research Podcast"",""the Machine Learning for Alignment Bootcamp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,AGI Safety Fundamentals Course,Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,the AI Alignment Newsletter,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,Human Compatible (book),AXRP - the AI X-risk Research Podcast,the Machine Learning for Alignment Bootcamp",2 | 3,AGI Safety Fundamentals Course,2 | 4,0 | 0, | ,,,,,
7412079,43,,2022-06-07 09:47:10,2022-06-07 12:10:42,143.52,t6togir,0,,,"[""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,AGI Safety Fundamentals Course,4,AGI Safety Fundamentals Course,3,2,"As a beginner in the AI Alignment realm, this course helped to navigate what readings one should look into and read to gain knowledge as sought of a foundation. It also meant that one could gain different perspectives from people with various backgrounds in AI, be it governance or technical. Listening to new ideas and having discussions with people made the readings more enjoyable and learning was more fun as no idea is right or wrong, just that it's what a person took from what they read and any other prior knowledge they may have had to bring to the discussions. It also opened for new ideas to be looked into and thought about. 

For a friend who is being paid, if they are also a rookie like i am the formative would be a takeaway that they would experience. However, if they are more experienced in AI Alignment, this course will be helpful in gaining resources for their research while gaining insight from talking to other like-minded and curious participants who can provide advice.",,This was an amazing course that was eye-opening and challenged me to think more about AI,Jamie Bernardi posted it on Twitter,,
7412084,43,,2022-06-07 09:49:28,,0.83,u166gt1,0,,,,,,"I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",,,,,,,,,,,,,
7412087,43,,2022-06-07 09:51:04,,0.47,72erk1d,0,,,"[""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time doing AI alignment field/community-building,"[""I spend some of my time doing AI alignment field/community-building""]",,I spend some of my time doing AI alignment field/community-building,"the 80,000 Hours podcast",,,,,,,,,,
7412119,43,,2022-06-07 10:14:49,,1.12,8d2bvx6,0,,,"[""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""Human Compatible (book)"",""the Machine Learning for Alignment Bootcamp"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",0,I am trying to move into a technical AI alignment career,"AGI Safety Fundamentals Course,Superintelligence (book),Human Compatible (book),the Machine Learning for Alignment Bootcamp,the 80,000 Hours podcast",,,,,,,,,,
7412122,43,,2022-06-07 10:18:27,2022-06-07 10:27:46,9.30,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""the FLI podcast"",""conversations with AI alignment researchers at conferences"",""the Value Learning sequence on the Alignment Forum"",""Rob Miles videos"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building""]",5,I spend some of my time doing AI alignment field/community-building,"AXRP - the AI X-risk Research Podcast,the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,talks by AI alignment researchers,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum,the ARCHES agenda by Andrew Critch and David Krueger,Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,the FLI podcast,conversations with AI alignment researchers at conferences,the Value Learning sequence on the Alignment Forum,Rob Miles videos,the 80,000 Hours podcast",2 | 2 | 2 | 4 | 2 | 1 | 2 | 3 | 4 | 2 | 2 | 1 | 2 | 1 | 3 | 4,"the 80,000 Hours podcast",0 | 4 | 4 | 4 | 4 | 1 | 4 | 0 | 4 | 2 | 1 | 0 | 1 | 1 | 4 | 4,3 | 4 | 4 | 1 | 2 | 1 | 2 | 3 | 3 | 2 | 3 | 1 | 3 | 3 | 2 | 3," |  | I've recommended it to a friend doing an ML PhD, who wasn't familiar with alignment work | I'd be unlikely to recommend it to someone who is paid to do alignment research as it seems obvious and patronising! |  | Already have given it and recommended it to many people new to alignment |  | I think it's excellent | Very uncertain about my answers here... I struggle to remember a lot of its details, but I refer back to certain aspects of it myself (for its definitions and explained dilemmas) and expect I'd recommend friends read those parts too, if I think they're relevant. |  |  |  |  | Really good for career advice, as well as getting a sense of individual researchers' opinions that doesn't always come through in writing.",I find it difficult to bring specific talks to mind | ,"","Jamie Bernadi tweeted it, and Habiba retweeted that tweet.",,
7412147,43,,2022-06-07 10:30:18,2022-06-07 10:32:33,2.23,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"AXRP - the AI X-risk Research Podcast,Rob Miles videos,the AI Alignment Newsletter,the 80,000 Hours podcast,the FLI podcast",2 | 4 | 2 | 4 | 2,the FLI podcast,4 | 4 | 4 | 2 | 2,2 | 2 | 4 | 2 | 3, |  |  |  | ,,"",Twitter,,
7412164,43,,2022-06-07 10:40:39,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7412190,43,,2022-06-07 11:04:28,,1.92,8d2bvx6,0,,,"[""the FLI podcast"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""the Machine Learning for Alignment Bootcamp"",""the ML Safety newsletter"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"the FLI podcast,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,talks by AI alignment researchers,the 80,000 Hours podcast,the Machine Learning for Alignment Bootcamp,the ML Safety newsletter,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,Rob Miles videos",,,,,,,,,,
7412230,43,,2022-06-07 11:23:09,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7412293,43,,2022-06-07 12:11:02,,0.57,u166gt1,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",,,,,,,,,,,,,
7412505,43,,2022-06-07 13:43:00,,1.20,4t3j1q5,0,,,"[""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",1,I am interested in AI alignment research,"Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,The Alignment Problem (book)",,,,,,,,,,
7412728,43,,2022-06-07 14:44:16,2022-06-07 14:47:21,3.07,t6togir,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""The Alignment Problem (book)"",""the AI Alignment Newsletter"",""Life 3.0 (book)"",""Rob Miles videos"",""Human Compatible (book)"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"Unsolved Problems in ML Safety by Hendrycks et al,The Alignment Problem (book),the AI Alignment Newsletter,Life 3.0 (book),Rob Miles videos,Human Compatible (book),Concrete Problems in AI Safety by Amodei et al",2 | 2 | 2 | 3 | 3 | 3 | 3,Concrete Problems in AI Safety by Amodei et al,2 | 1 | 2 | 3 | 3 | 3 | 3,2 | 1 | 2 | 2 | 3 | 3 | 3, |  |  |  |  |  | ,,"","",,
7413214,43,,2022-06-07 17:26:21,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7413442,43,,2022-06-07 18:55:02,2022-06-07 18:57:30,2.47,t6togir,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Superintelligence (book)"",""Human Compatible (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,Superintelligence (book),Human Compatible (book),Rob Miles videos",2 | 3 | 3 | 3 | 1 | 2 | 2 | 4 | 2 | 4,Rob Miles videos,0 | 2 | 2 | 4 | 1 | 2 | 3 | 3 | 3 | 4,3 | 2 | 2 | 2 | 1 | 1 | 1 | 1 | 1 | 1, |  |  |  |  |  |  |  | ,"","","Twitter, jamie bernardi",,
7414005,43,,2022-06-07 23:37:49,2022-06-09 17:51:53,2534.07,t6togir,0,,,"[""the 80,000 Hours podcast"",""the FLI podcast"",""Human Compatible (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""Rob Miles videos"",""the AI Alignment Newsletter"",""Life 3.0 (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",3 | 3 | 3 | 3,I am interested in AI alignment research,"talks by AI alignment researchers,the AI Alignment Newsletter,the FLI podcast,the annual AI Alignment Literature Review and Charity Comparison,the ML Safety newsletter,Life 3.0 (book),Human Compatible (book),the 80,000 Hours podcast,The Alignment Problem (book),Rob Miles videos | Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison,Life 3.0 (book),Rob Miles videos,the AI Alignment Newsletter,the ML Safety newsletter,The Alignment Problem (book),the FLI podcast,the 80,000 Hours podcast | Life 3.0 (book),the AI Alignment Newsletter,Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison,Rob Miles videos,the FLI podcast,the 80,000 Hours podcast | the 80,000 Hours podcast,the FLI podcast,Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison,Rob Miles videos,the AI Alignment Newsletter,Life 3.0 (book)",4 | 3 | 3 | 3 | 3 | 3 | 3,Life 3.0 (book),4 | 3 | 3 | 3 | 4 | 3 | 4,3 | 3 | 3 | 3 | 3 | 3 | 3,It's how I first learned about AI alignment. Most episodes aren't related to AI alignment. |  |  | It's how I first learned about AI alignment. Most episodes aren't related to AI alignment. |  |  | ,,"",twitter,,
7414008,43,,2022-06-07 23:38:30,,0.07,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7414013,43,,2022-06-07 23:41:09,,494.85,72erk1d,0,,,"[""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",0,I am interested in AI alignment research,talks by AI alignment researchers,,talks by AI alignment researchers,,,,,,,,
7414015,43,,2022-06-07 23:42:05,,6.90,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",4,I am interested in AI alignment research,,,,,,,,,,,
7414020,43,,2022-06-07 23:45:03,,0.13,u166gt1,0,,,,,,I have heard of AI alignment,"[""I have heard of AI alignment""]",,,,,,,,,,,,,
7414039,43,,2022-06-07 23:55:07,,41.58,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time publicly communicating about AI alignment""]",15,I am interested in AI alignment research,,,,,,,,,,,
7414040,43,,2022-06-07 23:55:08,2022-06-07 23:58:08,2.98,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Superintelligence (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",20,I spend some of my time publicly communicating about AI alignment,"conversations with AI alignment researchers at conferences,Unsolved Problems in ML Safety by Hendrycks et al,Superintelligence (book),talks by AI alignment researchers",4 | 3 | 3 | 2,talks by AI alignment researchers,4 | 4 | 3 | 2,4 | 1 | 0 | 1, | , | ,"",Stefan Schubert on Twitter,,
7414091,43,,2022-06-08 00:30:57,,12.88,now6hjo,0,,,"[""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""AGI Safety Fundamentals Course"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos"",""the Value Learning sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time publicly communicating about AI alignment""]",6,I spend some of my time publicly communicating about AI alignment,"the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al,AGI Safety Fundamentals Course,the AI Alignment Newsletter,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,Rob Miles videos,the Value Learning sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,the FLI podcast",2,"the 80,000 Hours podcast",1,2,"",,,,,
7414259,43,,2022-06-08 01:55:32,2022-06-08 03:34:41,99.13,t6togir,0,,,"[""Rob Miles videos"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",5,I am interested in AI alignment research,"Rob Miles videos,Superintelligence (book)",4 | 4,Superintelligence (book),4 | 4,2 | 2, | ,,"",Twitter.,,
7414273,43,,2022-06-08 02:04:24,2022-06-08 02:09:43,5.30,t6togir,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""The Alignment Problem (book)"",""Rob Miles videos"",""the ML Safety newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,The Alignment Problem (book),Rob Miles videos,the ML Safety newsletter",3 | 2 | 1 | 2 | 1,the ML Safety newsletter,3 | 2 | 2 | 4 | 1,2 | 0 | 0 | 1 | 2, |  |  |  | ,,"","",,
7414319,43,,2022-06-08 02:28:05,2022-06-08 02:34:03,5.95,t6togir,0,,,"[""the FLI podcast"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""The Alignment Problem (book)"",""AXRP - the AI X-risk Research Podcast"",""the Value Learning sequence on the Alignment Forum"",""Human Compatible (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the Embedded Agency sequence on the Alignment Forum"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the Iterated Amplification sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""Life 3.0 (book)"",""Rob Miles videos"",""Superintelligence (book)"",""conversations with AI alignment researchers at conferences"",""the annual AI Alignment Literature Review and Charity Comparison"",""Concrete Problems in AI Safety by Amodei et al"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",3,I am paid to work on technical AI alignment research,"the FLI podcast,talks by AI alignment researchers,the AI Alignment Newsletter,The Alignment Problem (book),AXRP - the AI X-risk Research Podcast,the Value Learning sequence on the Alignment Forum,Human Compatible (book),the ARCHES agenda by Andrew Critch and David Krueger,the Embedded Agency sequence on the Alignment Forum,Unsolved Problems in ML Safety by Hendrycks et al,the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast,Life 3.0 (book),Rob Miles videos,Superintelligence (book),conversations with AI alignment researchers at conferences,the annual AI Alignment Literature Review and Charity Comparison,Concrete Problems in AI Safety by Amodei et al,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda"")",1 | 1 | 2 | 2 | 2 | 3 | 3 | 1 | 2 | 1 | 1 | 4 | 1 | 1 | 4 | 3 | 2 | 1 | 0,"Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda"")",0 | 0 | 1 | 4 | 0 | 4 | 4 | 0 | 4 | 1 | 1 | 4 | 0 | 4 | 4 | 2 | 2 | 0 | 0,0 | 0 | 1 | 2 | 4 | 4 | 2 | 0 | 4 | 1 | 1 | 2 | 0 | 0 | 4 | 4 | 2 | 0 | 0, |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | , | ,"",eleutherai discord,1,
7414597,43,,2022-06-08 05:51:27,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7414628,43,,2022-06-08 06:18:35,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7414688,43,,2022-06-08 07:22:24,2022-06-08 07:28:16,5.87,t6togir,0,,,"[""talks by AI alignment researchers"",""Rob Miles videos"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison"",""Unsolved Problems in ML Safety by Hendrycks et al"",""conversations with AI alignment researchers at conferences"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the Value Learning sequence on the Alignment Forum"",""AI Safety Camp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",4,I spend some of my time publicly communicating about AI alignment,"talks by AI alignment researchers,Rob Miles videos,the ARCHES agenda by Andrew Critch and David Krueger,the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison,Unsolved Problems in ML Safety by Hendrycks et al,conversations with AI alignment researchers at conferences,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,the Value Learning sequence on the Alignment Forum,AI Safety Camp",3 | 2 | 4 | 4 | 4 | 2 | 3 | 3 | 4 | 3 | 4,AI Safety Camp,3 | 4 | 2 | 2 | 4 | 3 | 1 | 2 | 4 | 2 | 4,4 | 2 | 4 | 4 | 4 | 3 | 4 | 2 | 2 | 3 | 2, |  |  |  |  |  |  |  | , | ,"","",,
7414693,43,,2022-06-08 07:26:19,,1.10,8d2bvx6,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",10,I am interested in AI alignment research,"the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast",,,,,,,,,,
7414734,43,,2022-06-08 08:00:37,2022-06-08 08:06:10,5.55,t6togir,0,,,"[""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""Human Compatible (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""the 80,000 Hours podcast"",""The Alignment Problem (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""the Embedded Agency sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison,the 80,000 Hours podcast,The Alignment Problem (book),the ARCHES agenda by Andrew Critch and David Krueger,AGI Safety Fundamentals Course,the FLI podcast,the Iterated Amplification sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers",4 | 4 | 1 | 2 | 4 | 2 | 2 | 3 | 3 | 2 | 4 | 3 | 4 | 2 | 4 | 3 | 3,talks by AI alignment researchers,4 | 4 | 4 | 2 | 4 | 3 | 0 | 4 | 4 | 2 | 4 | 1 | 4 | 2 | 3 | 4 | 3,4 | 4 | 0 | 2 | 4 | 3 | 3 | 0 | 3 | 4 | 4 | 4 | 4 | 2 | 4 | 3 | 3, | AXRPs value relative to other resources is that it goes into lots of technical detail |  |  |  |  |  |  |  |  |  |  |  |  | , | ,"","",2,
7414968,43,,2022-06-08 11:32:23,,2.82,72erk1d,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""talks by AI alignment researchers"",""Human Compatible (book)"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",3,I am trying to move into a technical AI alignment career,"the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,Superintelligence (book),talks by AI alignment researchers,Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison",,,,,,,,,,
7415080,43,,2022-06-08 12:44:50,2022-06-08 12:50:57,6.10,t6togir,0,,,"[""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""the Machine Learning for Alignment Bootcamp"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""conversations with AI alignment researchers at conferences"",""the ML Safety newsletter"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Life 3.0 (book)"",""the Value Learning sequence on the Alignment Forum"",""Rob Miles videos"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment""]",25,I spend some of my time solving technical problems related to AI alignment,"Superintelligence (book),AXRP - the AI X-risk Research Podcast,the Machine Learning for Alignment Bootcamp,talks by AI alignment researchers,the AI Alignment Newsletter,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),conversations with AI alignment researchers at conferences,the ML Safety newsletter,Unsolved Problems in ML Safety by Hendrycks et al,Life 3.0 (book),the Value Learning sequence on the Alignment Forum,Rob Miles videos,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum",1 | 1 | 2 | 2 | 2 | 1 | 3 | 1 | 1 | 0 | 1 | 2 | 0 | 1 | 2,the Embedded Agency sequence on the Alignment Forum,1 | 1 | 2 | 3 | 4 | 0 | 4 | 1 | 1 | 0 | 1 | 3 | 0 | 2 | 2,0 | 2 | 1 | 3 | 4 | 1 | 4 | 1 | 1 | 0 | 1 | 2 | 0 | 2 | 2, |  |  |  |  |  |  |  |  |  |  |  | , | ,"","",,
7415089,43,,2022-06-08 12:50:21,,1.30,now6hjo,0,,,"[""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""Concrete Problems in AI Safety by Amodei et al"",""The Alignment Problem (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,Superintelligence (book),AGI Safety Fundamentals Course,Concrete Problems in AI Safety by Amodei et al,The Alignment Problem (book),the 80,000 Hours podcast",2,the Value Learning sequence on the Alignment Forum,1,3,"",,,,,
7415161,43,,2022-06-08 13:53:06,,1.87,72erk1d,0,,,"[""Rob Miles videos"",""Superintelligence (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time publicly communicating about AI alignment,"[""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"Rob Miles videos,Superintelligence (book),the 80,000 Hours podcast",,,,,,,,,,
7415307,43,,2022-06-08 14:47:28,,0.80,8d2bvx6,0,,,"[""Superintelligence (book)"",""Rob Miles videos"",""Human Compatible (book)"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""the FLI podcast"",""The Alignment Problem (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"Superintelligence (book),Rob Miles videos,Human Compatible (book),the 80,000 Hours podcast,the AI Alignment Newsletter,the FLI podcast,The Alignment Problem (book),talks by AI alignment researchers",,,,,,,,,,
7415488,43,,2022-06-08 16:23:15,,0.10,72erk1d,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,,,,
7415990,43,,2022-06-08 20:03:21,,0.98,8d2bvx6,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research | I have heard of AI alignment,"[""I have heard of AI alignment""]",3,I have heard of AI alignment,"talks by AI alignment researchers,Superintelligence (book),Rob Miles videos",,,,,,,,,,
7419588,43,,2022-06-10 09:20:59,2022-06-10 09:42:41,21.68,t6togir,0,,,"[""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""the annual AI Alignment Literature Review and Charity Comparison"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,the annual AI Alignment Literature Review and Charity Comparison,AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum",2 | 2 | 1 | 1 | 3 | 3 | 4 | 2,the Iterated Amplification sequence on the Alignment Forum,1 | 1 | 3 | 2 | 4 | 2 | 4 | 2,3 | 1 | 2 | 2 | 3 | 1 | 2 | 1, |  | You need to concentrate a lot to follow AXRP episodes. |  |  | ,"Connor Leahy, Eliezer Yudkowsky | ","",twitter,,
7421377,43,,2022-06-10 23:23:05,,0.03,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7422537,43,,2022-06-11 15:59:18,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7422538,43,,2022-06-11 15:59:25,,918.25,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7424479,43,,2022-06-12 05:40:58,2022-06-12 05:44:27,3.48,t6togir,0,,,"[""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""AGI Safety Fundamentals Course"",""the AI Alignment Newsletter"",""The Alignment Problem (book)"",""Human Compatible (book)"",""Rob Miles videos"",""talks by AI alignment researchers"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",2,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,AGI Safety Fundamentals Course,the AI Alignment Newsletter,The Alignment Problem (book),Human Compatible (book),Rob Miles videos,talks by AI alignment researchers,the 80,000 Hours podcast",3 | 2 | 4 | 4 | 1 | 4 | 4 | 4 | 1,"the 80,000 Hours podcast",1 | 0 | 4 | 3 | 0 | 2 | 4 | 0 | 0,3 | 4 | 4 | 4 | 0 | 0 | 1 | 4 | 0, |  |  |  |  |  |  | ,"","",Jamie Bernard's twitter,,
7428565,43,,2022-06-13 21:37:46,2022-06-13 21:43:22,5.58,t6togir,0,,,"[""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"Concrete Problems in AI Safety by Amodei et al,AXRP - the AI X-risk Research Podcast,the Machine Learning for Alignment Bootcamp,the AI Alignment Newsletter,the 80,000 Hours podcast | the 80,000 Hours podcast,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,Concrete Problems in AI Safety by Amodei et al",2 | 1 | 2 | 2 | 1 | 2,Concrete Problems in AI Safety by Amodei et al,1 | 0 | 4 | 2 | 0 | 1,2 | 0 | 3 | 3 | 0 | 1, |  |  |  |  | ,,"","",,
7431327,43,,2022-06-14 20:04:14,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7438099,43,,2022-06-16 18:55:24,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7441431,43,,2022-06-18 00:04:10,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7442386,43,,2022-06-18 13:22:55,,0.68,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",,,,,,,,,,,,,
7445817,43,,2022-06-20 07:32:06,2022-06-20 07:34:05,2.13,auqgbpq,0,,,"[""the 80,000 Hours podcast"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,Superintelligence (book)",4 | 2,Superintelligence (book),4 | 4,0 | 0, | ,,"","",,
7468686,43,,2022-06-25 17:54:55,2022-06-25 17:55:11,0.25,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,"","",,
7468742,43,,2022-06-25 17:58:01,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7468750,43,,2022-06-25 17:58:17,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7468754,43,,2022-06-25 17:58:25,,1.67,8d2bvx6,0,,,"[""conversations with AI alignment researchers at conferences"",""the AI Alignment Newsletter"",""the ML Safety newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""Rob Miles videos"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""the FLI podcast"",""the Value Learning sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""Unsolved Problems in ML Safety by Hendrycks et al"",""The Alignment Problem (book)"",""Life 3.0 (book)"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",4,I spend some of my time publicly communicating about AI alignment,"conversations with AI alignment researchers at conferences,the AI Alignment Newsletter,the ML Safety newsletter,Concrete Problems in AI Safety by Amodei et al,Rob Miles videos,AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),AGI Safety Fundamentals Course,Superintelligence (book),the FLI podcast,the Value Learning sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,Unsolved Problems in ML Safety by Hendrycks et al,The Alignment Problem (book),Life 3.0 (book),Human Compatible (book)",,,,,,,,,,
7468780,43,,2022-06-25 17:59:33,2022-06-25 18:01:12,1.65,t6togir,0,,,"[""the Machine Learning for Alignment Bootcamp"",""the 80,000 Hours podcast"",""the Value Learning sequence on the Alignment Forum"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",1,I spend some of my time solving technical problems related to AI alignment,"the Machine Learning for Alignment Bootcamp,the 80,000 Hours podcast,the Value Learning sequence on the Alignment Forum,Rob Miles videos",3 | 2 | 2 | 2,Rob Miles videos,4 | 1 | 2 | 3,4 | 1 | 1 | 1, |  |  | ,,"","",,
7468793,43,,2022-06-25 17:59:53,,0.88,72erk1d,0,,,"[""AXRP - the AI X-risk Research Podcast"",""The Alignment Problem (book)"",""AGI Safety Fundamentals Course"",""AI Safety Camp"",""the ML Safety newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"AXRP - the AI X-risk Research Podcast,The Alignment Problem (book),AGI Safety Fundamentals Course,AI Safety Camp,the ML Safety newsletter",,,,,,,,,,
7468856,43,,2022-06-25 18:02:29,,1.73,72erk1d,0,,,"[""The Alignment Problem (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",1.5 | 2,I am interested in AI alignment research,"The Alignment Problem (book),the annual AI Alignment Literature Review and Charity Comparison,AGI Safety Fundamentals Course,Superintelligence (book),the AI Alignment Newsletter,the 80,000 Hours podcast,talks by AI alignment researchers",,,,,,,,,,
7468871,43,,2022-06-25 18:03:10,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7468930,43,,2022-06-25 18:05:51,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7468943,43,,2022-06-25 18:06:19,,0.73,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7468956,43,,2022-06-25 18:07:24,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7468958,43,,2022-06-25 18:07:31,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469033,43,,2022-06-25 18:12:31,,1.12,u166gt1,0,,,,,,"I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,,,,,,,,,,,
7469052,43,,2022-06-25 18:13:41,,0.70,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469064,43,,2022-06-25 18:14:51,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469088,43,,2022-06-25 18:16:30,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469090,43,,2022-06-25 18:16:55,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7469093,43,,2022-06-25 18:17:09,2022-06-25 18:22:46,5.60,t6togir,0,,,"[""AGI Safety Fundamentals Course"",""AXRP - the AI X-risk Research Podcast"",""the Embedded Agency sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the Value Learning sequence on the Alignment Forum"",""the ML Safety newsletter"",""talks by AI alignment researchers"",""the Iterated Amplification sequence on the Alignment Forum"",""The Alignment Problem (book)"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",1,I spend some of my time solving technical problems related to AI alignment,"AGI Safety Fundamentals Course,AXRP - the AI X-risk Research Podcast,the Embedded Agency sequence on the Alignment Forum,the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al,the Value Learning sequence on the Alignment Forum,the ML Safety newsletter,talks by AI alignment researchers,the Iterated Amplification sequence on the Alignment Forum,The Alignment Problem (book),the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison",3 | 2 | 4 | 3 | 2 | 2 | 4 | 3 | 2 | 2 | 4 | 4,the annual AI Alignment Literature Review and Charity Comparison,3 | 1 | 4 | 3 | 2 | 2 | 3 | 3 | 2 | 4 | 4 | 3,0 | 4 | 4 | 0 | 0 | 2 | 3 | 3 | 1 | 0 | 4 | 3, |  |  |  |  |  |  |  |  |  | ,"","",twitter,,
7469096,43,,2022-06-25 18:17:13,,1.77,72erk1d,0,,,"[""Rob Miles videos"",""AXRP - the AI X-risk Research Podcast"",""the annual AI Alignment Literature Review and Charity Comparison"",""the Embedded Agency sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the FLI podcast"",""the Value Learning sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",7,I spend some of my time doing AI alignment field/community-building,"Rob Miles videos,AXRP - the AI X-risk Research Podcast,the annual AI Alignment Literature Review and Charity Comparison,the Embedded Agency sequence on the Alignment Forum,talks by AI alignment researchers,Superintelligence (book),the FLI podcast,the Value Learning sequence on the Alignment Forum,the AI Alignment Newsletter,the 80,000 Hours podcast",,,,,,,,,,
7469102,43,,2022-06-25 18:17:41,2022-06-25 19:32:38,74.95,t6togir,0,,,"[""Rob Miles videos"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time publicly communicating about AI alignment""]",8,I spend some of my time publicly communicating about AI alignment,"Rob Miles videos,the AI Alignment Newsletter,the 80,000 Hours podcast,Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum,the Iterated Amplification sequence on the Alignment Forum",2 | 2 | 1 | 4 | 1 | 1,the Iterated Amplification sequence on the Alignment Forum,4 | 0 | 3 | 1 | 1 | 0,1 | 3 | 1 | 4 | 2 | 2, |  |  |  |  | ,,"","",,
7469103,43,,2022-06-25 18:17:46,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469108,43,,2022-06-25 18:18:03,,1.33,8d2bvx6,0,,,"[""Rob Miles videos"",""the 80,000 Hours podcast"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",3,I am trying to move into a technical AI alignment career,"Rob Miles videos,the 80,000 Hours podcast,Superintelligence (book)",,,,,,,,,,
7469112,43,,2022-06-25 18:18:12,2022-06-25 18:23:43,5.52,t6togir,0,,,"[""the AI Alignment Newsletter"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""the annual AI Alignment Literature Review and Charity Comparison"",""the Embedded Agency sequence on the Alignment Forum"",""Human Compatible (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the FLI podcast"",""talks by AI alignment researchers"",""the Value Learning sequence on the Alignment Forum"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",20,I spend some of my time publicly communicating about AI alignment,"the AI Alignment Newsletter,Unsolved Problems in ML Safety by Hendrycks et al,Superintelligence (book),AXRP - the AI X-risk Research Podcast,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast,the annual AI Alignment Literature Review and Charity Comparison,the Embedded Agency sequence on the Alignment Forum,Human Compatible (book),the ARCHES agenda by Andrew Critch and David Krueger,the FLI podcast,talks by AI alignment researchers,the Value Learning sequence on the Alignment Forum,The Alignment Problem (book)",2 | 1 | 3 | 2 | 2 | 2 | 3 | 2 | 1 | 1 | 1 | 2 | 1 | 1 | 1,The Alignment Problem (book),3 | 1 | 4 | 1 | 4 | 0 | 3 | 0 | 0 | 1 | 0 | 2 | 0 | 0 | 2,3 | 1 | 4 | 1 | 2 | 1 | 2 | 1 | 2 | 0 | 1 | 1 | 2 | 1 | 0, |  |  |  |  |  |  |  |  |  |  |  |  | ,"","",lxrjl twitter,,
7469137,43,,2022-06-25 18:20:46,2022-06-25 18:26:43,5.93,t6togir,0,,,"[""the 80,000 Hours podcast"",""Human Compatible (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the FLI podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""AXRP - the AI X-risk Research Podcast"",""Superintelligence (book)"",""the Embedded Agency sequence on the Alignment Forum"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""talks by AI alignment researchers"",""the ML Safety newsletter"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,Human Compatible (book),the ARCHES agenda by Andrew Critch and David Krueger,the FLI podcast,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,AXRP - the AI X-risk Research Podcast,Superintelligence (book),the Embedded Agency sequence on the Alignment Forum,AGI Safety Fundamentals Course,Rob Miles videos,talks by AI alignment researchers,the ML Safety newsletter,the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),Unsolved Problems in ML Safety by Hendrycks et al",3 | 2 | 2 | 2 | 2 | 2 | 3 | 2 | 2 | 3 | 2 | 2 | 2 | 3 | 3 | 1 | 2,Unsolved Problems in ML Safety by Hendrycks et al,3 | 2 | 0 | 2 | 0 | 2 | 2 | 2 | 0 | 4 | 3 | 1 | 1 | 1 | 3 | 1 | 0,2 | 0 | 2 | 1 | 2 | 1 | 3 | 0 | 1 | 1 | 0 | 3 | 1 | 3 | 2 | 0 | 1, |  |  |  |  |  |  |  |  |  |  |  |  |  |  | ,Paul Christiano at EA global,"",Twitter,,
7469217,43,,2022-06-25 18:28:09,2022-06-25 18:35:51,7.70,t6togir,0,,,"[""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,the AI Alignment Newsletter,,the AI Alignment Newsletter,,,,,,,,
7469326,43,,2022-06-25 18:35:58,,0.13,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7469414,43,,2022-06-25 18:40:57,,0.33,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7469421,43,,2022-06-25 18:41:21,2022-06-25 21:07:25,146.05,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""AI Safety Camp"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""AXRP - the AI X-risk Research Podcast"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course"",""the ML Safety newsletter"",""conversations with AI alignment researchers at conferences"",""the FLI podcast"",""The Alignment Problem (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",1,I am paid to work on technical AI alignment research,"the Embedded Agency sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,AI Safety Camp,the 80,000 Hours podcast,talks by AI alignment researchers,AXRP - the AI X-risk Research Podcast,the AI Alignment Newsletter,AGI Safety Fundamentals Course,the ML Safety newsletter,conversations with AI alignment researchers at conferences,the FLI podcast,The Alignment Problem (book),Rob Miles videos",2 | 1 | 0 | 3 | 4 | 2 | 3 | 2 | 2 | 2 | 2 | 3 | 1 | 1 | 4,Rob Miles videos,1 | 2 | 0 | 2 | 4 | 4 | 3 | 2 | 1 | 3 | 1 | 4 | 1 | 3 | 4,2 | 1 | 0 | 3 | 0 | 1 | 2 | 3 | 2 | 2 | 3 | 4 | 0 | 1 | 4," | I expect those who are getting paid in the field already know everything they need to know from that review. But would be more useful for those entering the field to get an overview. Though I think it could be condensed a lot more. | Seems mostly irrelevant at this point. I dropped the book after 2 chapters. The waitbutwhy article covers what is necessary. Though I’d also add the key points from the rebuttals of Superintelligence. |  | It’s one of those things where you get what you put in. I think there’s a decent amount of people who do the AISC, but try to do it part-time, off the side of their desk. They don’t end up reaping the full benefits. I think it’s mostly only worth it if you focus much of your attention in the program. |  |  |  | It’s OK, but I don’t think it should be read from beginning to end. Should devote time to understanding jargon and trying to use it as a resource to answer questions rather than trying to read it from beginning to end. |  | There are some specific episodes that are useful like the Anthropic one. | I knew most of the stuff in the book and the stuff I didn’t know hasn’t been super useful yet. | ","Useful to have talks + Q&A. However, there’s a ton of variance in the quality of the talks. | Useful for getting tacit knowledge and getting feedback.","",Twitter,0,
7469441,43,,2022-06-25 18:42:54,2022-06-25 18:45:05,2.18,t6togir,0,,,"[""Rob Miles videos"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"Rob Miles videos,the 80,000 Hours podcast,the AI Alignment Newsletter,AGI Safety Fundamentals Course,conversations with AI alignment researchers at conferences",2 | 4 | 3 | 4 | 4,conversations with AI alignment researchers at conferences,3 | 4 | 3 | 4 | 4,0 | 2 | 1 | 0 | 4, |  |  | ,"","",Twitter :),,
7469548,43,,2022-06-25 18:50:15,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469565,43,,2022-06-25 18:51:33,,72.17,8d2bvx6,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the 80,000 Hours podcast"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""the AI Alignment Newsletter"",""the ARCHES agenda by Andrew Critch and David Krueger"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"Concrete Problems in AI Safety by Amodei et al,Unsolved Problems in ML Safety by Hendrycks et al,the 80,000 Hours podcast,AGI Safety Fundamentals Course,the FLI podcast,the AI Alignment Newsletter,the ARCHES agenda by Andrew Critch and David Krueger,AXRP - the AI X-risk Research Podcast,Rob Miles videos,talks by AI alignment researchers",,,,,,,,,,
7469578,43,,2022-06-25 18:52:28,2022-06-25 23:14:32,262.07,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the annual AI Alignment Literature Review and Charity Comparison"",""Life 3.0 (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""the ML Safety newsletter"",""the 80,000 Hours podcast"",""AXRP - the AI X-risk Research Podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""AGI Safety Fundamentals Course"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"conversations with AI alignment researchers at conferences,the annual AI Alignment Literature Review and Charity Comparison,Life 3.0 (book),the ARCHES agenda by Andrew Critch and David Krueger,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,the AI Alignment Newsletter,the ML Safety newsletter,the 80,000 Hours podcast,AXRP - the AI X-risk Research Podcast,Unsolved Problems in ML Safety by Hendrycks et al,AGI Safety Fundamentals Course,the FLI podcast",3 | 2 | 0 | 1 | 2 | 2 | 1 | 2 | 4 | 2 | 2 | 3 | 4 | 3 | 1,the FLI podcast,3 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 4 | 2 | 3 | 1 | 4 | 3 | 0,4 | 0 | 0 | 1 | 2 | 0 | 1 | 4 | 3 | 3 | 0 | 4 | 4 | 2 | 0,"I don’t think donating to most AIS organizations is valuable given incredible funding for the field. The report therefore seems based on a false premise, but still provides a useful overview of many relevant organizations. | Didn’t like it, didn’t finish it. | I didn’t find it very accessible, but only read maybe 10 or 20 pages before stopping. I should probably go back to it soon. | Good for academics and people who don’t respect the authority of blog posts. | I think the Eliezer worldview is theatrical and unlikely. Specifically I find fault in: recursive self improvement, the “narrow range of intelligence” argument, the focus on human evolutionary analogies. I find Paul Christiano’s “What Failure Looks Like” more representative of my views and useful for describing the outcomes  Superintelligence fails to describe. 

Separately, even if you agree with the Boston Yudkowsky worldview, the book is too long and very dense. I listened to the audiobook. |  | Incredible resource, very reliable. Would be nice if it covered greater diversity of topics, but understandably it’s mostly Rohin’s favorites. | Great analysis, but without regular publishing it’s not as good at getting someone hooked on reading about the field. | Great intro stuff, but not useful after that. | I really like podcasts, and this is my favorite on AI safety. I do wish Daniel tried to have more guests outside the EA bubble. The Gradient Podcast is a great alternative with a more diverse audience, though plenty of episodes are not about safety. | Clear, easy to read, provides tractable research directions. Not comprehensive of work in the field, but a great starting point for research. | I quit the discussion section as soon as I realized the other participants were curious laypeople and not closely studying to begin working in the field. I’d already read most
of the content in the syllabus, but it was still a good resource. 

The main benefit of AGISF for me was the Slack channel. It gave me access to hundreds of people interested in the topic, and showed me several opportunities that I applied for including my current full-time gig. I wish we had more Slack channels for broader groups of people interested in AI safety, but this one was incredible. | Honestly haven’t listened to it much.","EAGx Virtual was my only opportunity to do so. After years of online reading, it allowed me to ask basic fundamental questions about the field that I was still uncertain on. This helped me realize where others are still uncertain, and where there are good answers available. | The value is (a) learning about brand new papers that haven’t been published or discussed yet, and (b) getting to know the authors a little bit in a group setting before approaching them 1-1.","","",1,
7469627,43,,2022-06-25 18:57:20,2022-06-28 03:22:14,3384.90,t6togir,0,,,"[""the 80,000 Hours podcast"",""Human Compatible (book)"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,Human Compatible (book),The Alignment Problem (book)",3 | 3 | 4,The Alignment Problem (book),2 | 3 | 4,2 | 3 | 4, |  | ,,"","",,
7469656,43,,2022-06-25 18:59:47,,1.20,8d2bvx6,0,,,"[""The Alignment Problem (book)"",""Superintelligence (book)"",""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"The Alignment Problem (book),Superintelligence (book),talks by AI alignment researchers,the 80,000 Hours podcast,the AI Alignment Newsletter,AGI Safety Fundamentals Course",,,,,,,,,0,
7469758,43,,2022-06-25 19:07:30,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7469790,43,,2022-06-25 19:09:46,,0.78,8d2bvx6,0,,,"[""conversations with AI alignment researchers at conferences"",""the FLI podcast"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""Superintelligence (book)"",""Human Compatible (book)"",""Life 3.0 (book)"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",6,I spend some of my time publicly communicating about AI alignment,"conversations with AI alignment researchers at conferences,the FLI podcast,AGI Safety Fundamentals Course,talks by AI alignment researchers,Superintelligence (book),Human Compatible (book),Life 3.0 (book),the 80,000 Hours podcast,the AI Alignment Newsletter,The Alignment Problem (book)",,,,,,,,,,
7469798,43,,2022-06-25 19:10:15,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469862,43,,2022-06-25 19:15:43,2022-06-25 19:48:50,33.12,t6togir,0,,,"[""AGI Safety Fundamentals Course"",""the Embedded Agency sequence on the Alignment Forum"",""Life 3.0 (book)"",""conversations with AI alignment researchers at conferences"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""Human Compatible (book)"",""The Alignment Problem (book)"",""AXRP - the AI X-risk Research Podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the annual AI Alignment Literature Review and Charity Comparison"",""the FLI podcast"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the ML Safety newsletter"",""the Value Learning sequence on the Alignment Forum"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",8,I spend some of my time publicly communicating about AI alignment,"AGI Safety Fundamentals Course,the Embedded Agency sequence on the Alignment Forum,Life 3.0 (book),conversations with AI alignment researchers at conferences,the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book),the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al,Human Compatible (book),The Alignment Problem (book),AXRP - the AI X-risk Research Podcast,Unsolved Problems in ML Safety by Hendrycks et al,the annual AI Alignment Literature Review and Charity Comparison,the FLI podcast,the 80,000 Hours podcast,talks by AI alignment researchers,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the ML Safety newsletter,the Value Learning sequence on the Alignment Forum,Rob Miles videos",4 | 2 | 2 | 4 | 3 | 4 | 4 | 3 | 3 | 4 | 3 | 3 | 2 | 2 | 3 | 4 | 2 | 2 | 3 | 4,Rob Miles videos,4 | 1 | 1 | 4 | 2 | 2 | 3 | 1 | 1 | 4 | 2 | 3 | 1 | 2 | 3 | 3 | 1 | 2 | 2 | 4,4 | 2 | 1 | 4 | 4 | 3 | 4 | 2 | 1 | 3 | 3 | 3 | 2 | 2 | 2 | 4 | 2 | 2 | 4 | 3, |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | , | ,"",twitter of various AI alignment people,,
7469870,43,,2022-06-25 19:16:15,,0.02,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469896,43,,2022-06-25 19:18:28,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469911,43,,2022-06-25 19:19:40,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469914,43,,2022-06-25 19:19:56,,3.18,8d2bvx6,0,,,"[""The Alignment Problem (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Human Compatible (book)"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"The Alignment Problem (book),the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum,talks by AI alignment researchers,Human Compatible (book),AGI Safety Fundamentals Course,Rob Miles videos,the 80,000 Hours podcast,the AI Alignment Newsletter",,,,,,,,,,
7469929,43,,2022-06-25 19:21:46,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469947,43,,2022-06-25 19:22:54,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7469954,43,,2022-06-25 19:23:17,,2.53,now6hjo,0,,,"[""The Alignment Problem (book)"",""Human Compatible (book)"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""the Value Learning sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",5,I am trying to move into a technical AI alignment career,"The Alignment Problem (book),Human Compatible (book),talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,Superintelligence (book),the Value Learning sequence on the Alignment Forum,the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum",2 | 2 | 4 | 2 | 4 | 3 | 3 | 3,"the 80,000 Hours podcast",3 | 3 | 1 | 1 | 2 | 1 | 0 | 4,0 | 0 | 4 | 3 | 4 | 1 | 4 | 4, |  |  |  |  |  | ,"",,,,
7469970,43,,2022-06-25 19:24:33,2022-06-25 19:29:01,4.45,t6togir,0,,,"[""Superintelligence (book)"",""the 80,000 Hours podcast"",""The Alignment Problem (book)"",""Life 3.0 (book)"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",9,I am interested in AI alignment research,"Superintelligence (book),the 80,000 Hours podcast,The Alignment Problem (book),Life 3.0 (book),Human Compatible (book)",3 | 3 | 4 | 3 | 3,Human Compatible (book),4 | 3 | 4 | 3 | 3,3 | 3 | 4 | 3 | 3, |  |  |  | ,,"",Tweet,,
7470000,43,,2022-06-25 19:27:20,,0.07,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7470012,43,,2022-06-25 19:28:49,2022-06-25 19:51:32,22.72,t6togir,0,,,"[""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""the ML Safety newsletter"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",4,I am interested in AI alignment research,"the AI Alignment Newsletter,talks by AI alignment researchers,the ML Safety newsletter,the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,Rob Miles videos",2 | 2 | 2 | 3 | 2 | 2,Rob Miles videos,3 | 3 | 3 | 3 | 3 | 4,2 | 3 | 2 | 3 | 3 | 1," |  |  |  | Usually (but not always) I’ve seen most of the ideas in the video on alignment forum, but they are excellent introductions and summaries (and sometimes go into important things I’ve missed)","Im not sure quite what you mean by “how useful”, because I haven’t produced any alignment research, just read it. The main talks in remembering watching are the ones describing logical induction. I think Garrabrant was giving the talk but Idr.","I’m not sure the following is relevant to this survey, but: It might be cool if there were a repository for math conjectures/open-questions that people doing alignment research would benefit from having solved, but which can be stated and approached without reference to alignment, for people who aren’t alignment researchers but are doing math research, could tackle? But I don’t know if setting up such a repository would be worth the costs of doing so. (Or if there are even enough such problems to do so? Also what if attacking such problems without context could risk contributing to capabilities?)",Twitter,,
7470024,43,,2022-06-25 19:30:21,2022-06-26 05:05:18,574.95,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""Human Compatible (book)"",""the Value Learning sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""the FLI podcast"",""the 80,000 Hours podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""AGI Safety Fundamentals Course"",""The Alignment Problem (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the Iterated Amplification sequence on the Alignment Forum"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"the Embedded Agency sequence on the Alignment Forum,Human Compatible (book),the Value Learning sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,Superintelligence (book),AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,the FLI podcast,the 80,000 Hours podcast,the ARCHES agenda by Andrew Critch and David Krueger,AGI Safety Fundamentals Course,The Alignment Problem (book),Concrete Problems in AI Safety by Amodei et al,the Iterated Amplification sequence on the Alignment Forum,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda"")",2 | 1 | 3 | 4 | 4 | 1 | 1 | 1 | 2 | 2 | 1 | 1 | 1 | 2 | 1,"Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda"")",1 | 1 | 3 | 4 | 3 | 1 | 1 | 1 | 2 | 1 | 4 | 1 | 1 | 2 | 1,3 | 1 | 4 | 4 | 2 | 1 | 1 | 1 | 1 | 2 | 1 | 1 | 1 | 3 | 2, |  |  |  |  |  |  |  |  |  |  |  | , | ,"","",,
7470067,43,,2022-06-25 19:34:59,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470069,43,,2022-06-25 19:35:01,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470146,43,,2022-06-25 19:43:17,2022-06-25 19:45:32,2.25,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",1,I am interested in AI alignment research,"",,,,,,,Alignement is really about prompt design which I didn't see mentioned in the previous list.,The blue bird,,
7470171,43,,2022-06-25 19:46:20,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470179,43,,2022-06-25 19:46:49,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470272,43,,2022-06-25 19:56:29,,5.25,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7470309,43,,2022-06-25 20:02:22,2022-06-25 20:04:36,2.22,t6togir,0,,,"[""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Rob Miles videos"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",13,I spend some of my time publicly communicating about AI alignment,"the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,Rob Miles videos,Superintelligence (book)",2 | 2 | 3 | 3,Superintelligence (book),3 | 4 | 4 | 4,3 | 1 | 2 | 4, |  | ,"","","",,
7470328,43,,2022-06-25 20:04:22,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470450,43,,2022-06-25 20:17:50,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470455,43,,2022-06-25 20:18:34,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470518,43,,2022-06-25 20:23:52,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470561,43,,2022-06-25 20:28:46,2022-06-25 20:30:34,1.80,t6togir,0,,,"[""the 80,000 Hours podcast"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I have heard of AI alignment,"[""I have heard of AI alignment""]",,I have heard of AI alignment,"the 80,000 Hours podcast,the FLI podcast",2 | 3,the FLI podcast,2 | 3,0 | 4, | ,,I also listen to Sam Harris podcast on this topic,Twitter,,
7470585,43,,2022-06-25 20:31:16,,0.42,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",5,I am interested in AI alignment research,,,,,,,,,,,
7470665,43,,2022-06-25 20:39:35,,7118.45,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7470917,43,,2022-06-25 21:03:08,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7470931,43,,2022-06-25 21:04:31,2022-06-25 21:05:13,0.68,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,"","",,
7471009,43,,2022-06-25 21:11:56,2022-06-25 21:19:29,7.53,t6togir,0,,,"[""the 80,000 Hours podcast"",""Life 3.0 (book)"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""Rob Miles videos"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)"",""the FLI podcast"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",4,I am paid to work on technical AI alignment research,"the 80,000 Hours podcast,Life 3.0 (book),Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,Rob Miles videos,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book),the FLI podcast,AXRP - the AI X-risk Research Podcast",3 | 1 | 3 | 4 | 4 | 4 | 2 | 2 | 4 | 3 | 3,AXRP - the AI X-risk Research Podcast,1 | 0 | 0 | 0 | 4 | 4 | 0 | 0 | 1 | 2 | 2,2 | 0 | 3 | 4 | 4 | 4 | 3 | 1 | 1 | 2 | 2,"Less technical than other podcasts (which definitely fills a niche). Podcasts are quite long so better for deep listening as opposed to generating interest. | Good first chapter about how things can go wrong. Basically nothing relevant after that. | Very technical. Not entry level stuff. | I consider supporting his Patreon very effective giving! NEEDS MORE SCALE. How can we get these videos in front of 100+ million people? | Interesting agenda. Not entry level stuff though. Think you already have to be sold on the alignment problem to engage with it. | I personally find PC's stuff very hard to read for whatever reason. I have found others who distill his stuff to be very, very useful. | First thing that got me interested in safety! However, it's a bit dated now and probably better resources out there. |  | ",High variance. | ,"",Alex Lawson tweet,1,
7471033,43,,2022-06-25 21:13:55,,6.40,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7471257,43,,2022-06-25 21:36:06,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7471274,43,,2022-06-25 21:37:46,2022-06-25 21:48:09,10.38,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""Rob Miles videos"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""AI Safety Camp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time publicly communicating about AI alignment""]",3,I spend some of my time publicly communicating about AI alignment,"conversations with AI alignment researchers at conferences,Rob Miles videos,AGI Safety Fundamentals Course,Superintelligence (book),AI Safety Camp",3 | 1 | 3 | 2 | 4,AI Safety Camp,2 | 3 | 3 | 3 | 3,4 | 2 | 3 | 1 | 0," | It's a very useful resource I think (haven't read all). It's kind of in between these levels. | If you're getting paid as AI alignment researcher, you don't need this recommendation anymore. For beginners, I find it useful, but it also turns some people off due to futuristic language and some not so mainstream (or kind) world views.. Would depend on the person whether I'd recommend. | I found it very useful, not so much as a resource, but more to talk to people, gauge ideas and opinions, and build a bit of a network. It also gets people to take a next step","At EAG, I've talked to a few DeepMind people, one from the safety team, that was quite interesting. Also met David Krueger there.","",twitter,,
7471510,43,,2022-06-25 22:01:31,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471584,43,,2022-06-25 22:12:41,,0.27,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7471594,43,,2022-06-25 22:14:03,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471635,43,,2022-06-25 22:18:59,2022-06-25 22:21:19,2.33,t6togir,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,Superintelligence (book),the 80,000 Hours podcast",3 | 2 | 2 | 2,"the 80,000 Hours podcast",3 | 3 | 3 | 2,2 | 1 | 1 | 1, |  | ,"","","",,
7471649,43,,2022-06-25 22:21:03,,4.32,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7471654,43,,2022-06-25 22:21:30,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471680,43,,2022-06-25 22:25:27,2022-06-25 22:29:30,4.05,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""Human Compatible (book)"",""Rob Miles videos"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",4,I am paid to work on technical AI alignment research,"AXRP - the AI X-risk Research Podcast,Human Compatible (book),Rob Miles videos,Superintelligence (book),the AI Alignment Newsletter,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum",3 | 1 | 2 | 3 | 3 | 3 | 2,the Embedded Agency sequence on the Alignment Forum,3 | 4 | 4 | 2 | 0 | 1 | 0,4 | 0 | 1 | 2 | 4 | 3 | 3, |  |  |  | It appears to have stopped being updated so idk if it’s relevant anymore | ,"","",Twitter,0,
7471730,43,,2022-06-25 22:31:40,2022-06-25 22:34:47,3.12,t6togir,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",9,I spend some of my time solving technical problems related to AI alignment,"talks by AI alignment researchers,Superintelligence (book),the 80,000 Hours podcast,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter",2 | 3 | 2 | 3 | 2,the AI Alignment Newsletter,1 | 4 | 2 | 0 | 0,3 | 1 | 1 | 4 | 3, |  |  | ,"","","",,
7471893,43,,2022-06-25 22:55:38,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471894,43,,2022-06-25 22:56:00,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471914,43,,2022-06-25 23:00:02,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471956,43,,2022-06-25 23:05:26,,0.33,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7471967,43,,2022-06-25 23:06:45,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472059,43,,2022-06-25 23:23:32,2022-06-25 23:29:39,6.10,t6togir,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",4,I am paid to work on technical AI alignment research,"talks by AI alignment researchers,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast",2 | 1 | 3 | 1,"the 80,000 Hours podcast",0 | 0 | 3 | 2,1 | 1 | 3 | 1, |  | ,"","",twitter,1,
7472060,43,,2022-06-25 23:23:37,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472102,43,,2022-06-25 23:33:19,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472264,43,,2022-06-26 00:06:21,,0.10,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7472299,43,,2022-06-26 00:12:00,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472300,43,,2022-06-26 00:12:01,,3.22,auqgbpq,0,,,"[""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""the Embedded Agency sequence on the Alignment Forum"",""the Iterated Amplification sequence on the Alignment Forum"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the 80,000 Hours podcast,the AI Alignment Newsletter,the Embedded Agency sequence on the Alignment Forum,the Iterated Amplification sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast",1 | 2 | 3 | 3 | 4,AXRP - the AI X-risk Research Podcast,0 | 0 | 4 | 2 | 2,0 | 0 | 1 | 0 | 2, |  |  |  | ,,"",,0 | 0,
7472321,43,,2022-06-26 00:16:40,,4309.27,eb2kjj8,0,,,"[""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",,[],,,,,Superintelligence (book),,,,,,,,
7472367,43,,2022-06-26 00:28:46,,40.25,now6hjo,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Rob Miles videos"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building","[""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building""]",,I spend some of my time doing AI alignment field/community-building,"Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,Rob Miles videos,the AI Alignment Newsletter",3 | 1,AGI Safety Fundamentals Course,3 | 1,4 | 3, | ,,,,,
7472444,43,,2022-06-26 00:45:39,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472502,43,,2022-06-26 00:55:10,,6.90,auqgbpq,0,,,"[""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""conversations with AI alignment researchers at conferences"",""AIRCS workshops"",""Concrete Problems in AI Safety by Amodei et al"",""the annual AI Alignment Literature Review and Charity Comparison"",""the Embedded Agency sequence on the Alignment Forum"",""Human Compatible (book)"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",5,I am paid to work on technical AI alignment research,"talks by AI alignment researchers,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,the ARCHES agenda by Andrew Critch and David Krueger,conversations with AI alignment researchers at conferences,AIRCS workshops,Concrete Problems in AI Safety by Amodei et al,the annual AI Alignment Literature Review and Charity Comparison,the Embedded Agency sequence on the Alignment Forum,Human Compatible (book),Superintelligence (book)",3 | 3 | 3 | 4 | 4 | 3 | 2 | 2 | 1 | 1 | 3,Superintelligence (book),0 | 3 | 0 | 4 | 0 | 4 | 1 | 1 | 0 | 2 | 4,2 | 4 | 4 | 3 | 4 | 1 | 0 | 1 | 1 | 0 | 1, |  |  |  |  |  |  |  | ,"Rohin Shah, John Wentworth, Andrew Critch | Andrew Crirch, Nate Soares, Eliezer Yudkowsky, Ramana Kumar, John Wentworth, Rohin Shah, Anna Salamon","",,1,
7472669,43,,2022-06-26 01:33:47,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472670,43,,2022-06-26 01:33:48,2022-06-26 01:38:10,4.35,t6togir,0,,,"[""Rob Miles videos"",""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""Superintelligence (book)"",""the ML Safety newsletter"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"Rob Miles videos,talks by AI alignment researchers,the 80,000 Hours podcast,Superintelligence (book),the ML Safety newsletter,The Alignment Problem (book)",3 | 3 | 2 | 1 | 1 | 2,The Alignment Problem (book),2 | 1 | 0 | 1 | 1 | 2,2 | 1 | 0 | 1 | 1 | 1, |  |  |  | ,"","","",,
7472700,43,,2022-06-26 01:39:51,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472708,43,,2022-06-26 01:40:58,,0.27,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7472770,43,,2022-06-26 01:55:59,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7472890,43,,2022-06-26 02:21:11,2022-06-26 02:34:16,13.08,t6togir,0,,,"[""Superintelligence (book)"",""Human Compatible (book)"",""the AI Alignment Newsletter"",""Life 3.0 (book)"",""the 80,000 Hours podcast"",""the FLI podcast"",""talks by AI alignment researchers"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",6,I am interested in AI alignment research,"Superintelligence (book),Human Compatible (book),the AI Alignment Newsletter,Life 3.0 (book),the 80,000 Hours podcast,the FLI podcast,talks by AI alignment researchers,Rob Miles videos",3 | 3 | 2 | 2 | 3 | 4 | 2 | 3,Rob Miles videos,4 | 4 | 4 | 3 | 3 | 3 | 3 | 3,4 | 4 | 4 | 3 | 3 | 3 | 3 | 3, |  |  |  |  |  | ,"",Thanks for doing this. There were a lot of resources that I wasn't aware of that I've now added to my list.,Link tweeted by David Krueger.,,
7472904,43,,2022-06-26 02:23:45,2022-06-26 02:31:14,7.47,t6togir,0,,,"[""The Alignment Problem (book)"",""the FLI podcast"",""the 80,000 Hours podcast"",""Superintelligence (book)"",""Human Compatible (book)"",""conversations with AI alignment researchers at conferences"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",7,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"The Alignment Problem (book),the FLI podcast,the 80,000 Hours podcast,Superintelligence (book),Human Compatible (book),conversations with AI alignment researchers at conferences,talks by AI alignment researchers",2 | 3 | 3 | 3 | 2 | 3 | 2,talks by AI alignment researchers,2 | 3 | 4 | 2 | 1 | 3 | 2,1 | 3 | 2 | 1 | 1 | 4 | 2, |  |  |  | , | ,"",Twitter,,
7473010,43,,2022-06-26 02:50:04,,0.67,u166gt1,0,,,,,,"I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",,,,,,,,,,,,,
7473017,43,,2022-06-26 02:51:09,2022-06-26 02:53:45,2.60,t6togir,0,,,"[""Superintelligence (book)"",""The Alignment Problem (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",0,I am interested in AI alignment research,"Superintelligence (book),The Alignment Problem (book),the 80,000 Hours podcast",4 | 3 | 4,"the 80,000 Hours podcast",4 | 4 | 4,2 | 1 | 4, |  | ,,"",Twitter,,
7473054,43,,2022-06-26 02:58:09,,0.13,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7473070,43,,2022-06-26 03:00:44,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473090,43,,2022-06-26 03:04:26,,1.50,8d2bvx6,0,,,"[""conversations with AI alignment researchers at conferences"",""Life 3.0 (book)"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",7,I spend some of my time solving technical problems related to AI alignment,"conversations with AI alignment researchers at conferences,Life 3.0 (book),Superintelligence (book),the 80,000 Hours podcast,talks by AI alignment researchers",,,,,,,,,,
7473134,43,,2022-06-26 03:08:44,,1.03,now6hjo,0,,,"[""the 80,000 Hours podcast"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I have heard of AI alignment,"[""I have heard of AI alignment""]",,I have heard of AI alignment,"the 80,000 Hours podcast,Rob Miles videos",1,"the 80,000 Hours podcast",0,0,I love 80k's podcast but don't find it useful as an AIS resource (although I may simply not be listening to the right episodes),,,,,
7473143,43,,2022-06-26 03:12:16,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473148,43,,2022-06-26 03:13:59,,0.13,u166gt1,0,,,,,,"",[],,,,,,,,,,,,,
7473171,43,,2022-06-26 03:20:17,,54.60,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473344,43,,2022-06-26 04:03:58,,0.32,u166gt1,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",,,,,,,,,,,,,
7473347,43,,2022-06-26 04:04:34,2022-06-26 04:05:59,1.40,t6togir,0,,,"[""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",1,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)",Superintelligence (book),1,Superintelligence (book),1,0,"",,"",twitter,,
7473354,43,,2022-06-26 04:06:55,,1.92,eb2kjj8,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"",,,,,,,,,,
7473358,43,,2022-06-26 04:10:36,,4.40,8d2bvx6,0,,,"[""Superintelligence (book)"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""Life 3.0 (book)"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Superintelligence (book),the 80,000 Hours podcast,the AI Alignment Newsletter,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,Life 3.0 (book),Human Compatible (book)",,,,,,,,,,
7473374,43,,2022-06-26 04:16:17,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473388,43,,2022-06-26 04:18:34,,18579.13,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7473420,43,,2022-06-26 04:33:53,,0.28,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7473423,43,,2022-06-26 04:34:27,,0.50,e5pk481,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",.5,I am interested in AI alignment research,,,,,,,,,,,
7473483,43,,2022-06-26 04:51:28,,3.70,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7473618,43,,2022-06-26 05:28:40,,1.03,72erk1d,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""the FLI podcast"",""conversations with AI alignment researchers at conferences"",""talks by AI alignment researchers"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,the FLI podcast,conversations with AI alignment researchers at conferences,talks by AI alignment researchers,Unsolved Problems in ML Safety by Hendrycks et al",,,,,,,,,,
7473633,43,,2022-06-26 05:33:29,2022-06-26 06:20:27,46.97,t6togir,0,,,"[""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",4,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,talks by AI alignment researchers,the AI Alignment Newsletter",3 | 3 | 2,the AI Alignment Newsletter,4 | 3 | 2,4 | 4 | 2, | ,"","","",,
7473636,43,,2022-06-26 05:33:40,,0.02,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473643,43,,2022-06-26 05:36:18,,0.48,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,,,,,,,,,,,
7473689,43,,2022-06-26 05:51:46,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473722,43,,2022-06-26 06:04:08,,0.10,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7473723,43,,2022-06-26 06:04:25,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473883,43,,2022-06-26 07:04:20,,0.30,u166gt1,0,,,,,,I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,,,,,,,,,,,,
7473885,43,,2022-06-26 07:05:46,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7473916,43,,2022-06-26 07:15:22,,0.10,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7473922,43,,2022-06-26 07:17:39,,3.82,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7474015,43,,2022-06-26 07:47:37,2022-06-26 08:01:17,13.65,t6togir,0,,,"[""Life 3.0 (book)"",""Superintelligence (book)"",""talks by AI alignment researchers"",""The Alignment Problem (book)"",""Rob Miles videos"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",6,I am interested in AI alignment research,"Life 3.0 (book),Superintelligence (book),talks by AI alignment researchers,The Alignment Problem (book),Rob Miles videos,Human Compatible (book)",2 | 3 | 3 | 2 | 4 | 2,Human Compatible (book),1 | 2 | 3 | 1 | 4 | 1,0 | 0 | 3 | 0 | 3 | 0, |  |  |  | ,"", | ,"",,
7474041,43,,2022-06-26 07:57:06,,1.53,8d2bvx6,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""AIRCS workshops"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum"",""talks by AI alignment researchers"",""the FLI podcast"",""conversations with AI alignment researchers at conferences"",""AI Safety Camp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",4,I spend some of my time solving technical problems related to AI alignment,"the Iterated Amplification sequence on the Alignment Forum,AIRCS workshops,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum,talks by AI alignment researchers,the FLI podcast,conversations with AI alignment researchers at conferences,AI Safety Camp",,,,,,,,,,
7474277,43,,2022-06-26 09:44:28,,1.75,now6hjo,0,,,"[""The Alignment Problem (book)"",""AI Safety Camp"",""talks by AI alignment researchers"",""the Machine Learning for Alignment Bootcamp"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"The Alignment Problem (book),AI Safety Camp,talks by AI alignment researchers,the Machine Learning for Alignment Bootcamp,the AI Alignment Newsletter,AGI Safety Fundamentals Course,Superintelligence (book),AXRP - the AI X-risk Research Podcast,the Embedded Agency sequence on the Alignment Forum,Rob Miles videos,the 80,000 Hours podcast,conversations with AI alignment researchers at conferences",1,The Alignment Problem (book),1,0,"",,,,,
7474356,43,,2022-06-26 10:12:15,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7474397,43,,2022-06-26 10:26:50,,1.53,now6hjo,0,,,"[""The Alignment Problem (book)"",""Life 3.0 (book)"",""AI Safety Camp"",""Superintelligence (book)"",""the ML Safety newsletter"",""AGI Safety Fundamentals Course"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time publicly communicating about AI alignment,"[""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"The Alignment Problem (book),Life 3.0 (book),AI Safety Camp,Superintelligence (book),the ML Safety newsletter,AGI Safety Fundamentals Course,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast",1 | 2 | 2 | 2 | 2 | 2 | 2,the AI Alignment Newsletter,2 | 1 | 2 | 2 | 2 | 2 | 2,2 | 2 | 2 | 2 | 2 | 2 | 2, |  |  |  |  |  | ,,,,,
7474453,43,,2022-06-26 10:49:54,,1.43,8d2bvx6,0,,,"[""Rob Miles videos"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time publicly communicating about AI alignment""]",2,I spend some of my time publicly communicating about AI alignment,"Rob Miles videos,Superintelligence (book)",,,,,,,,,,
7474564,43,,2022-06-26 11:29:16,,4.78,now6hjo,0,,,"[""conversations with AI alignment researchers at conferences"",""AIRCS workshops"",""talks by AI alignment researchers"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",11,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Superintelligence (book),AIRCS workshops,talks by AI alignment researchers,conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al | AIRCS workshops,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),conversations with AI alignment researchers at conferences,talks by AI alignment researchers | conversations with AI alignment researchers at conferences,AIRCS workshops,talks by AI alignment researchers,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al",2,conversations with AI alignment researchers at conferences,1,3,,"",,,,
7474577,43,,2022-06-26 11:34:12,2022-06-26 11:38:14,4.02,t6togir,0,,,"[""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""the FLI podcast"",""Life 3.0 (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",8,I am interested in AI alignment research,"Superintelligence (book),AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,the FLI podcast,Life 3.0 (book)",3 | 2 | 4 | 3 | 2,Life 3.0 (book),3 | 2 | 4 | 4 | 3,3 | 3 | 2 | 2 | 1, |  |  |  | ,,"","",,
7474585,43,,2022-06-26 11:37:50,,1.20,72erk1d,0,,,"[""Human Compatible (book)"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course"",""AXRP - the AI X-risk Research Podcast"",""Concrete Problems in AI Safety by Amodei et al"",""AI Safety Camp"",""Rob Miles videos"",""the Value Learning sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""the ARCHES agenda by Andrew Critch and David Krueger"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the Embedded Agency sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time doing AI alignment field/community-building,"[""I spend some of my time doing AI alignment field/community-building""]",,I spend some of my time doing AI alignment field/community-building,"Human Compatible (book),the AI Alignment Newsletter,AGI Safety Fundamentals Course,AXRP - the AI X-risk Research Podcast,Concrete Problems in AI Safety by Amodei et al,AI Safety Camp,Rob Miles videos,the Value Learning sequence on the Alignment Forum,the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,the ARCHES agenda by Andrew Critch and David Krueger,talks by AI alignment researchers,Superintelligence (book),the Embedded Agency sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),the FLI podcast",,,,,,,,,,
7474608,43,,2022-06-26 11:45:08,2022-06-26 11:48:33,3.40,t6togir,0,,,"[""Rob Miles videos"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"Rob Miles videos,Superintelligence (book),AGI Safety Fundamentals Course,the AI Alignment Newsletter,talks by AI alignment researchers,conversations with AI alignment researchers at conferences",1 | 3 | 4 | 4 | 1 | 4,conversations with AI alignment researchers at conferences,2 | 3 | 4 | 4 | 1 | 2,0 | 0 | 0 | 2 | 1 | 4, |  |  | , | ,"",Twitter,,
7474612,43,,2022-06-26 11:45:52,2022-06-27 01:14:03,808.18,t6togir,0,,,"[""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""AGI Safety Fundamentals Course"",""Life 3.0 (book)"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison"",""Superintelligence (book)"",""the Value Learning sequence on the Alignment Forum"",""AI Safety Camp"",""The Alignment Problem (book)"",""talks by AI alignment researchers"",""the FLI podcast"",""Human Compatible (book)"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",8,I spend some of my time publicly communicating about AI alignment,"AI Safety Camp,the FLI podcast,the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),Life 3.0 (book),AIRCS workshops,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,The Alignment Problem (book),AGI Safety Fundamentals Course,the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,Human Compatible (book),the Value Learning sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,Rob Miles videos,the 80,000 Hours podcast | Rob Miles videos,the AI Alignment Newsletter,Human Compatible (book),Concrete Problems in AI Safety by Amodei et al,The Alignment Problem (book),Life 3.0 (book),the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the FLI podcast,AGI Safety Fundamentals Course,the Value Learning sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,AI Safety Camp,Superintelligence (book),talks by AI alignment researchers,the annual AI Alignment Literature Review and Charity Comparison | Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),AGI Safety Fundamentals Course,Life 3.0 (book),the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),the Value Learning sequence on the Alignment Forum,AI Safety Camp,The Alignment Problem (book),talks by AI alignment researchers,the FLI podcast,Human Compatible (book),the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,Rob Miles videos",3 | 1 | 3 | 2 | 1 | 4 | 1 | 2 | 3 | 4 | 4 | 1 | 2 | 3 | 4 | 1 | 4 | 1 | 1 | 3 | 2 | 2 | 3 | 3 | 2 | 1 | 3 | 1 | 2 | 3 | 1 | 4,Rob Miles videos,1 | 2 | 0 | 1 | 1 | 4 | 2 | 3 | 2 | 4 | 4 | 2 | 3 | 2 | 4 | 2 | 4 | 1 | 1 | 0 | 1 | 2 | 1 | 4 | 3 | 2 | 3 | 2 | 2 | 2 | 1 | 4,0 | 0 | 4 | 0 | 0 | 1 | 2 | 2 | 3 | 1 | 1 | 2 | 2 | 3 | 1 | 2 | 1 | 0 | 3 | 4 | 0 | 2 | 0 | 2 | 1 | 1 | 1 | 1 | 2 | 3 | 3 | 4,"AISC fits right in the middle: an excellent resource once you have read some stuff in the space and is willing to dive much deeper; by the time you're paid, it has served its purpose. | They're not always talking about AI alignment of course, but when they are, the content is accessible, thought not the most concise. | Not an introductory resource (aside from the ""look at all the activity"" effect), but an excellent thing to browse to fill your gaps about the players in the field. Even though it doesn't include independent researchers as far as I remember, it's always a good idea to check it out, plus, it's only once a year. | Used to be the best and foremost introduction to the field, but now... it's a bit dated and unfocused, though most of the arguments hold up. The field has moved quickly. Not the resource I'd recommend now. | Same as Superintelligence, a bit dated now, though much more accessible. Not the resource I'd recommend for technical profile, though a good book for a popular audience. | Nearly the best curriculum broadly available, though it's lengthy and suffers from a few blind spots, that you can discuss anyway if you have a knowledgeable facilitator and/or if you engage with the group chats. If you're paid to do research you will not benefit much from it, tough I do recommend facilitating to train your communication skills! | Yeah, sure. It belongs in the standard reading list, it's not the most accessible thing, it's not a broad intro, it's worth reading at least once. | A classic one, though I'd only recommend the beginning of the sequence to newcomers. Introduces a fair number of concepts and subproblems. Even for the pros, it's an excellent resource. | An excellent broader view of alignment. Technical AI alignment as a field is mostly concerned by the latter third of the book, but it really helps with the junction between the AI ethics, AI governance, AI alignment, with examples and a nice progression. Not useful for the technical content (there is hardly any in there), but it captures many of the more mainstream concerns about AI systems, worth knowing. |  | Yeah, sure. It belongs in the standard reading list, it's not the most accessible thing, it's not a broad intro, it's worth reading at least once. | A classic one, though I'd only recommend the beginning of the sequence to newcomers. Introduces a fair number of concepts and subproblems. Even for the pros, it's an excellent resource. | An excellent broader view of alignment. Technical AI alignment as a field is mostly concerned by the latter third of the book, but it really helps with the junction between the AI ethics, AI governance, AI alignment, with examples and a nice progression. Not useful for the technical content (there is hardly any in there), but it captures many of the more mainstream concerns about AI systems, worth knowing. | Yeah, sure. It belongs in the standard reading list, it's not the most accessible thing, it's not a broad intro, it's worth reading at least once. | Nearly the best curriculum broadly available, though it's lengthy and suffers from a few blind spots, that you can discuss anyway if you have a knowledgeable facilitator and/or if you engage with the group chats. If you're paid to do research you will not benefit much from it, tough I do recommend facilitating to train your communication skills! | Same as Superintelligence, a bit dated now, though much more accessible. Not the resource I'd recommend for technical profile, though a good book for a popular audience. | Useful to get some glimpses of the field for newcomers but not an introductory resource. A good one to check to fill some gaps even for established researchers. | Not an introductory resource (aside from the ""look at all the activity"" effect), but an excellent thing to browse to fill your gaps about the players in the field. Even though it doesn't include independent researchers as far as I remember, it's always a good idea to check it out, plus, it's only once a year. | Used to be the best and foremost introduction to the field, but now... it's a bit dated and unfocused, though most of the arguments hold up. The field has moved quickly. Not the resource I'd recommend now. | Also belongs in the standard reading list, rather accessible, not a broad intro, it's worth reading at least once. | AISC fits right in the middle: an excellent resource once you have read some stuff in the space and is willing to dive much deeper; by the time you're paid, it has served its purpose. | An excellent broader view of alignment. Technical AI alignment as a field is mostly concerned by the latter third of the book, but it really helps with the junction between the AI ethics, AI governance, AI alignment, with examples and a nice progression. Not useful for the technical content (there is hardly any in there), but it captures many of the more mainstream concerns about AI systems, worth knowing. | They're not always talking about AI alignment of course, but when they are, the content is accessible, thought not the most concise. | A very nice introduction to the main problems, though not very technical not useful to established researchers, except as an example of well-managed communication. Has some gaps, still well worth a read. | Like other podcasts, not the most concise. Some interviews can go into a lot of depth and show arguments not displayed well elsewhere, which is nice! | As with the others: a standard read, worth reading at least once, not exactly an introduction. | A classic one, though I'd only recommend the beginning of the sequence to newcomers. Introduces a fair number of concepts and subproblems. Even for the pros, it's an excellent resource. | Skip the paper if you're a newcomer, unless you're familiar with ML, prefer Rob Miles's videos about the paper. A paper that every AI alignment researcher should have read at least once, it's an excellent baseline to compare more recent efforts to. | There isn't enough of these videos, they're mostly excellent as introductions to the field, and an excellent example of how the technical ideas can be broadcast to more people. I recommend them to everyone.","The two I have in mind is Yudkowsky's ""AI alignment: Why it's hard and where to start"", old but good, Christiano's ""Current work in AI alignment"" and more recently Krakovna's ""Paradigms of AI alignment: components and enablers"" (see comments on the LW post).

Basically not the most concise content, but it's always a good thing to hear some concepts explained through something other than text. |  | Thinking of Yudkowsky ""AI alignment: why it's hard and where to start"", Christiano's ""Current problems in AI alignment"" and more recently Krakovna's ""Paradigms of AI alignment"" (check the comments in the LW post). Always nice to have live explanations, something other than text. Not the most concise but refreshing.","This took way more than 5-10 minutes, hope I didn't mess up the form by going back and forth, if there's any problem please contact [REDACTED] on [REDACTED].
Also please add Ngo's ""AI safety from first principles"", at least.",First on Twitter!,,
7474737,43,,2022-06-26 12:23:28,,0.15,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7474833,43,,2022-06-26 13:07:23,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7474928,43,,2022-06-26 13:31:08,2022-06-26 13:32:19,1.23,auqgbpq,0,,,"[""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am paid to work on technical AI alignment research""]",13,I am paid to work on technical AI alignment research,Superintelligence (book),1,Superintelligence (book),2,2,"",,"","",1,
7475037,43,,2022-06-26 13:47:43,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7475099,43,,2022-06-26 14:00:19,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7475218,43,,2022-06-26 14:25:35,,1.37,8d2bvx6,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""the annual AI Alignment Literature Review and Charity Comparison"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""AIRCS workshops"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""AI Safety Camp"",""conversations with AI alignment researchers at conferences"",""Human Compatible (book)"",""the ARCHES agenda by Andrew Critch and David Krueger""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the Embedded Agency sequence on the Alignment Forum,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,the annual AI Alignment Literature Review and Charity Comparison,the AI Alignment Newsletter,Superintelligence (book),AIRCS workshops,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),AI Safety Camp,conversations with AI alignment researchers at conferences,Human Compatible (book),the ARCHES agenda by Andrew Critch and David Krueger",,,,,,,,,2,
7475307,43,,2022-06-26 14:47:04,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7475348,43,,2022-06-26 14:59:41,,0.67,72erk1d,0,,,"[""the 80,000 Hours podcast"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""the Machine Learning for Alignment Bootcamp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"the 80,000 Hours podcast,Superintelligence (book),AGI Safety Fundamentals Course,the Machine Learning for Alignment Bootcamp",,,,,,,,,,
7475773,43,,2022-06-26 17:09:51,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7475797,43,,2022-06-26 17:17:38,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7475868,43,,2022-06-26 17:43:00,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7476115,43,,2022-06-26 19:25:13,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476521,43,,2022-06-26 21:57:33,,2723.48,8d2bvx6,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""the Machine Learning for Alignment Bootcamp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I am paid to work on technical AI alignment research","[""I am trying to move into a technical AI alignment career"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"Unsolved Problems in ML Safety by Hendrycks et al,the Machine Learning for Alignment Bootcamp",,,,,,,,,6,
7476537,43,,2022-06-26 22:02:20,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR113KgscT8nmHD1l2k7MMTMxQ10rAHFKhm8ah9L8IowqwSV4Q2LlPPKQPI
7476548,43,,2022-06-26 22:06:10,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476550,43,,2022-06-26 22:06:25,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476565,43,,2022-06-26 22:13:13,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476567,43,,2022-06-26 22:13:45,,0.98,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,,,,,,,,,,,
7476585,43,,2022-06-26 22:22:27,2022-06-26 22:28:21,5.88,t6togir,0,,,"[""AIRCS workshops"",""the Iterated Amplification sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Life 3.0 (book)"",""AXRP - the AI X-risk Research Podcast"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",6,I am trying to move into a technical AI alignment career,"conversations with AI alignment researchers at conferences,talks by AI alignment researchers,Life 3.0 (book),the Embedded Agency sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the AI Alignment Newsletter,the Iterated Amplification sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast,AIRCS workshops,the 80,000 Hours podcast,Superintelligence (book) | AIRCS workshops,the Iterated Amplification sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,Life 3.0 (book),AXRP - the AI X-risk Research Podcast,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,the 80,000 Hours podcast",2 | 2 | 3 | 2 | 0 | 3 | 1 | 1 | 3 | 2,"the 80,000 Hours podcast",3 | 3 | 2 | 2 | 2 | 3 | 2 | 0 | 3 | 4,3 | 3 | 3 | 2 | 0 | 4 | 1 | 1 | 4 | 4, |  |  |  |  |  |  |  | ,"","",Facebook,,IwAR183EkpQ7Z7RXRNzfVNLGsvy59fUvB_ywLaWYfPePnBmBfW9HiuTbOVnKM
7476589,43,,2022-06-26 22:24:12,2022-07-02 17:26:50,8342.63,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""the Value Learning sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment","[""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,the AI Alignment Newsletter,the Value Learning sequence on the Alignment Forum",3 | 2 | 3 | 1,the Value Learning sequence on the Alignment Forum,2 | 4 | 4 | 1,4 | 4 | 4 | 1,Found the podcast a very good way to hear experts current thoughts. |  | bit out of date now,Unfortunately these talks are few and far between. Would like them to be more readily available.,"","",,IwAR0hZ8K-C-OgeUfM8K307k-qYKOHVnHypE2Pn6WQZSkLZ2oT6gDI3aKEDPk
7476634,43,,2022-06-26 22:47:19,,3.22,eb2kjj8,0,,,"[""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""conversations with AI alignment researchers at conferences"",""the Iterated Amplification sequence on the Alignment Forum"",""Rob Miles videos"",""AIRCS workshops"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""the Machine Learning for Alignment Bootcamp"",""the 80,000 Hours podcast"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",19,I spend some of my time publicly communicating about AI alignment,"Superintelligence (book),AXRP - the AI X-risk Research Podcast,conversations with AI alignment researchers at conferences,the Iterated Amplification sequence on the Alignment Forum,Rob Miles videos,AIRCS workshops,talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,the Machine Learning for Alignment Bootcamp,the 80,000 Hours podcast,the annual AI Alignment Literature Review and Charity Comparison",,,,,,,,,,
7476677,43,,2022-06-26 23:10:11,2022-06-26 23:23:54,13.70,t6togir,0,,,"[""The Alignment Problem (book)"",""the 80,000 Hours podcast"",""Superintelligence (book)"",""Human Compatible (book)"",""talks by AI alignment researchers"",""conversations with AI alignment researchers at conferences"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"The Alignment Problem (book),the 80,000 Hours podcast,Superintelligence (book),Human Compatible (book),talks by AI alignment researchers,conversations with AI alignment researchers at conferences,AXRP - the AI X-risk Research Podcast",2 | 2 | 2 | 2 | 1 | 1 | 1,AXRP - the AI X-risk Research Podcast,0 | 2 | 1 | 0 | 1 | 3 | 2,0 | 0 | 0 | 0 | 0 | 4 | 2,"havent finished it. answers may differ when i do | I would maybe recommend it because i know that there is at least one with Paul, and my guess is that listening to Paul is helpful, though I haven't done it. |  |  | only ""a little"" is because I have mostly engaged with it by going on it one time,","I remember watching one such talk, which seems probably unrepresentative of 'talks'. I might suggest it to a person getting into alignment just because i don't know of any great resources, and maybe there are relevant talks, and other people like talks more than me. | It's weird that this one is 'at conferences' but there isn't some broader category of talking not at conferences. I don't really remember which talking was at conferences.

I feel weird saying I would recommend things to someone working on this already, because often they seems too obvious to recommend. Like, even if I like talking to alignment researchers, I'm not going to go to my friend at MIRI and be and ask if they have considered talking to people about alignment. I say I would recommend,  because if they were obviously not aware of the option, I would.","",a slack,,
7476686,43,,2022-06-26 23:16:09,,1337.18,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR12W2otIEoz4ybLxafwSLnc3ASBnjedgXP36DD1vEK25JlNNBpOvQeNkk4
7476745,43,,2022-06-26 23:43:22,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR3_YdJnlDxAsvD0hKGOQuARgOLh4XbrejMC8PiJw-CrME8L1pmrE-PsuJ0
7476746,43,,2022-06-26 23:43:31,,0.02,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR2-5XQjpoE_KWVm27r1TE_6rqS7T88jkDgvCQZuU7IEbGLsC-8hA0sL-wE
7476758,43,,2022-06-26 23:49:22,,1.43,eb2kjj8,0,,,,,,I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,,,,,,,,,,,,
7476778,43,,2022-06-27 00:00:17,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476787,43,,2022-06-27 00:09:31,,4.78,eb2kjj8,0,,,"[""Superintelligence (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",12,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"Superintelligence (book),talks by AI alignment researchers",,,,,,,,,,
7476799,43,,2022-06-27 00:20:07,,4503.52,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476818,43,,2022-06-27 00:38:54,2022-06-27 00:41:17,2.38,t6togir,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"talks by AI alignment researchers,Superintelligence (book),conversations with AI alignment researchers at conferences",3 | 2 | 3,conversations with AI alignment researchers at conferences,4 | 3 | 4,1 | 0 | 4,"","Yudkowsky, Bostrom, | ","",Facebook,,
7476837,43,,2022-06-27 00:54:27,2022-06-27 00:56:06,1.70,auqgbpq,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I have heard of AI alignment,"[""I have heard of AI alignment""]",,I have heard of AI alignment,"talks by AI alignment researchers,Superintelligence (book),the 80,000 Hours podcast",1 | 1 | 3,"the 80,000 Hours podcast",0 | 0 | 3,0 | 0 | 1, | ,"","","",,
7476840,43,,2022-06-27 00:57:38,,7.88,8d2bvx6,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Unsolved Problems in ML Safety by Hendrycks et al"",""AIRCS workshops"",""Superintelligence (book)"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",3,I am interested in AI alignment research,"the Iterated Amplification sequence on the Alignment Forum,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Unsolved Problems in ML Safety by Hendrycks et al,AIRCS workshops,Superintelligence (book),talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum",,,,,,,,,,
7476863,43,,2022-06-27 01:14:14,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476864,43,,2022-06-27 01:15:50,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7476885,43,,2022-06-27 01:29:11,,1.02,8d2bvx6,0,,,"[""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""Rob Miles videos"",""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",4,I am interested in AI alignment research,"Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,Rob Miles videos,AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,AGI Safety Fundamentals Course",,,,,,,,,,
7476896,43,,2022-06-27 01:33:05,2022-06-27 01:41:10,8.07,t6togir,0,,,"[""Rob Miles videos"",""Human Compatible (book)"",""Life 3.0 (book)"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""the Value Learning sequence on the Alignment Forum"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",11,I spend some of my time doing AI alignment field/community-building,"Rob Miles videos,Human Compatible (book),Life 3.0 (book),the 80,000 Hours podcast,the AI Alignment Newsletter,talks by AI alignment researchers,the Value Learning sequence on the Alignment Forum,Superintelligence (book)",3 | 2 | 2 | 4 | 1 | 2 | 3 | 4,Superintelligence (book),4 | 4 | 4 | 4 | 2 | 1 | 2 | 4,3 | 4 | 4 | 4 | 3 | 3 | 3 | 4, |  |  |  |  |  | ,"","",Facebook post,,
7476911,43,,2022-06-27 01:39:24,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR2zbBtNtzTvMuXOgAvhmio-HFWsM3Cl7ZqjwNTcVq_yH9aR4-iNlG0ov_Q
7476944,43,,2022-06-27 02:06:44,2022-06-27 02:11:17,4.55,t6togir,0,,,"[""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences"",""Life 3.0 (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",6,I spend some of my time doing AI alignment field/community-building,"Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,conversations with AI alignment researchers at conferences,Life 3.0 (book)",3 | 2 | 0 | 1 | 4 | 2,Life 3.0 (book),3 | 0 | 2 | 0 | 4 | 3,0 | 0 | 2 | 0 | 4 | 0, |  | Didn't read it enough to have a take | Not primarily about AI | More accessible than Superintelligence,"","Wait But Why post on superintelligence is the most accessible and engaging thing I've ever read on AI safety for an introduction to why you should care, although it's a bit out of date at this point",Fscevook,,
7476946,43,,2022-06-27 02:07:07,,14378.05,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,IwAR36VzEE59W-VYcduCl5LH1cbj7hXPy6gV3bEKkdCdszlBACBEVSVxT5xnU
7476999,43,,2022-06-27 02:40:37,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7477068,43,,2022-06-27 03:23:57,,13012.47,8d2bvx6,0,,,"[""talks by AI alignment researchers"",""Rob Miles videos"",""the AI Alignment Newsletter"",""The Alignment Problem (book)"",""Human Compatible (book)"",""Concrete Problems in AI Safety by Amodei et al"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""AXRP - the AI X-risk Research Podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the 80,000 Hours podcast"",""Life 3.0 (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"talks by AI alignment researchers,Rob Miles videos,the AI Alignment Newsletter,The Alignment Problem (book),Human Compatible (book),Concrete Problems in AI Safety by Amodei et al,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,the FLI podcast,AXRP - the AI X-risk Research Podcast,the ARCHES agenda by Andrew Critch and David Krueger,the 80,000 Hours podcast,Life 3.0 (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Superintelligence (book)",,talks by AI alignment researchers,,,,,,,,IwAR0qa9aHSW9pmlxLtBTqTnJaORkX1P0vZBfiKD869o3jBEu2NqLSpaESFX8
7477085,43,,2022-06-27 03:37:47,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7477178,43,,2022-06-27 04:26:48,2022-06-27 04:41:33,14.73,t6togir,0,,,"[""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""Rob Miles videos"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"the Iterated Amplification sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,Rob Miles videos,talks by AI alignment researchers,the 80,000 Hours podcast,Superintelligence (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the AI Alignment Newsletter | the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,Rob Miles videos,the AI Alignment Newsletter,Superintelligence (book),talks by AI alignment researchers",3 | 1 | 0 | 1 | 3 | 1 | 2,talks by AI alignment researchers,1 | 2 | 0 | 2 | 2 | 0 | 0,1 | 2 | 0 | 2 | 2 | 0 | 0, |  |  |  |  | ,"","","",,
7477206,43,,2022-06-27 04:42:17,,6.38,now6hjo,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""AGI Safety Fundamentals Course"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Rob Miles videos"",""the annual AI Alignment Literature Review and Charity Comparison"",""Superintelligence (book)"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",5,I am paid to work on technical AI alignment research,"Unsolved Problems in ML Safety by Hendrycks et al,AGI Safety Fundamentals Course,the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,Rob Miles videos,the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),the AI Alignment Newsletter",0 | 2 | 3,the Iterated Amplification sequence on the Alignment Forum,0 | 3 | 1,0 | 1 | 3,"Felt like it was trying to be academia-friendly but wasn’t really getting into the interesting technical problems to be tackled, or talking about things in particularly concise ways |  | ",,,,0,
7477246,43,,2022-06-27 04:58:47,,0.08,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7477247,43,,2022-06-27 05:02:32,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR1NaJoNCNrQOCTy5V6tt4XRlZOBTgefVJjkfrytl0eVZZDbwvopNzynyQM
7477301,43,,2022-06-27 05:36:53,,2.85,8d2bvx6,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""talks by AI alignment researchers"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",12,I spend some of my time solving technical problems related to AI alignment,"Concrete Problems in AI Safety by Amodei et al,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,talks by AI alignment researchers,the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book)",,,,,,,,,,
7477338,43,,2022-06-27 06:01:49,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7477388,43,,2022-06-27 06:25:27,,5172.05,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",9,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)",,,,,,,,,,,IwAR0oK8LYV_xMORnuhkH8Juc7s3BCLkjr1mHbwCXAFpji9VTkKcKqTI3l6b0
7477394,43,,2022-06-27 06:29:06,2022-06-27 06:54:45,25.63,t6togir,0,,,"[""the 80,000 Hours podcast"",""AXRP - the AI X-risk Research Podcast"",""conversations with AI alignment researchers at conferences"",""Rob Miles videos"",""Superintelligence (book)"",""talks by AI alignment researchers"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the Value Learning sequence on the Alignment Forum"",""the FLI podcast"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment""]",11,I spend some of my time publicly communicating about AI alignment,"the AI Alignment Newsletter,the 80,000 Hours podcast,the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),conversations with AI alignment researchers at conferences,the FLI podcast,Rob Miles videos,the Value Learning sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,talks by AI alignment researchers,the ARCHES agenda by Andrew Critch and David Krueger,the Iterated Amplification sequence on the Alignment Forum,Human Compatible (book),AIRCS workshops,Concrete Problems in AI Safety by Amodei et al,AXRP - the AI X-risk Research Podcast | the 80,000 Hours podcast,AXRP - the AI X-risk Research Podcast,conversations with AI alignment researchers at conferences,Rob Miles videos,Superintelligence (book),talks by AI alignment researchers,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,the ARCHES agenda by Andrew Critch and David Krueger,the Value Learning sequence on the Alignment Forum,the FLI podcast,Concrete Problems in AI Safety by Amodei et al",3 | 1 | 2 | 2 | 3 | 1 | 3 | 2 | 1 | 3 | 2 | 3 | 1 | 3 | 1 | 2 | 2 | 3 | 1 | 3 | 2 | 1 | 3 | 2 | 3 | 1 | 1 | 1,Concrete Problems in AI Safety by Amodei et al,4 | 1 | 2 | 2 | 3 | 1 | 4 | 1 | 2 | 3 | 2 | 3 | 1 | 4 | 1 | 2 | 2 | 3 | 1 | 4 | 1 | 2 | 3 | 2 | 3 | 1 | 2 | 1,4 | 0 | 2 | 1 | 4 | 0 | 2 | 1 | 0 | 2 | 2 | 2 | 0 | 4 | 0 | 2 | 1 | 4 | 0 | 2 | 1 | 0 | 2 | 2 | 2 | 0 | 1 | 1, |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | , |  |  | ,"",Facebook,,
7477541,43,,2022-06-27 07:53:51,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7477630,43,,2022-06-27 08:59:32,,0.15,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7477798,43,,2022-06-27 11:22:59,,0.07,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7477837,43,,2022-06-27 11:51:46,2022-06-27 11:57:22,5.58,t6togir,0,,,"[""talks by AI alignment researchers"",""Rob Miles videos"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time publicly communicating about AI alignment""]",4,I spend some of my time publicly communicating about AI alignment,"talks by AI alignment researchers,Rob Miles videos,Superintelligence (book)",3 | 4 | 4,Superintelligence (book),4 | 4 | 4,4 | 4 | 4,Very accessible and explains the concerns using simple examples while also extrapolating to larger consequences in a non-hyperbolic way. | ,Yudkowsky and Bostrom,"I am not actively in communication with too many AI engineers, but I have a friend who is.  They told me that several engineers are dismissing the concerns about alignment as not worth concerning themselves with.  In the most generous interpretation they mean ""this is not going to be an issue in my lifetime, or if it is I will be able to pivot when we can see it coming"", but they described several engineers who legitimately felt AGI it could never be achieved, and AI alignment is therefore not something we will ever need to worry about period.

This is unsettling. There are some basics of AI alignment and potential risks that you should understand before pursuing this field.",Facebook. Ronny Fernandez.,,
7477972,43,,2022-06-27 12:59:15,,0.15,u166gt1,0,,,,,,I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,,,,,,,,,,,,
7477983,43,snhxg8q,2022-06-27 13:09:12,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR183EkpQ7Z7RXRNzfVNLGsvy59fUvB_ywLaWYfPePnBmBfW9HiuTbOVnKM
7478007,43,,2022-06-27 13:19:48,2022-06-27 13:24:26,4.62,t6togir,0,,,"[""the AI Alignment Newsletter"",""Rob Miles videos"",""the Embedded Agency sequence on the Alignment Forum"",""AI Safety Camp"",""the Value Learning sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""Concrete Problems in AI Safety by Amodei et al"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"the AI Alignment Newsletter,Rob Miles videos,the Embedded Agency sequence on the Alignment Forum,AI Safety Camp,the Value Learning sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,Concrete Problems in AI Safety by Amodei et al,the Iterated Amplification sequence on the Alignment Forum",1 | 1 | 2 | 3 | 1 | 3 | 1 | 0,the Iterated Amplification sequence on the Alignment Forum,3 | 4 | 3 | 4 | 3 | 4 | 3 | 0,4 | 0 | 3 | 0 | 3 | 4 | 1 | 0, |  |  |  |  |  | ,"","",Repost by Facebook friend,,
7478217,43,,2022-06-27 14:33:26,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR2-ml-vLfX_sruyYSgopkSnFMfSEVpQnBEZOs1UcHNQEz7AT_SX3wWiQMc
7478325,43,,2022-06-27 15:06:54,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7478443,43,,2022-06-27 15:55:41,,0.40,u166gt1,0,,,,,,I have heard of AI alignment,"[""I have heard of AI alignment""]",,,,,,,,,,,,,
7478688,43,,2022-06-27 17:33:48,,9.40,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7478696,43,,2022-06-27 17:35:51,,2.43,auqgbpq,0,,,"[""Rob Miles videos"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"Rob Miles videos,Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,Human Compatible (book)",1 | 3 | 1 | 1,Human Compatible (book),1 | 4 | 0 | 0,0 | 3 | 0 | 0, |  | ,"","",,,
7478710,43,,2022-06-27 17:41:33,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7478738,43,,2022-06-27 17:53:35,2022-06-27 18:02:10,8.57,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""the Value Learning sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""AI Safety Camp"",""the 80,000 Hours podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Human Compatible (book)"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""AXRP - the AI X-risk Research Podcast"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",4,I am paid to work on technical AI alignment research,"the Embedded Agency sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,the Value Learning sequence on the Alignment Forum,the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al,AI Safety Camp,the 80,000 Hours podcast,the ARCHES agenda by Andrew Critch and David Krueger,Human Compatible (book),AGI Safety Fundamentals Course,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,AXRP - the AI X-risk Research Podcast,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),The Alignment Problem (book)",1 | 3 | 2 | 2 | 2 | 2 | 1 | 1 | 1 | 2 | 1 | 1 | 2 | 2 | 0 | 1,The Alignment Problem (book),1 | 0 | 3 | 3 | 0 | 3 | 1 | 0 | 0 | 3 | 4 | 1 | 2 | 1 | 0 | 2,2 | 4 | 3 | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 3 | 3 | 0 | 0, |  |  |  | I was a mentor |  |  |  | Considered being a mentor but decided not to. |  |  |  |  | , | ,"",Daniel Filan,4,
7478752,43,,2022-06-27 18:01:41,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7478762,43,,2022-06-27 18:04:17,2022-06-27 18:10:36,6.30,t6togir,0,,,"[""the FLI podcast"",""AXRP - the AI X-risk Research Podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Human Compatible (book)"",""AI Safety Camp"",""The Alignment Problem (book)"",""the Embedded Agency sequence on the Alignment Forum"",""Superintelligence (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the annual AI Alignment Literature Review and Charity Comparison"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""Rob Miles videos"",""talks by AI alignment researchers"",""the Iterated Amplification sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""the Value Learning sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the FLI podcast,AXRP - the AI X-risk Research Podcast,the ARCHES agenda by Andrew Critch and David Krueger,Human Compatible (book),AI Safety Camp,The Alignment Problem (book),the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the annual AI Alignment Literature Review and Charity Comparison,the AI Alignment Newsletter,the 80,000 Hours podcast,Rob Miles videos,talks by AI alignment researchers,the Iterated Amplification sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,the Value Learning sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al",1 | 1 | 2 | 3 | 2 | 3 | 2 | 2 | 2 | 1 | 2 | 2 | 1 | 1 | 1 | 2 | 1 | 2 | 1 | 2,Concrete Problems in AI Safety by Amodei et al,0 | 1 | 2 | 4 | 1 | 4 | 1 | 3 | 0 | 0 | 2 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1,1 | 1 | 3 | 2 | 1 | 2 | 1 | 2 | 1 | 0 | 2 | 1 | 0 | 0 | 1 | 2 | 0 | 1 | 0 | 0, |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | , | ,"","",5,
7478763,43,,2022-06-27 18:04:55,2022-06-27 18:09:04,4.15,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the FLI podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Superintelligence (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",8,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","conversations with AI alignment researchers at conferences,the FLI podcast,the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),Rob Miles videos",3 | 1 | 1 | 3 | 3,Rob Miles videos,3 | 1 | 1 | 4 | 4,4 | 1 | 4 | 4 | 4, |  |  | ,"Scott Garrabrant, Nate Soares","",Rafe Kennedy,,
7478779,43,,2022-06-27 18:10:15,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7478802,43,,2022-06-27 18:15:52,2022-06-27 18:30:42,14.83,t6togir,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""Human Compatible (book)"",""conversations with AI alignment researchers at conferences"",""The Alignment Problem (book)"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,Superintelligence (book),talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,Human Compatible (book),conversations with AI alignment researchers at conferences,The Alignment Problem (book),AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,the annual AI Alignment Literature Review and Charity Comparison",1 | 3 | 2 | 3 | 2 | 2 | 3 | 3 | 4 | 3 | 3,the annual AI Alignment Literature Review and Charity Comparison,1 | 3 | 1 | 4 | 2 | 3 | 4 | 2 | 3 | 4 | 2,0 | 2 | 0 | 4 | 1 | 3 | 4 | 1 | 4 | 3 | 3,"I mostly do not remember it very well, and I have the impression it is at least a bit dated. |  | I think it's pretty dated, but it was very useful for me when I initially read the first few chapters.

My dad (mechanical engineering PhD, lots of experience with scientific programming, somewhat suspicious of sci-fi or alarmist-sounding claims about technology) found it very unpersuasive, to the extent it may have *reduced* his interest in alignment and his concern about AI risk. | I don't remember it super well. | I think it's good for:
1. Getting familiar if you're currently very unfamiliar
2. Seeing a good example of how to talk about AI alignment/risk without sounding too alarmist

The main downside is that I think the proposed alignment schemes are unlikely to be sufficient | Similar to Human Compatible, I think it's good for getting roughly oriented, and for seeing a nice example of how to explain things in mainstream-compatible ways. | Some of it is not very accessible, but overall I really like having the topic be concrete and more specialized than, for example, 80000 hours. | It all depends on the guest, but this is one of the standard things I recommend to people who are not yet familiar. I also think it's good for keeping up with what people are thinking once you are familiar with the basics. | ","I think technical talks are very underrated, which is part of what goes into my wanting to recommend them. | This seems obviously good to me, if you have access to people's time. My recommendation would mainly be about talking to particular people (for example, Rohin Shah is imo unusually good at explaining things in a way that's useful for many different audiences, while I think Paul Christiano can be harder to understand if you don't already know some things).",Most of this is very audience specific.,It showed up in a couple different Slacks I'm in.,,
7478837,43,,2022-06-27 18:30:43,2022-06-27 18:31:58,1.25,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",3,I spend some of my time solving technical problems related to AI alignment,"",,,,,,,"","",,
7478906,43,,2022-06-27 18:55:49,,2.33,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7478933,43,,2022-06-27 19:08:45,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7479066,43,,2022-06-27 19:54:09,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7479277,43,,2022-06-27 21:07:01,,0.50,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7479371,43,,2022-06-27 21:55:11,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7479372,43,,2022-06-27 21:55:14,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7479656,43,,2022-06-28 00:01:38,2022-06-28 00:08:06,6.47,t6togir,0,,,"[""Superintelligence (book)"",""Life 3.0 (book)"",""the 80,000 Hours podcast"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",3 | 3,I am trying to move into a technical AI alignment career,"Superintelligence (book),Life 3.0 (book),the 80,000 Hours podcast,talks by AI alignment researchers",4 | 4 | 4 | 3,talks by AI alignment researchers,4 | 4 | 4 | 2,2 | 2 | 1 | 3, |  | ,"","",Facebook friend,,
7479858,43,,2022-06-28 01:24:07,,2.05,now6hjo,0,,,"[""talks by AI alignment researchers"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"talks by AI alignment researchers,Unsolved Problems in ML Safety by Hendrycks et al",3,talks by AI alignment researchers,3,4,,"",,,,
7479874,43,,2022-06-28 01:27:32,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7480021,43,,2022-06-28 02:49:38,,1.32,8d2bvx6,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""the ARCHES agenda by Andrew Critch and David Krueger"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""Life 3.0 (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,the ARCHES agenda by Andrew Critch and David Krueger,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,Life 3.0 (book),the 80,000 Hours podcast",,,,,,,,,3,
7480024,43,,2022-06-28 02:51:33,2022-07-10 19:17:01,18265.47,t6togir,0,,,"[""the ARCHES agenda by Andrew Critch and David Krueger"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""the Embedded Agency sequence on the Alignment Forum"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""Life 3.0 (book)"",""conversations with AI alignment researchers at conferences"",""the 80,000 Hours podcast"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",7,I am paid to work on technical AI alignment research,"the ARCHES agenda by Andrew Critch and David Krueger,AGI Safety Fundamentals Course,talks by AI alignment researchers,Superintelligence (book),the AI Alignment Newsletter,the Embedded Agency sequence on the Alignment Forum,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,Life 3.0 (book),conversations with AI alignment researchers at conferences,the 80,000 Hours podcast,AXRP - the AI X-risk Research Podcast",3 | 0 | 3 | 3 | 4 | 3 | 4 | 1 | 1 | 2 | 2 | 1,AXRP - the AI X-risk Research Podcast,2 | 4 | 3 | 2 | 4 | 3 | 4 | 2 | 2 | 2 | 4 | 2,3 | 1 | 2 | 2 | 4 | 4 | 4 | 0 | 0 | 3 | 1 | 2, |  |  |  |  |  |  |  |  | , | ,"","",3,
7480130,43,,2022-06-28 03:41:24,,0.10,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7480164,43,,2022-06-28 04:01:27,2022-06-28 04:09:18,7.85,t6togir,0,,,"[""talks by AI alignment researchers"",""the ML Safety newsletter"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""Life 3.0 (book)"",""The Alignment Problem (book)"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",3,I am trying to move into a technical AI alignment career,"AXRP - the AI X-risk Research Podcast,The Alignment Problem (book),the 80,000 Hours podcast,Superintelligence (book),the FLI podcast,the AI Alignment Newsletter,Life 3.0 (book),the ML Safety newsletter,talks by AI alignment researchers | talks by AI alignment researchers,the ML Safety newsletter,the AI Alignment Newsletter,Superintelligence (book),the 80,000 Hours podcast,Life 3.0 (book),The Alignment Problem (book),the FLI podcast",2 | 1 | 1 | 3 | 4 | 3 | 3 | 2,the FLI podcast,2 | 1 | 1 | 3 | 4 | 4 | 4 | 2,3 | 2 | 1 | 4 | 4 | 2 | 3 | 2, |  |  | My first introduction to AI as an existential threat! | Great introduction | Great concrete examples | ,"Holden Karnofsky, Nick Bostrom","For those interested in getting more involved, it’s hard to transition from passive engagement to active engagement! Also, post PhD recruitment opportunities for those who are interested, potentially in a central location! Connecting with professors is difficult and intimidating.",Twitter!,,
7480203,43,,2022-06-28 04:23:24,2022-06-28 04:33:37,10.22,t6togir,0,,,"[""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",5,I am interested in AI alignment research,"talks by AI alignment researchers,the 80,000 Hours podcast,Superintelligence (book)",2 | 1 | 4,Superintelligence (book),1 | 0 | 3,0 | 0 | 3,"I've listened to a few episodes, generally not really my thing | This was what really shaped my view on the alignment problem, but from what I understand there are better and more accessible books on the topic now, like Human Compatible (haven't read).  I think it's probably still a valuable book if you're a paid researcher.",Yudkowsky and Bostrom mostly,Another resource that really affected me on this topic is Tim Urban's AI posts.  I think they helped make the whole thing feel real and scary in a way that most other talks and essays couldn't.,Ronny's Facebook post,,
7480363,43,,2022-06-28 06:20:16,,0.17,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7480445,43,,2022-06-28 07:16:56,,4.23,e5pk481,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",3,I am interested in AI alignment research,,,,,,,,,,,
7480479,43,,2022-06-28 07:52:51,2022-06-28 07:54:59,2.13,t6togir,0,,,"[""Rob Miles videos"",""The Alignment Problem (book)"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"Rob Miles videos,The Alignment Problem (book),Human Compatible (book)",3 | 3 | 2,Human Compatible (book),3 | 2 | 2,3 | 2 | 2, |  | ,,"","",1,
7480637,43,,2022-06-28 09:54:13,,0.17,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481083,43,,2022-06-28 14:15:28,,0.25,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7481101,43,,2022-06-28 14:22:33,,119.43,8d2bvx6,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""the Machine Learning for Alignment Bootcamp"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Concrete Problems in AI Safety by Amodei et al"",""AGI Safety Fundamentals Course"",""conversations with AI alignment researchers at conferences"",""the ML Safety newsletter"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",2,I spend some of my time solving technical problems related to AI alignment,"Unsolved Problems in ML Safety by Hendrycks et al,the 80,000 Hours podcast,talks by AI alignment researchers,the Machine Learning for Alignment Bootcamp,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Concrete Problems in AI Safety by Amodei et al,AGI Safety Fundamentals Course,conversations with AI alignment researchers at conferences,the ML Safety newsletter,the AI Alignment Newsletter",,,,,,,,,,
7481103,43,,2022-06-28 14:22:50,,0.08,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7481106,43,,2022-06-28 14:23:11,,0.12,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7481107,43,,2022-06-28 14:25:41,2022-06-28 14:34:30,8.82,t6togir,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""Human Compatible (book)"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time publicly communicating about AI alignment""]",3,I spend some of my time publicly communicating about AI alignment,"talks by AI alignment researchers,Superintelligence (book),Human Compatible (book),Rob Miles videos",3 | 4 | 3 | 4,Rob Miles videos,0 | 4 | 3 | 4,3 | 1 | 0 | 3,"I think its a great intro but actual researchers very likely are familiar with the subject | I think it is good at giving a view of alignment from a ""mainstream"" point of view. But i am not convinced about the feasibility of the presented ideas. | Very well explained concepts and also entertaining so its a great intro. Could be very useful for researchers also to get the gist of alignment topics.","Yudkowski, Connor Leahy, Nate Soares","",Twitter,,
7481110,43,,2022-06-28 14:25:43,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481196,43,,2022-06-28 14:46:11,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481206,43,,2022-06-28 14:54:06,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481247,43,,2022-06-28 15:06:09,,3.00,8d2bvx6,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""the AI Alignment Newsletter"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"Unsolved Problems in ML Safety by Hendrycks et al,the AI Alignment Newsletter,AGI Safety Fundamentals Course,the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,Rob Miles videos",,,,,,,,,,
7481259,43,,2022-06-28 15:13:35,2022-06-28 15:41:31,27.93,t6togir,0,,,"[""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""Rob Miles videos"",""Superintelligence (book)"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",2,I am trying to move into a technical AI alignment career,"talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,Rob Miles videos,Superintelligence (book),the 80,000 Hours podcast",2 | 3 | 1 | 3 | 1,"the 80,000 Hours podcast",3 | 2 | 0 | 3 | 0,0 | 1 | 0 | 0 | 0, |  |  | Very long. Feels like a big commitment to recommend,"",No,Twitter,,
7481309,43,,2022-06-28 15:28:24,,1.58,8d2bvx6,0,,,"[""talks by AI alignment researchers"",""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Superintelligence (book)"",""the ML Safety newsletter"",""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""the Embedded Agency sequence on the Alignment Forum"",""the FLI podcast"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",2,I am paid to work on technical AI alignment research,"talks by AI alignment researchers,the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al,Superintelligence (book),the ML Safety newsletter,the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,the Embedded Agency sequence on the Alignment Forum,the FLI podcast,Human Compatible (book)",,,,,,,,,1,
7481332,43,,2022-06-28 15:36:24,,2.27,now6hjo,0,,,"[""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""the ML Safety newsletter"",""the Machine Learning for Alignment Bootcamp""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",1,I spend some of my time publicly communicating about AI alignment,"the 80,000 Hours podcast,talks by AI alignment researchers,the ML Safety newsletter,the Machine Learning for Alignment Bootcamp",3,"the 80,000 Hours podcast",2,4,"",,,,,
7481356,43,,2022-06-28 15:47:50,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481419,43,,2022-06-28 16:12:26,2022-06-28 16:37:11,24.73,t6togir,0,,,"[""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""conversations with AI alignment researchers at conferences"",""the ML Safety newsletter"",""Human Compatible (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the Iterated Amplification sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""the FLI podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",2,I am paid to work on technical AI alignment research,"AGI Safety Fundamentals Course,Superintelligence (book),conversations with AI alignment researchers at conferences,the ML Safety newsletter,Human Compatible (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the Iterated Amplification sequence on the Alignment Forum,talks by AI alignment researchers,Unsolved Problems in ML Safety by Hendrycks et al,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,the 80,000 Hours podcast,the FLI podcast",3 | 2 | 3 | 4 | 2 | 1 | 1 | 3 | 4 | 1 | 3 | 2 | 1,the FLI podcast,2 | 1 | 1 | 4 | 1 | 0 | 1 | 3 | 2 | 1 | 3 | 1 | 0,0 | 0 | 4 | 4 | 0 | 0 | 0 | 3 | 4 | 0 | 2 | 1 | 0," |  |  |  |  |  | If they were already more interested in x-risk, I'd probably recommend Open Problems in AI x-risk instead: https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5 |  | Lower than otherwise because there hasn't been a newsletter in quite a while |  | ","Evan Hubinger, Richard Ngo, Andy Jones, Ethan Perez, Owain Evans, Rohin Shah, Dan Hendrycks, Zhijing Jin, probably more I'm not thinking of | Dan Hendrycks, Evan Hubinger, Buck Shlegeris, Joe Carlsmith, probably others I'm not thinking of","","Many different places, various slacks, twitter. Once I saw it enough I decided to do it.",1,
7481460,43,,2022-06-28 16:23:28,,37.72,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7481685,43,,2022-06-28 17:47:10,,1.57,e5pk481,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",30,I am interested in AI alignment research,,,,,,,,,,,
7481697,43,,2022-06-28 17:53:47,,5.57,now6hjo,0,,,"[""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""conversations with AI alignment researchers at conferences"",""the AI Alignment Newsletter"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"AGI Safety Fundamentals Course,Superintelligence (book),conversations with AI alignment researchers at conferences,the AI Alignment Newsletter,The Alignment Problem (book)",4 | 2 | 3 | 3,the AI Alignment Newsletter,4 | 0 | 3 | 0,2 | 1 | 4 | 3, |  | ,"",,,,
7481700,43,,2022-06-28 17:55:36,,0.15,e5pk481,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",1,I am interested in AI alignment research,,,,,,,,,,,
7481707,43,,2022-06-28 17:59:13,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7481735,43,,2022-06-28 18:11:34,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,IwAR3C9c29F9RrpZuZh3uADMO58gEAKQbTfBc3CjD7PeX8OEWmtiE9fJLJzy4
7481791,43,,2022-06-28 18:51:15,2022-06-28 18:54:28,3.20,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the Embedded Agency sequence on the Alignment Forum"",""Life 3.0 (book)"",""the ML Safety newsletter"",""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""the Value Learning sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I am paid to work on technical AI alignment research""]",2,I am paid to work on technical AI alignment research,"conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,Life 3.0 (book),the ML Safety newsletter,AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,the Value Learning sequence on the Alignment Forum",4 | 1 | 0 | 2 | 3 | 3 | 1,the Value Learning sequence on the Alignment Forum,2 | 0 | 1 | 0 | 1 | 3 | 1,4 | 0 | 0 | 1 | 3 | 1 | 1, |  |  |  | , | ,"","",1,
7482105,43,,2022-06-28 20:17:06,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7482124,43,,2022-06-28 20:23:45,2022-06-28 20:35:21,11.58,t6togir,0,,,"[""the 80,000 Hours podcast"",""AGI Safety Fundamentals Course"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",2,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"the 80,000 Hours podcast,AGI Safety Fundamentals Course,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the annual AI Alignment Literature Review and Charity Comparison",3 | 3 | 4 | 3 | 4 | 4,the annual AI Alignment Literature Review and Charity Comparison,3 | 4 | 4 | 0 | 2 | 2,1 | 0 | 1 | 3 | 2 | 3, |  |  |  |  | ,,"I think that the most useful resources that I recommend to people to convince people that alignment is a problem worth thinking about are: 
1. AGI safety from first principles from Richard Ngo 
2. The Bitter Lesson from Richard Sutton 
3. Bio-anchors summarized by Cold Takes
I also think that there lacks a resource on AI capabilities and why we could expect AGI to come soon (scaling laws etc.)",Twitter from Ben Hilton,,
7482163,43,,2022-06-28 20:42:05,,0.15,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7482236,43,,2022-06-28 21:15:39,,1.13,8d2bvx6,0,,,"[""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I have heard of AI alignment,"[""I have heard of AI alignment""]",,I have heard of AI alignment,"the 80,000 Hours podcast",,,,,,,,,,
7482337,43,,2022-06-28 22:17:18,,0.15,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7482462,43,,2022-06-28 23:46:26,,4.88,8d2bvx6,0,,,"[""AIRCS workshops"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,," | AIRCS workshops,the AI Alignment Newsletter,the 80,000 Hours podcast,Rob Miles videos",,,,,,,"",,,
7482489,43,,2022-06-29 00:10:04,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7482646,43,,2022-06-29 02:30:15,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7482783,43,,2022-06-29 04:39:08,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7482889,43,,2022-06-29 05:51:11,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7482902,43,,2022-06-29 06:04:07,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7482990,43,vljl08l,2022-06-29 07:10:16,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7483004,43,,2022-06-29 07:27:30,,531.52,u166gt1,0,,,,,,I am interested in AI alignment research,"[""I am interested in AI alignment research""]",,I am interested in AI alignment research,,,,,,,,,,,
7483184,43,,2022-06-29 09:58:53,2022-06-29 10:00:10,1.28,t6togir,0,,,"[""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",1,I am interested in AI alignment research,The Alignment Problem (book),3,The Alignment Problem (book),4,1,"",,"",Someone linked it on Twitter,,
7483844,43,,2022-06-29 15:27:43,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7484299,43,,2022-06-29 18:14:50,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7484958,43,,2022-06-29 23:05:45,2022-06-29 23:09:43,3.97,t6togir,0,,,"[""talks by AI alignment researchers"",""Human Compatible (book)"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"talks by AI alignment researchers,Human Compatible (book),the Embedded Agency sequence on the Alignment Forum",2 | 2 | 3,the Embedded Agency sequence on the Alignment Forum,2 | 2 | 3,0 | 0 | 1, | ,"","",twitter,,
7485054,43,,2022-06-30 00:14:44,,117.50,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7485654,43,,2022-06-30 07:38:09,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7486554,43,,2022-06-30 16:02:21,,0.00,eb2kjj8,0,,,,,,,,,,,,,,,,,,,,
7487258,44,,2022-06-30 19:13:39,,0.02,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7487268,44,,2022-06-30 19:17:03,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7487270,44,,2022-06-30 19:18:10,,1.08,8d2bvx6,0,,,"[""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",7,I am interested in AI alignment research,talks by AI alignment researchers,,,,,,,,,,
7487281,44,,2022-06-30 19:20:33,,0.20,t0y9zda,0,,,,,,I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,,,,,,,,,,2,
7487356,44,,2022-06-30 19:47:16,,0.93,t0y9zda,0,,,,,,"I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,,,,,,,,,,1,
7487435,44,,2022-06-30 20:15:38,2022-06-30 20:18:47,3.13,t6togir,0,,,"[""the 80,000 Hours podcast"",""the Value Learning sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",1,I spend some of my time publicly communicating about AI alignment,"the 80,000 Hours podcast,the Value Learning sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter",2 | 3 | 4 | 4,the AI Alignment Newsletter,0 | 2 | 4 | 4,0 | 4 | 4 | 4, |  |  | ,,"",LessWrong,,
7487485,44,,2022-06-30 20:36:20,2022-06-30 20:37:23,1.05,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,"",EA forum,,
7487556,44,,2022-06-30 20:58:33,,1.18,8d2bvx6,0,,,"[""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""The Alignment Problem (book)"",""Superintelligence (book)"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,the 80,000 Hours podcast,talks by AI alignment researchers,The Alignment Problem (book),Superintelligence (book),Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum",,,,,,,,,,
7487698,44,,2022-06-30 21:56:19,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7487780,44,,2022-06-30 22:27:42,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7487798,44,,2022-06-30 22:37:31,2022-06-30 22:53:14,15.70,t6togir,0,,,"[""the FLI podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Human Compatible (book)"",""talks by AI alignment researchers"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""The Alignment Problem (book)"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",1,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"The Alignment Problem (book),Superintelligence (book),the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast,conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,talks by AI alignment researchers,AGI Safety Fundamentals Course,Rob Miles videos,Human Compatible (book),the FLI podcast | the FLI podcast,the Embedded Agency sequence on the Alignment Forum,Human Compatible (book),talks by AI alignment researchers,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,The Alignment Problem (book),AGI Safety Fundamentals Course,the 80,000 Hours podcast,conversations with AI alignment researchers at conferences",4 | 1 | 2 | 3 | 2 | 2 | 2 | 4 | 4 | 2 | 3,conversations with AI alignment researchers at conferences,4 | 0 | 2 | 4 | 1 | 3 | 0 | 4 | 4 | 2 | 3,2 | 0 | 1 | 2 | 0 | 2 | 0 | 2 | 3 | 1 | 4,"A really great overview of the scene (given that it's just a single book). Not very deep, though, so I guess a person who is paid to do alignment would benefit from it much though. | I listened to one podcast (Pinker and Russell debating AI safety). It was interesting, but not interesting enough to recommend to others, and don't have other data. | Don't have much data here. I think it explains important topics, but can't say much else. | A good intro book, would recommend for newcomers. I'd guess it's pretty basic for those working on AI alignment, though. | My impression is that they are very good, and the ones I have watched explained the topics exceptionally clearly. Personally I haven't watched that much to get much use out of them, but would recommend for newcomers. | I've read some texts in the sequence at the AGI safety fundamentals course, and it looks like one reasonable agenda for alignment. Wouldn't exactly recommend it to people, though. | A really great overview of the scene (given that it's just a single book). Not very deep, though, so I guess a person who is paid to do alignment would benefit from it much though. | Great course, gives you a good overall view of what kind of work is being done. Definitely would recommend for newcomers, and probably many professionals would benefit from it as well. | I've listened to a few podcasts (maybe a dozen hours). It's fun to hear people talking about stuff instead of just reading, and good stimulation for when you go for a walk or a run, but educationally I have not learnt that much from them.","Not much data here. I've watched a couple of talks, and they were alright. | It was great talking to Gavin Leech from Arb. He definitely masters the area he is working on (AI forecasting), and had some good insights on that, and of course alignment work in general.

There are other conversations I also had at EAGxPrague 2022, though I don't remember many of the names.","I first learnt about AGI and AGI safety through The Codex (""Superintelligence FAQ""), and immediately afterwards from The Sequences. It's a cliche at this point, but The Sequences are really good, and would recommend The Codex as well..",LessWrong post,,
7487826,44,,2022-06-30 22:47:54,,0.88,8d2bvx6,0,,,"[""AIRCS workshops"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""Human Compatible (book)"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences"",""AXRP - the AI X-risk Research Podcast"",""the ARCHES agenda by Andrew Critch and David Krueger""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment","[""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"AIRCS workshops,AGI Safety Fundamentals Course,the FLI podcast,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),the AI Alignment Newsletter,talks by AI alignment researchers,Human Compatible (book),the 80,000 Hours podcast,conversations with AI alignment researchers at conferences,AXRP - the AI X-risk Research Podcast,the ARCHES agenda by Andrew Critch and David Krueger",,,,,,,,,,
7487838,44,,2022-06-30 22:55:59,,2.43,now6hjo,0,,,"[""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""Concrete Problems in AI Safety by Amodei et al"",""Superintelligence (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",3,I spend some of my time doing AI alignment field/community-building,"the AI Alignment Newsletter,the 80,000 Hours podcast,Concrete Problems in AI Safety by Amodei et al,Superintelligence (book),talks by AI alignment researchers",3 | 3,"the 80,000 Hours podcast",4 | 4,2 | 0, | ,,,,,
7487842,44,,2022-06-30 22:58:55,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7487900,44,,2022-06-30 23:25:39,,0.38,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7487911,44,,2022-06-30 23:29:25,,0.88,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7487963,44,,2022-06-30 23:58:18,2022-07-01 00:01:48,3.48,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the annual AI Alignment Literature Review and Charity Comparison"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Superintelligence (book)"",""Rob Miles videos"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",7,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","conversations with AI alignment researchers at conferences,the annual AI Alignment Literature Review and Charity Comparison,the ARCHES agenda by Andrew Critch and David Krueger,Superintelligence (book),Rob Miles videos,Human Compatible (book)",2 | 3 | 2 | 4 | 4 | 2,Human Compatible (book),3 | 2 | 2 | 4 | 4 | 2,4 | 4 | 3 | 4 | 4 | 2, |  |  |  | ,"","",LW post,,
7488567,44,,2022-07-01 06:01:39,,7278.47,8d2bvx6,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""Human Compatible (book)"",""AGI Safety Fundamentals Course"",""AXRP - the AI X-risk Research Podcast"",""the 80,000 Hours podcast"",""Concrete Problems in AI Safety by Amodei et al"",""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the Value Learning sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building","[""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"the Embedded Agency sequence on the Alignment Forum,Human Compatible (book),AGI Safety Fundamentals Course,AXRP - the AI X-risk Research Podcast,the 80,000 Hours podcast,Concrete Problems in AI Safety by Amodei et al,the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,the Value Learning sequence on the Alignment Forum",,the Embedded Agency sequence on the Alignment Forum,,,,,,,,
7489011,44,,2022-07-01 10:57:17,2022-07-01 11:03:43,6.43,t6togir,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""AXRP - the AI X-risk Research Podcast"",""AI Safety Camp"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the Iterated Amplification sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast,AI Safety Camp,AGI Safety Fundamentals Course,Superintelligence (book),the 80,000 Hours podcast,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum",1 | 2 | 4 | 1 | 3 | 3 | 3 | 3,the Embedded Agency sequence on the Alignment Forum,0 | 0 | 3 | 4 | 3 | 4 | 3 | 1,3 | 4 | 1 | 0 | 2 | 3 | 3 | 3, |  |  |  |  |  | ,"","","",1,
7489933,44,,2022-07-01 19:51:41,2022-07-01 19:53:53,2.20,t6togir,0,,,"[""talks by AI alignment researchers"",""the ML Safety newsletter"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"talks by AI alignment researchers,the ML Safety newsletter,the AI Alignment Newsletter,Superintelligence (book),AGI Safety Fundamentals Course",2 | 2 | 1 | 2 | 3,AGI Safety Fundamentals Course,1 | 3 | 2 | 3 | 3,0 | 3 | 2 | 0 | 1, |  |  | ,"","","",,
7489966,44,,2022-07-01 20:08:04,,23.65,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7490978,44,,2022-07-02 11:02:26,2022-07-02 11:34:04,31.63,t6togir,0,,,"[""the ARCHES agenda by Andrew Critch and David Krueger"",""AXRP - the AI X-risk Research Podcast"",""AIRCS workshops"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the Embedded Agency sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""the FLI podcast"",""Rob Miles videos"",""Human Compatible (book)"",""the Value Learning sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""conversations with AI alignment researchers at conferences"",""The Alignment Problem (book)"",""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""Life 3.0 (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",5,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the ARCHES agenda by Andrew Critch and David Krueger,AXRP - the AI X-risk Research Podcast,AIRCS workshops,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the Embedded Agency sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,talks by AI alignment researchers,the FLI podcast,Rob Miles videos,Human Compatible (book),the Value Learning sequence on the Alignment Forum,the 80,000 Hours podcast,conversations with AI alignment researchers at conferences,The Alignment Problem (book),Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,Life 3.0 (book)",2 | 1 | 0 | 1 | 2 | 1 | 4 | 4 | 2 | 1 | 0 | 2 | 3 | 1 | 2 | 1 | 1 | 1 | 1,Life 3.0 (book),1 | 2 | 3 | 2 | 3 | 0 | 2 | 1 | 3 | 3 | 3 | 3 | 3 | 3 | 4 | 3 | 1 | 0 | 1,2 | 2 | 0 | 3 | 3 | 0 | 3 | 4 | 3 | 2 | 1 | 2 | 2 | 2 | 4 | 1 | 1 | 2 | 1, |  |  |  |  |  | Very useful ideas but often hard for people to understand; the latter factor makes me more hesitant to recommend it to others |  |  |  |  |  |  |  | I'm particularly uncertain here; I don't remember that well what the book said |  | ,"Friend getting into alignment: Paul Christiano, Rohin Shah, Stuart Russell, Rob Miles (if he counts)

Friend who is paid to do alignment research: Basically all the alignment researchers who seem to do good work? Most people prefer listening to talks over reading papers, and I think it's pretty important for them to learn about what other people are doing (which set of people depends on what they are trying to do). | I didn't have particular researchers in mind; it just seems like a lot of the cutting edge ends up being in researchers' heads rather than written down publicly, and conversations are the best way to get at that.","",I helped Daniel with early versions of the survey design and he sent it to me once it was ready.,5,
7492393,44,,2022-07-03 01:55:50,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496530,44,,2022-07-04 23:50:04,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496699,44,,2022-07-05 02:16:37,,4.27,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7496933,44,,2022-07-05 06:00:19,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496954,44,,2022-07-05 06:12:29,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496961,44,,2022-07-05 06:21:45,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496966,44,,2022-07-05 06:22:22,2022-07-05 06:29:46,7.40,t6togir,0,,,"[""the ARCHES agenda by Andrew Critch and David Krueger"",""conversations with AI alignment researchers at conferences"",""the Embedded Agency sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""the FLI podcast"",""Superintelligence (book)"",""the Machine Learning for Alignment Bootcamp"",""the Iterated Amplification sequence on the Alignment Forum"",""the Value Learning sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""Rob Miles videos"",""talks by AI alignment researchers"",""AXRP - the AI X-risk Research Podcast"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",10,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the ARCHES agenda by Andrew Critch and David Krueger,conversations with AI alignment researchers at conferences,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,the FLI podcast,Superintelligence (book),the Machine Learning for Alignment Bootcamp,the Iterated Amplification sequence on the Alignment Forum,the Value Learning sequence on the Alignment Forum,the 80,000 Hours podcast,Rob Miles videos,talks by AI alignment researchers,AXRP - the AI X-risk Research Podcast,the annual AI Alignment Literature Review and Charity Comparison",0 | 3 | 4 | 2 | 0 | 2 | 0 | 2 | 1 | 1 | 2 | 2 | 3 | 2,the annual AI Alignment Literature Review and Charity Comparison,0 | 4 | 3 | 2 | 0 | 1 | 1 | 1 | 0 | 1 | 1 | 3 | 3 | 0,1 | 4 | 4 | 2 | 0 | 1 | 1 | 2 | 0 | 1 | 1 | 3 | 3 | 0," | Totally shook my understanding of Agency. |  |  | Was incredible at the time, kinda superceeded by online content now. | I helped organize it, so I guess an odd data point | Was best thing at the time for understanding Paul's research. There's been better writing since. |  | Paul's episodes were fine. I can't remember any others. |  | The realest interviews with people. | Has been helpful for an overview and setting the standard for community evaluation, but isn't something I'd use for learning about the details. At best a reference for work-done.","Scott Garrabrant, Abram Demski, John Wentworth... probably many more, but I can't think of any quickly. | Scott Garrabrant, Eliezer Yudkowsky, John Wentworth, Buck Shlegeris, Paul Christiano, probably more.","",Alignment Newsletter,,
7496976,44,,2022-07-05 06:26:33,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7496990,44,,2022-07-05 06:40:23,2022-07-05 06:42:25,2.02,t6togir,0,,,"[""Rob Miles videos"",""Superintelligence (book)"",""talks by AI alignment researchers"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"Rob Miles videos,Superintelligence (book),talks by AI alignment researchers,the 80,000 Hours podcast",3 | 2 | 2 | 3,"the 80,000 Hours podcast",3 | 2 | 3 | 3,0 | 0 | 2 | 3, |  | ,paul christiano,"","",,
7497004,44,,2022-07-05 06:56:49,2022-07-05 06:57:09,0.33,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,"","",,
7497028,44,,2022-07-05 07:16:36,,2.43,now6hjo,0,,,"[""The Alignment Problem (book)"",""talks by AI alignment researchers"",""Life 3.0 (book)"",""Rob Miles videos"",""Human Compatible (book)"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""AXRP - the AI X-risk Research Podcast"",""the FLI podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""the ML Safety newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",5,I spend some of my time doing AI alignment field/community-building,"The Alignment Problem (book),talks by AI alignment researchers,Life 3.0 (book),Rob Miles videos,Human Compatible (book),the AI Alignment Newsletter,Superintelligence (book),AXRP - the AI X-risk Research Podcast,the FLI podcast,the Embedded Agency sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,the ML Safety newsletter",1,The Alignment Problem (book),0,0,"",,,,,
7497035,44,,2022-07-05 07:23:09,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497039,44,,2022-07-05 07:31:18,2022-07-05 07:45:24,14.08,t6togir,0,,,"[""the Iterated Amplification sequence on the Alignment Forum"",""AXRP - the AI X-risk Research Podcast"",""the Machine Learning for Alignment Bootcamp"",""the Value Learning sequence on the Alignment Forum"",""The Alignment Problem (book)"",""talks by AI alignment researchers"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the 80,000 Hours podcast"",""Human Compatible (book)"",""AGI Safety Fundamentals Course"",""the Embedded Agency sequence on the Alignment Forum"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al"",""the FLI podcast"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",3,I am paid to work on technical AI alignment research,"the Iterated Amplification sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast,the Machine Learning for Alignment Bootcamp,the Value Learning sequence on the Alignment Forum,The Alignment Problem (book),talks by AI alignment researchers,the ARCHES agenda by Andrew Critch and David Krueger,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the 80,000 Hours podcast,Human Compatible (book),AGI Safety Fundamentals Course,the Embedded Agency sequence on the Alignment Forum,Unsolved Problems in ML Safety by Hendrycks et al,Superintelligence (book),the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al,the FLI podcast,conversations with AI alignment researchers at conferences",4 | 4 | 2 | 3 | 3 | 4 | 2 | 4 | 4 | 2 | 3 | 3 | 1 | 2 | 4 | 4 | 3 | 4,conversations with AI alignment researchers at conferences,1 | 2 | 2 | 4 | 3 | 4 | 3 | 4 | 4 | 4 | 4 | 1 | 2 | 2 | 4 | 4 | 3 | 4,3 | 4 | 2 | 2 | 2 | 4 | 2 | 2 | 4 | 0 | 2 | 4 | 2 | 2 | 4 | 2 | 3 | 4,"I guess it's not the best thing for beginners | AXRP is excellent for going into detail about specific papers/ideas | I applied but didn't attend! | It is good for beginners | This is best for ML researchers getting into alignment | Haven't read it in a while, fuzzy on the details | It's a good accessible intro to technical alignment research but doesn't really attack the fundamental problem | Can't recommend it more highly | It's a nice accessible/non technical intro. Good for beginners and non technical (e.g. policy) people | It's a good simple outline of the basic arguments. I ran a reading group that started with this. | Probably too complicated for most beginners (in my experience people usually find it confusing at first). But a great summary of some core challenges. | I haven't engaged with it that much but seem to recall it made some good points | Haven't read in a while. Obviously classic but not a good introduction to the current space and arguments | It's great. | It was something I looked at frequently when I was first getting into alignment research. Probably out of date now | I like it.","I've attended introductory talks by Rohin Shah, Richard Ngo, Tom Everitt and attended more technical talks with hosted by the coop AI foundation (e.g. by Jesse Clifton,Vince Cognitzer) and many others... In general talks are great because of the opportunity to chat and ask questions. In person talks are much better than online. | ","",Alignment newsletter,2,
7497044,44,,2022-07-05 07:39:56,2022-07-05 07:47:13,7.28,t6togir,0,,,"[""Rob Miles videos"",""AGI Safety Fundamentals Course"",""the Embedded Agency sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment""]",7,I spend some of my time solving technical problems related to AI alignment,"Rob Miles videos,AGI Safety Fundamentals Course,the Embedded Agency sequence on the Alignment Forum,the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,AXRP - the AI X-risk Research Podcast",3 | 4 | 4 | 2 | 3 | 2,AXRP - the AI X-risk Research Podcast,4 | 4 | 3 | 2 | 2 | 2,3 | 4 | 4 | 1 | 3 | 3, |  |  |  |  | ,,"",Twitter,,
7497052,44,,2022-07-05 07:49:19,2022-07-05 08:00:47,11.45,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the Machine Learning for Alignment Bootcamp"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",2,I am interested in AI alignment research,"conversations with AI alignment researchers at conferences,the Machine Learning for Alignment Bootcamp,Rob Miles videos",3 | 4 | 2,Rob Miles videos,2 | 3 | 2,3 | 1 | 0,"it was very useful to me, but it's difficult to make it work for people who have a full-time job, it was convenient to me that it happened to be around the time I had quit my job anyway | mostly useful at the introductory level","I explicitly remember talking to Nate Thomas, but I'm sure I've had other conversations that I'm not remembering (and also conversations outside of conferences with Rohin Shah and others)","",alignment newsletter,,
7497056,44,,2022-07-05 07:53:06,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7497059,44,,2022-07-05 07:55:24,2022-07-05 08:14:42,19.28,t6togir,0,,,"[""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the annual AI Alignment Literature Review and Charity Comparison"",""Superintelligence (book)"",""Human Compatible (book)"",""the FLI podcast"",""the Value Learning sequence on the Alignment Forum"",""Concrete Problems in AI Safety by Amodei et al"",""the 80,000 Hours podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""AGI Safety Fundamentals Course"",""AXRP - the AI X-risk Research Podcast"",""the Embedded Agency sequence on the Alignment Forum"",""AI Safety Camp"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""the ARCHES agenda by Andrew Critch and David Krueger""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",7,I am paid to work on technical AI alignment research,"Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),Human Compatible (book),the FLI podcast,the Value Learning sequence on the Alignment Forum,Concrete Problems in AI Safety by Amodei et al,the 80,000 Hours podcast,the Iterated Amplification sequence on the Alignment Forum,AGI Safety Fundamentals Course,AXRP - the AI X-risk Research Podcast,the Embedded Agency sequence on the Alignment Forum,AI Safety Camp,the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,the ARCHES agenda by Andrew Critch and David Krueger",1 | 1 | 4 | 3 | 2 | 1 | 3 | 2 | 1 | 4 | 2 | 2 | 4 | 4 | 4 | 2 | 1,the ARCHES agenda by Andrew Critch and David Krueger,1 | 0 | 1 | 3 | 0 | 0 | 0 | 1 | 0 | 3 | 0 | 0 | 1 | 2 | 2 | 0 | 0,0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3 | 3 | 1 | 0,"Very audience-specific. More attractive to ML-background people, imo. | Not sure if I remember it correctly. My impression is that it assumes a lot of background knowledge, so itsn't good as an intro resource. Also, people _deeply_ engaged in the field might know most of the stuff there anyway. | Good resource, except somewhat outdated. But puts down the basic concepts well.
The ""friend who is paid to do AI alignment research"" part is a very weird counterfactual - most people who do that already read the book. Unsure how to answer that :D. |  | Uhm, anyway, what do you mean by these ""How likely would you be to recommend X to Y?"". If I take it literally, then the true answers to all of these questions for me are ""0-20%"", because empirically I don't go around recommending stuff to people unless they ask :D. Also, when they do ask, I only give 1-2 recommendations that seem most relevant for them. So if you spread it out among all the resources, none of them is getting >20% :-). |  | Very important at the time. But somewhat dated now, imo. |  |  |  |  |  | Has the (important) side effect of networking. |  |  | ","I assume you mean ""big conferences"", rather than ""small workshops, where you actually get down to discuss object-level problems for a longer time"". The latter would be more useful :).","",Alignment Newsletter,2,
7497066,44,,2022-07-05 08:00:01,,2.22,8d2bvx6,0,,,"[""The Alignment Problem (book)"",""talks by AI alignment researchers"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time publicly communicating about AI alignment,"[""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"The Alignment Problem (book),talks by AI alignment researchers,the AI Alignment Newsletter",,,,,,,,,,
7497098,44,,2022-07-05 08:27:40,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497179,44,,2022-07-05 09:24:18,2022-07-05 09:27:01,2.70,t6togir,0,,,"[""Rob Miles videos"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time publicly communicating about AI alignment""]",2,I spend some of my time publicly communicating about AI alignment,"Rob Miles videos,the AI Alignment Newsletter,the 80,000 Hours podcast",4 | 2 | 3,"the 80,000 Hours podcast",4 | 1 | 4,3 | 4 | 3, |  | ,,"",AI Alignment Newsletter,,
7497188,44,,2022-07-05 09:30:58,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497197,44,,2022-07-05 09:36:05,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497219,44,,2022-07-05 09:54:39,,2.93,8d2bvx6,0,,,"[""AXRP - the AI X-risk Research Podcast"",""the Iterated Amplification sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""talks by AI alignment researchers"",""the FLI podcast"",""the Embedded Agency sequence on the Alignment Forum"",""the Value Learning sequence on the Alignment Forum"",""Rob Miles videos"",""the AI Alignment Newsletter"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I am paid to work on technical AI alignment research","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"AXRP - the AI X-risk Research Podcast,the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast,talks by AI alignment researchers,the FLI podcast,the Embedded Agency sequence on the Alignment Forum,the Value Learning sequence on the Alignment Forum,Rob Miles videos,the AI Alignment Newsletter,the annual AI Alignment Literature Review and Charity Comparison",,,,,,,,,0,
7497323,44,,2022-07-05 11:19:33,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497343,44,,2022-07-05 11:31:19,2022-07-05 12:31:52,60.63,auqgbpq,0,,,"[""Rob Miles videos"",""talks by AI alignment researchers"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""the FLI podcast"",""Life 3.0 (book)"",""the annual AI Alignment Literature Review and Charity Comparison""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",5,I am interested in AI alignment research,"Rob Miles videos,talks by AI alignment researchers,Superintelligence (book),the AI Alignment Newsletter,the 80,000 Hours podcast,the FLI podcast,Life 3.0 (book),the annual AI Alignment Literature Review and Charity Comparison",3 | 3 | 3 | 3 | 3 | 3 | 2 | 4,the annual AI Alignment Literature Review and Charity Comparison,4 | 3 | 4 | 3 | 4 | 4 | 2 | 4,0 | 3 | 0 | 4 | 1 | 2 | 0 | 4, |  |  |  |  |  | ,"","",Alignment newsletter,,
7497406,44,,2022-07-05 12:29:18,2022-07-05 12:48:56,19.62,t6togir,0,,,"[""the annual AI Alignment Literature Review and Charity Comparison"",""AXRP - the AI X-risk Research Podcast"",""conversations with AI alignment researchers at conferences"",""the AI Alignment Newsletter"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""The Alignment Problem (book)"",""Concrete Problems in AI Safety by Amodei et al"",""Human Compatible (book)"",""talks by AI alignment researchers"",""the FLI podcast"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the 80,000 Hours podcast"",""Rob Miles videos"",""the ML Safety newsletter"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Unsolved Problems in ML Safety by Hendrycks et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",9,I spend some of my time publicly communicating about AI alignment,"the annual AI Alignment Literature Review and Charity Comparison,AXRP - the AI X-risk Research Podcast,conversations with AI alignment researchers at conferences,the AI Alignment Newsletter,Superintelligence (book),AGI Safety Fundamentals Course,The Alignment Problem (book),Concrete Problems in AI Safety by Amodei et al,Human Compatible (book),talks by AI alignment researchers,the FLI podcast,the ARCHES agenda by Andrew Critch and David Krueger,the 80,000 Hours podcast,Rob Miles videos,the ML Safety newsletter,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Unsolved Problems in ML Safety by Hendrycks et al",2 | 2 | 3 | 3 | 3 | 3 | 3 | 2 | 2 | 3 | 2 | 2 | 2 | 1 | 3 | 2 | 2 | 3 | 3,Unsolved Problems in ML Safety by Hendrycks et al,0 | 0 | 3 | 4 | 2 | 4 | 2 | 4 | 1 | 4 | 4 | 2 | 1 | 1 | 3 | 4 | 3 | 3 | 4,1 | 1 | 4 | 4 | 3 | 4 | 3 | 2 | 2 | 3 | 2 | 3 | 1 | 2 | 3 | 2 | 3 | 3 | 3," |  |  |  |  |  |  |  |  |  | Disregard my answer (I've only listened to half an episode) |  | The first episode they had with Christiano was especially good. |  |  |  | Seems great for 1) people who already understand ML (though that's not necessary) but not AI safety 2) people who look for concrete problems get started on (there it seems slightly better than the Amodei agenda). 

The introduction section is a great motivation of safety. Especially in a way that's very 'respectable/serious' but doesn't neglect future risks.", | Christiano's talk on the taxonomy of alignment/safety research was pretty good. Probably not the best for beginners though.,"If the goal is just to motivate why it's important to work on future-focused AI safety (rather than teach about alignment research) no resource exists that I'm remotely happy with. And I've looked at many over the years. That's a bit shocking to me. We haven't yet written a resource that's 1) not book length 2) respectable to academics (i.e. not a blog post, forum post, or news article; ideally peer reviewed)  3) comprehensive covers the most important motivations without getting too deep into philosophy and futurist-sounding speculation 4) well-written. Maybe you guys can help motivate people to create such resources. 

[REDACTED]. I also like Joe Carlsmith's recent Arxiv paper ""Is power-seeking AI an existential risk?"". It's pretty great aside from being long and dense.",Rohin's newsletter,,
7497462,44,,2022-07-05 13:04:15,2022-07-05 13:13:29,9.23,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos"",""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time publicly communicating about AI alignment""]",4,I spend some of my time publicly communicating about AI alignment,"the Embedded Agency sequence on the Alignment Forum,Rob Miles videos,the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,AGI Safety Fundamentals Course,the 80,000 Hours podcast,the AI Alignment Newsletter",3 | 4 | 3 | 3 | 2 | 1 | 2,the AI Alignment Newsletter,3 | 4 | 3 | 2 | 3 | 1 | 2,4 | 2 | 3 | 3 | 3 | 1 | 2, |  |  |  |  | ,"","",AI Alignment Newsletter,,
7497500,44,,2022-07-05 13:29:01,,0.07,u166gt1,0,,,,,,"",[],,,,,,,,,,,,,
7497537,44,,2022-07-05 13:46:42,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497623,44,,2022-07-05 14:28:36,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497630,44,,2022-07-05 14:31:20,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497637,44,,2022-07-05 14:33:12,2022-07-05 14:41:27,8.25,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",2,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"",,,,,,,"","",,
7497651,44,,2022-07-05 14:43:33,,0.33,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7497679,44,,2022-07-05 14:55:26,,0.47,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7497716,44,,2022-07-05 15:08:52,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497757,44,,2022-07-05 15:29:23,2022-07-05 15:35:45,6.35,t6togir,0,,,"[""Concrete Problems in AI Safety by Amodei et al"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""the FLI podcast"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""The Alignment Problem (book)"",""Superintelligence (book)"",""talks by AI alignment researchers"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I am paid to work on technical AI alignment research""]",3,I am paid to work on technical AI alignment research,"Concrete Problems in AI Safety by Amodei et al,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),the FLI podcast,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,The Alignment Problem (book),Superintelligence (book),talks by AI alignment researchers,the 80,000 Hours podcast",2 | 4 | 2 | 2 | 2 | 3 | 4 | 1 | 4,"the 80,000 Hours podcast",2 | 4 | 1 | 1 | 0 | 4 | 2 | 0 | 4,2 | 4 | 1 | 1 | 2 | 2 | 3 | 1 | 4, |  |  |  |  |  |  | This podcast is the reason I currently work on alignment.,"","",Rohin's AI alignment newsletter,0,
7497768,44,,2022-07-05 15:37:50,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7497839,44,,2022-07-05 16:14:17,,0.73,u166gt1,0,,,,,,"I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",,,,,,,,,,,,,
7497868,44,,2022-07-05 16:28:28,2022-07-05 16:38:06,9.63,t6togir,0,,,"[""Unsolved Problems in ML Safety by Hendrycks et al"",""the Iterated Amplification sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""the FLI podcast"",""the AI Alignment Newsletter"",""AXRP - the AI X-risk Research Podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Rob Miles videos"",""the 80,000 Hours podcast"",""AGI Safety Fundamentals Course"",""conversations with AI alignment researchers at conferences"",""Human Compatible (book)"",""Superintelligence (book)"",""the ML Safety newsletter"",""The Alignment Problem (book)"",""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""Life 3.0 (book)"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","Unsolved Problems in ML Safety by Hendrycks et al,the Iterated Amplification sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,the FLI podcast,the AI Alignment Newsletter,AXRP - the AI X-risk Research Podcast,the Embedded Agency sequence on the Alignment Forum,Rob Miles videos,the 80,000 Hours podcast,AGI Safety Fundamentals Course,conversations with AI alignment researchers at conferences,Human Compatible (book),Superintelligence (book),the ML Safety newsletter,The Alignment Problem (book),the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,Life 3.0 (book),Concrete Problems in AI Safety by Amodei et al",2 | 2 | 3 | 1 | 3 | 2 | 2 | 2 | 2 | 2 | 1 | 4 | 2 | 4 | 2 | 1 | 2 | 3 | 2 | 2,Concrete Problems in AI Safety by Amodei et al,2 | 1 | 0 | 3 | 2 | 1 | 3 | 3 | 4 | 4 | 3 | 3 | 3 | 3 | 2 | 2 | 2 | 3 | 3 | 2,2 | 3 | 3 | 1 | 3 | 3 | 3 | 3 | 3 | 1 | 2 | 4 | 2 | 4 | 3 | 1 | 2 | 4 | 1 | 3, |  |  |  |  |  |  | I knew all of it already but it's a very neat sequence about intuitions on embedded cognition for people who haven't heard of it. |  |  |  |  |  |  |  |  |  | , | ,I would show them https://aisafetyideas.com for them to look through.,Alignment newsletter,0.5,
7497888,44,,2022-07-05 16:40:02,,0.80,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7498032,44,,2022-07-05 17:50:44,2022-07-05 17:57:29,6.75,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""the 80,000 Hours podcast"",""Superintelligence (book)"",""talks by AI alignment researchers""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research""]",3,I am interested in AI alignment research,"conversations with AI alignment researchers at conferences,the 80,000 Hours podcast,Superintelligence (book),talks by AI alignment researchers",4 | 2 | 3 | 3,talks by AI alignment researchers,4 | 2 | 3 | 3,4 | 2 | 4 | 2,Depends on how well the format of podcasts works for people and how much time they have | ,"Marius Hobbhan, people I found on swapcard during EAGs | ",Thanks for doing this :),AI Alignment Newsletter,,
7498125,44,,2022-07-05 18:38:53,,2.30,u166gt1,0,,,,,,I have heard of AI alignment,"[""I have heard of AI alignment""]",,,,,,,,,,,,,
7498130,44,,2022-07-05 18:44:59,2022-07-05 18:48:31,3.52,t6togir,0,,,"[""the ML Safety newsletter"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""Unsolved Problems in ML Safety by Hendrycks et al"",""The Alignment Problem (book)"",""the AI Alignment Newsletter"",""Life 3.0 (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""the 80,000 Hours podcast"",""the Embedded Agency sequence on the Alignment Forum"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""Rob Miles videos"",""the Iterated Amplification sequence on the Alignment Forum"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the ML Safety newsletter,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,talks by AI alignment researchers,Unsolved Problems in ML Safety by Hendrycks et al,The Alignment Problem (book),the AI Alignment Newsletter,Life 3.0 (book),the ARCHES agenda by Andrew Critch and David Krueger,the 80,000 Hours podcast,the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,Rob Miles videos,the Iterated Amplification sequence on the Alignment Forum,Human Compatible (book)",3 | 4 | 2 | 2 | 2 | 2 | 4 | 3 | 3 | 1 | 4 | 4 | 3 | 1 | 2 | 2,Human Compatible (book),4 | 4 | 4 | 3 | 1 | 3 | 4 | 3 | 2 | 2 | 2 | 4 | 4 | 2 | 2 | 2,4 | 4 | 0 | 3 | 1 | 1 | 4 | 1 | 2 | 1 | 3 | 4 | 4 | 2 | 2 | 2, |  |  |  |  |  |  |  |  |  |  |  |  | , | ,"",AIS Newsletter,2,
7498137,44,,2022-07-05 18:48:12,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7498139,44,,2022-07-05 18:49:44,,1.00,8d2bvx6,0,,,"[""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Human Compatible (book)"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""The Alignment Problem (book)"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time solving technical problems related to AI alignment,"[""I spend some of my time solving technical problems related to AI alignment""]",,I spend some of my time solving technical problems related to AI alignment,"the AI Alignment Newsletter,the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al,Human Compatible (book),talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,The Alignment Problem (book),Superintelligence (book)",,,,,,,,,,
7498142,44,,2022-07-05 18:50:54,,0.13,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7498276,44,,2022-07-05 19:48:54,,40.97,72erk1d,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","",[],,,"",,,,,,,,,,
7498311,44,,2022-07-05 20:04:44,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7498312,44,,2022-07-05 20:05:04,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7498316,44,,2022-07-05 20:07:52,,5574.60,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7498404,44,,2022-07-05 20:54:53,2022-07-05 20:57:15,2.37,t6togir,0,,,"[""the 80,000 Hours podcast"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",2,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","the 80,000 Hours podcast,Rob Miles videos",2 | 3,Rob Miles videos,1 | 3,2 | 1, | ,,"","",,
7498418,44,,2022-07-05 21:03:25,,27.53,now6hjo,0,,,"[""AXRP - the AI X-risk Research Podcast"",""Human Compatible (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)"",""AGI Safety Fundamentals Course"",""the FLI podcast"",""the Value Learning sequence on the Alignment Forum"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum"",""the ARCHES agenda by Andrew Critch and David Krueger"",""Concrete Problems in AI Safety by Amodei et al"",""the AI Alignment Newsletter"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Rob Miles videos"",""conversations with AI alignment researchers at conferences"",""the 80,000 Hours podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building""]",2,I spend some of my time doing AI alignment field/community-building,"AXRP - the AI X-risk Research Podcast,Human Compatible (book),the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book),AGI Safety Fundamentals Course,the FLI podcast,the Value Learning sequence on the Alignment Forum,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,the ARCHES agenda by Andrew Critch and David Krueger,Concrete Problems in AI Safety by Amodei et al,the AI Alignment Newsletter,Unsolved Problems in ML Safety by Hendrycks et al,Rob Miles videos,conversations with AI alignment researchers at conferences,the 80,000 Hours podcast",3 | 3 | 2 | 2 | 3 | 1 | 3 | 2 | 3 | 2 | 3 | 2 | 1,Unsolved Problems in ML Safety by Hendrycks et al,2 | 4 | 1 | 2 | 4 | 2 | 3 | 3 | 2 | 1 | 3 | 2 | 0,4 | 2 | 3 | 4 | 4 | 0 | 4 | 2 | 4 | 3 | 4 | 2 | 0,"How good the episode is as an introduction very much depends on the episode. I would not want one to first listen to the episode of Scott Garrabrant explaining Finite Factored Sets since it is not terribly clear how this is relevant to the whole AI stuff. On the other hand, the episode with Katja Grace would be excellent as an introduction, as it goes into why and how one would expect AGI in the first place. | It's the first book I read about the area. His arguments are sound and less dense/(more casually readable) than superintelligence and also by an established researcher, thus making clearer that this isn't just some speculation by some philosophers. |  | As I already said pretty dense. It is also helpful for people like me who only joined when deep learning already was a thing to see what people thought before that time. | I went through this resource with 3 friends who had not yet read that much about alignment. It's really helpful to get started because you get some rough perspective about what kinds of research are being done (It can be kinda hard to get an overview as different source have quite different material (archive vs. alignment forum). | Only intro material. | Pretty important stuff that I think at least daily about (should probably reread it?) | I think some of the stuff on logic might confuse/(lead astray)/(turn away) people who aren't as motivated by pure curiosity about how reasoning/agency works. There is a certain kind of nerdy person (me) who also likes to get nerd snipped a bit and who is very much motivated by questions in the sequence. |  |  |  | I was very confused, who the audience for this paper was? Maybe I am confused and the contribution is differentiating between robustness, alignment, and other kinds of safety, but all in all, it felt more like a paper that tries to get prestige by connecting with mainstream safety concepts, like the chees model thing. It felt less like a paper that I learned something from.",I take this as watching talks on youtube?,,,,
7498658,44,,2022-07-05 23:19:29,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7498689,44,,2022-07-05 23:41:51,,0.27,8d2bvx6,0,,,"[""AGI Safety Fundamentals Course"",""AIRCS workshops""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"[""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,"AGI Safety Fundamentals Course,AIRCS workshops",,,,,,,,,,
7499012,44,,2022-07-06 03:03:45,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7499558,44,,2022-07-06 09:31:20,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7500002,44,,2022-07-06 14:08:48,,0.08,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7500013,44,,2022-07-06 14:15:13,2022-07-06 14:18:29,3.25,t6togir,0,,,"[""AGI Safety Fundamentals Course"",""Human Compatible (book)"",""the AI Alignment Newsletter"",""the 80,000 Hours podcast"",""the ML Safety newsletter"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am trying to move into a technical AI alignment career,"[""I am trying to move into a technical AI alignment career""]",,I am trying to move into a technical AI alignment career,"AGI Safety Fundamentals Course,Human Compatible (book),the AI Alignment Newsletter,the 80,000 Hours podcast,the ML Safety newsletter,Concrete Problems in AI Safety by Amodei et al",3 | 3 | 3 | 4 | 3 | 3,Concrete Problems in AI Safety by Amodei et al,4 | 4 | 3 | 4 | 3 | 4,4 | 1 | 4 | 4 | 3 | 2, |  |  |  |  | ,,"",AGI Safety Fundamentals Slack,,
7500064,44,,2022-07-06 14:37:57,,0.03,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7500088,44,,2022-07-06 14:51:46,,2.45,8d2bvx6,0,,,"[""conversations with AI alignment researchers at conferences"",""the 80,000 Hours podcast"",""The Alignment Problem (book)"",""the FLI podcast"",""AXRP - the AI X-risk Research Podcast"",""the AI Alignment Newsletter"",""Human Compatible (book)"",""Superintelligence (book)"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Rob Miles videos"",""AGI Safety Fundamentals Course"",""talks by AI alignment researchers"",""the Embedded Agency sequence on the Alignment Forum"",""the annual AI Alignment Literature Review and Charity Comparison"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment,I am paid to work on technical AI alignment research","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment"",""I am paid to work on technical AI alignment research""]",5,I am paid to work on technical AI alignment research,"conversations with AI alignment researchers at conferences,the 80,000 Hours podcast,The Alignment Problem (book),the FLI podcast,AXRP - the AI X-risk Research Podcast,the AI Alignment Newsletter,Human Compatible (book),Superintelligence (book),Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Rob Miles videos,AGI Safety Fundamentals Course,talks by AI alignment researchers,the Embedded Agency sequence on the Alignment Forum,the annual AI Alignment Literature Review and Charity Comparison,Concrete Problems in AI Safety by Amodei et al",,,,,,,,,1,
7500182,44,,2022-07-06 15:43:38,2022-07-06 15:48:20,4.70,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""The Alignment Problem (book)"",""AXRP - the AI X-risk Research Podcast"",""Rob Miles videos"",""the Value Learning sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""talks by AI alignment researchers"",""AGI Safety Fundamentals Course"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""Human Compatible (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am trying to move into a technical AI alignment career,I am paid to work on technical AI alignment research","[""I am trying to move into a technical AI alignment career"",""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"the Embedded Agency sequence on the Alignment Forum,The Alignment Problem (book),AXRP - the AI X-risk Research Podcast,Rob Miles videos,the Value Learning sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,talks by AI alignment researchers,AGI Safety Fundamentals Course,Superintelligence (book),the 80,000 Hours podcast,Human Compatible (book)",4 | 3 | 4 | 4 | 2 | 3 | 2 | 4 | 3 | 4 | 3,Human Compatible (book),2 | 4 | 2 | 4 | 1 | 3 | 0 | 4 | 0 | 4 | 2,4 | 2 | 3 | 3 | 2 | 4 | 2 | 4 | 3 | 4 | 3, |  |  |  |  |  |  |  | , | ,"",AI Safety Fundamentals Slack,0,
7500227,44,,2022-07-06 16:09:08,2022-07-06 16:32:51,23.70,t6togir,0,,,"[""the Embedded Agency sequence on the Alignment Forum"",""talks by AI alignment researchers"",""the FLI podcast"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""Unsolved Problems in ML Safety by Hendrycks et al"",""AGI Safety Fundamentals Course"",""AXRP - the AI X-risk Research Podcast"",""The Alignment Problem (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""the 80,000 Hours podcast"",""Human Compatible (book)"",""Life 3.0 (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",3,I am trying to move into a technical AI alignment career,"the Embedded Agency sequence on the Alignment Forum,talks by AI alignment researchers,the FLI podcast,Superintelligence (book),the AI Alignment Newsletter,Unsolved Problems in ML Safety by Hendrycks et al,AGI Safety Fundamentals Course,AXRP - the AI X-risk Research Podcast,The Alignment Problem (book),the Iterated Amplification sequence on the Alignment Forum,the 80,000 Hours podcast,Human Compatible (book),Life 3.0 (book)",3 | 4 | 3 | 4 | 1 | 2 | 2 | 4 | 4 | 3 | 4 | 2 | 1 | 4 | 4,Life 3.0 (book),1 | 0 | 1 | 4 | 0 | 0 | 0 | 4 | 4 | 1 | 4 | 0 | 1 | 4 | 1,0 | 4 | 4 | 2 | 0 | 0 | 0 | 2 | 2 | 4 | 4 | 2 | 0 | 4 | 2," |  |  |  |  |  |  | Someone paid to do alignment research probably has a good handle on the fundamentals. |  |  | I think iterated amplification is a useful example of a research agenda that will probably dead-end due to not being fundamental enough to yield transferable insights. | So far the interviews haven't been technical enough for me, but they'd be good for someone who's new to the idea of an alignment and safety problem. Good high-level, general-audience stuff. |  | This book might be too imaginative and speculative about the deep future for many people, but it all seemed reasonable to me, and was very inspiring while also teaching some technical details about computation and other basics.","","",Jamie Bernardi shared it in the AGISF Slack channel.,,
7500288,44,,2022-07-06 16:42:04,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7500707,44,,2022-07-06 20:01:04,2022-07-06 20:23:06,22.03,t6togir,0,,,"[""the annual AI Alignment Literature Review and Charity Comparison"",""AI Safety Camp"",""talks by AI alignment researchers"",""the AI Alignment Newsletter"",""Rob Miles videos"",""Human Compatible (book)"",""the 80,000 Hours podcast"",""The Alignment Problem (book)"",""Superintelligence (book)"",""Concrete Problems in AI Safety by Amodei et al"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building""]",4,I spend some of my time doing AI alignment field/community-building,"AI Safety Camp,the 80,000 Hours podcast,the FLI podcast,the Embedded Agency sequence on the Alignment Forum,the AI Alignment Newsletter,Rob Miles videos,The Alignment Problem (book),the annual AI Alignment Literature Review and Charity Comparison,Superintelligence (book),conversations with AI alignment researchers at conferences,talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,Human Compatible (book) | the annual AI Alignment Literature Review and Charity Comparison,AI Safety Camp,talks by AI alignment researchers,the AI Alignment Newsletter,Rob Miles videos,Human Compatible (book),the 80,000 Hours podcast,The Alignment Problem (book),Superintelligence (book),Concrete Problems in AI Safety by Amodei et al,conversations with AI alignment researchers at conferences",4 | 2 | 4 | 2 | 2 | 4 | 3 | 2 | 2 | 4 | 3 | 1 | 2,conversations with AI alignment researchers at conferences,0 | 1 | 0 | 1 | 0 | 4 | 4 | 3 | 1 | 4 | 0 | 1 | 2,0 | 1 | 0 | 1 | 1 | 4 | 2 | 0 | 1 | 4 | 0 | 1 | 2,"I would very much recommend the AISC for people who have read a reasonable amount about AIS, but are not yet being paid to do alignment research. |  | I would very much recommend the AISC for people who have read a reasonable amount about AIS, but are not yet being paid to do alignment research. |  |  |  |  |  |  | I think the concepts from Superintelligence are useful, because they are used in discussions about AIS, but the book is out-dated, and it is also very hard to read, e.g. compared to The Alignment Problem. | ",Paul Christiano | ,"",Alignment Newsletter,,
7500818,44,,2022-07-06 20:55:08,,0.02,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7500984,44,,2022-07-06 22:18:17,2022-07-06 22:30:55,12.63,t6togir,0,,,"[""Superintelligence (book)"",""the AI Alignment Newsletter"",""the FLI podcast"",""the 80,000 Hours podcast"",""the Embedded Agency sequence on the Alignment Forum"",""AGI Safety Fundamentals Course"",""AI Safety Camp"",""The Alignment Problem (book)"",""talks by AI alignment researchers"",""Rob Miles videos""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time solving technical problems related to AI alignment,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time doing AI alignment field/community-building""]",1 | 1,I spend some of my time doing AI alignment field/community-building,"Superintelligence (book),the AI Alignment Newsletter,the FLI podcast,the 80,000 Hours podcast,the Embedded Agency sequence on the Alignment Forum,AGI Safety Fundamentals Course,AI Safety Camp,The Alignment Problem (book),talks by AI alignment researchers,Rob Miles videos",2 | 2 | 2 | 3 | 0 | 2 | 3 | 1 | 3 | 4 | 4 | 4,Rob Miles videos,1 | 0 | 2 | 2 | 0 | 3 | 1 | 4 | 2 | 4 | 4 | 4,1 | 4 | 1 | 3 | 0 | 0 | 1 | 0 | 2 | 1 | 1 | 1,"Outdated and a bit tedious. Neglects whether AI will be goal-directed with long-term goals |  | I'm thinking about the Evan Hubinger and Rohin Shah episodes, maybe also the Anthropic episode | Just the AI episodes. Paul Christiano is a bit hard to follow for new people but a gold mine. | Didn't read much |  | I'd recommending it to people with moderate ~6 mo of interest. It was good for making connections. | Great writing, not that useful. | He's great at making the problem apparent. He neglects whether AI will have long-term goals. | He's great at making the problem apparent. He neglects discussion of whether AI will have long-term goals. | He's great at making the problem apparent and he's entertaining. He neglects discussion of whether AI will have long-term goals.","","",AGISFC slack,,
7501411,44,,2022-07-07 03:07:33,2022-07-07 03:09:36,2.05,t6togir,0,,,"[""the AI Alignment Newsletter"",""The Alignment Problem (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"the AI Alignment Newsletter,The Alignment Problem (book),the Iterated Amplification sequence on the Alignment Forum,conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course",3 | 4 | 2 | 4 | 4,AGI Safety Fundamentals Course,1 | 4 | 0 | 0 | 4,4 | 3 | 2 | 4 | 0, |  |  | ,"","",AN,,
7502093,44,,2022-07-07 12:17:10,,24.87,now6hjo,0,,,"[""The Alignment Problem (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""Human Compatible (book)"",""AGI Safety Fundamentals Course"",""the 80,000 Hours podcast"",""the FLI podcast"",""the Value Learning sequence on the Alignment Forum"",""the Embedded Agency sequence on the Alignment Forum"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""talks by AI alignment researchers"",""Concrete Problems in AI Safety by Amodei et al"",""AXRP - the AI X-risk Research Podcast""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"The Alignment Problem (book),the Iterated Amplification sequence on the Alignment Forum,Human Compatible (book),AGI Safety Fundamentals Course,the 80,000 Hours podcast,the FLI podcast,the Value Learning sequence on the Alignment Forum,the Embedded Agency sequence on the Alignment Forum,Superintelligence (book),the AI Alignment Newsletter,talks by AI alignment researchers,Concrete Problems in AI Safety by Amodei et al,AXRP - the AI X-risk Research Podcast",2 | 1 | 2 | 3 | 2 | 2,the FLI podcast,4 | 0 | 2 | 4 | 1 | 1,1 | 1 | 1 | 2 | 1 | 1," | I don't think of it as a high-quality resource - i.e. I don't think it's a well-written introduction to Christiano's ideas, despite the idea having been influential. |  | It's sort of the only real course that does what it does and that anyone can try to get into (i.e. it's not part of a university program or something), so I'd pretty much always recommend it to someone who's coming from the outside. | I've engaged with it but not as a resource to learn about AI Alignment; I wouldn't recommend it for that kind of reason. | ditto",,,,1,
7502255,44,,2022-07-07 13:49:07,2022-07-07 13:53:40,4.55,t6togir,0,,,"[""The Alignment Problem (book)"",""Human Compatible (book)"",""Superintelligence (book)"",""the 80,000 Hours podcast"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I spend some of my time doing AI alignment field/community-building","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I spend some of my time doing AI alignment field/community-building""]",1,I spend some of my time doing AI alignment field/community-building,"Human Compatible (book),The Alignment Problem (book),Superintelligence (book),the 80,000 Hours podcast | The Alignment Problem (book),Human Compatible (book),Superintelligence (book),the 80,000 Hours podcast,the AI Alignment Newsletter",3 | 2 | 3 | 3 | 3 | 3 | 1 | 2 | 3 | 3 | 3 | 1,the AI Alignment Newsletter,4 | 3 | 4 | 2 | 4 | 4 | 3 | 3 | 4 | 2 | 4 | 3,3 | 1 | 1 | 1 | 1 | 3 | 4 | 1 | 1 | 1 | 3 | 4, |  |  |  |  |  |  |  |  |  |  | ,,"","Jamie Bernardi, AGI Safety Fundamentals Slack group.",,
7502290,44,,2022-07-07 14:06:39,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7502545,44,,2022-07-07 15:13:40,2022-07-07 15:37:55,24.23,t6togir,0,,,"[""the ARCHES agenda by Andrew Critch and David Krueger"",""The Alignment Problem (book)"",""Life 3.0 (book)"",""Superintelligence (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""Concrete Problems in AI Safety by Amodei et al"",""Human Compatible (book)"",""Unsolved Problems in ML Safety by Hendrycks et al"",""the AI Alignment Newsletter"",""AI Safety Camp"",""the Embedded Agency sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I spend some of my time solving technical problems related to AI alignment,I spend some of my time publicly communicating about AI alignment","[""I spend some of my time solving technical problems related to AI alignment"",""I spend some of my time publicly communicating about AI alignment""]",,I spend some of my time publicly communicating about AI alignment,"the ARCHES agenda by Andrew Critch and David Krueger,The Alignment Problem (book),Life 3.0 (book),Superintelligence (book),the annual AI Alignment Literature Review and Charity Comparison,Concrete Problems in AI Safety by Amodei et al,Human Compatible (book),Unsolved Problems in ML Safety by Hendrycks et al,the AI Alignment Newsletter,AI Safety Camp,the Embedded Agency sequence on the Alignment Forum",2 | 3 | 1 | 3 | 2 | 3 | 3 | 2 | 1 | 2 | 0,the Embedded Agency sequence on the Alignment Forum,3 | 4 | 1 | 2 | 1 | 3 | 2 | 2 | 1 | 3 | 0,3 | 2 | 1 | 2 | 2 | 4 | 2 | 2 | 1 | 1 | 1," |  |  |  |  | I am recommending this because it shows a more mainstream AI researcher view compared to other sources |  | Found that it does not add a lot compared to earlier works | I feel that it covers a somewhat limited part of the alignment space only | Gave some expert feedback to project proposals being considered in the camp, never participated. | I feel that the sequence has done much more harm than good in driving people's research agendas, and in introducing newcomers to the field.",,"There used to be someone maintaining a conference agenda, of conferences relevant to AI alignment.  This stopped being maintained, but I found it extremely useful.     Much of the material you asked me to rank is almost exclusively  material that explains the alignment problem, and other problems with AI, not material that explains any solutions, or solution methods, or criteria for determining how good certain solutions are.. I think the field needs much more solutions-type material.    The book the alignment problem presents some good solutions material for short-term AI alignment problems in the first sections, but then fails to provide solutions material in the section on long-term alignment, reverting instead to the default line that 'more research is needed'.  The field by now has more than enough books and media projects that inform the public and researchers about potential problems -- the field needs to progress to a phase where conversations about solutions become the expected norm, both among practitioners and between practitioners and the public.",alignment newsletter,,
7502597,44,,2022-07-07 15:30:45,,0.13,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7502601,44,,2022-07-07 15:31:26,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7503205,44,,2022-07-07 20:51:27,2022-07-07 21:07:39,16.18,t6togir,0,,,"[""the annual AI Alignment Literature Review and Charity Comparison"",""The Alignment Problem (book)"",""the Iterated Amplification sequence on the Alignment Forum"",""the AI Alignment Newsletter"",""Human Compatible (book)"",""AXRP - the AI X-risk Research Podcast"",""AGI Safety Fundamentals Course"",""Scalable agent alignment via reward modeling: a research direction by Leike et al (aka \""the recursive reward modelling agenda\"")"",""Superintelligence (book)"",""the ARCHES agenda by Andrew Critch and David Krueger"",""talks by AI alignment researchers"",""the FLI podcast"",""the 80,000 Hours podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Rob Miles videos"",""the Embedded Agency sequence on the Alignment Forum"",""the Value Learning sequence on the Alignment Forum""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time publicly communicating about AI alignment""]",1,I spend some of my time publicly communicating about AI alignment,"the annual AI Alignment Literature Review and Charity Comparison,The Alignment Problem (book),the Iterated Amplification sequence on the Alignment Forum,the AI Alignment Newsletter,Human Compatible (book),AXRP - the AI X-risk Research Podcast,AGI Safety Fundamentals Course,Scalable agent alignment via reward modeling: a research direction by Leike et al (aka ""the recursive reward modelling agenda""),Superintelligence (book),the ARCHES agenda by Andrew Critch and David Krueger,talks by AI alignment researchers,the FLI podcast,the 80,000 Hours podcast,Unsolved Problems in ML Safety by Hendrycks et al,Rob Miles videos,the Embedded Agency sequence on the Alignment Forum,the Value Learning sequence on the Alignment Forum",2 | 3 | 2 | 4 | 3 | 4 | 4 | 2 | 4 | 3 | 3 | 3 | 4 | 2 | 3 | 3 | 2,the Value Learning sequence on the Alignment Forum,1 | 4 | 0 | 3 | 4 | 1 | 4 | 0 | 3 | 0 | 0 | 3 | 4 | 2 | 4 | 1 | 0,2 | 2 | 3 | 4 | 3 | 4 | 2 | 3 | 3 | 3 | 3 | 2 | 2 | 3 | 2 | 4 | 3," | It’s historically contextualised  which might be helpful for outsiders but less so for people familiar with the space. I like the context personally. I like that Christian emphasised equally the bias/privacy & alignment/XR aspects of AI safety. |  | It’s the best source to get distilled views from knowledgeable writers on AI research papers. | It’s very aligned with Russell’s research direction but has a lovely philosophical / big picture perspective I find useful. His footnotes are great (as are Christian’s & Bostrom’s ) | Great, detailed deep dives into specific questions. | It’s a great programme but I suspect hinges on the quality if the instructor.  Mine was brilliant. |  | Despite its shortcomings (ignoring deep learning) & age, I find Bostrom’s writing engaging & the book has a great breadth & depth. It also spans alignment & governance which should be useful to many people in the space. | A usefully fresh view on how ‘alignment’ is only part of the problem. |  |  |  |  |  | ","","",Slack for EA AGISF course,,
7503880,44,,2022-07-08 04:18:02,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7504139,44,,2022-07-08 07:58:49,,15.20,e5pk481,0,,,,,,"I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly""]",5,I am interested in AI alignment research,,,,,,,,,,,
7504382,44,,2022-07-08 10:52:03,2022-07-08 10:54:03,2.00,t6togir,0,,,[],"[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"",,,,,,,"","",,
7504691,44,,2022-07-08 14:15:23,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7504827,44,,2022-07-08 15:13:50,2022-07-08 15:18:15,4.40,t6togir,0,,,"[""the FLI podcast"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences"",""the 80,000 Hours podcast"",""AGI Safety Fundamentals Course"",""The Alignment Problem (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",4,I am trying to move into a technical AI alignment career,"the FLI podcast,Superintelligence (book),the AI Alignment Newsletter,conversations with AI alignment researchers at conferences,the 80,000 Hours podcast,AGI Safety Fundamentals Course,The Alignment Problem (book)",2 | 2 | 2 | 2 | 3 | 2 | 2,The Alignment Problem (book),3 | 1 | 0 | 3 | 3 | 3 | 3,2 | 2 | 4 | 4 | 2 | 1 | 2, |  |  |  |  | ,"","",AGI Alignment course Slack,,
7505355,44,,2022-07-08 19:54:38,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7505852,44,,2022-07-09 02:14:55,,545.12,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7506480,44,,2022-07-09 11:41:48,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7506584,44,,2022-07-09 13:14:28,,2.40,8d2bvx6,0,,,"[""Superintelligence (book)"",""Life 3.0 (book)"",""the FLI podcast"",""Unsolved Problems in ML Safety by Hendrycks et al"",""Rob Miles videos"",""AXRP - the AI X-risk Research Podcast"",""talks by AI alignment researchers"",""The Alignment Problem (book)"",""Human Compatible (book)"",""the annual AI Alignment Literature Review and Charity Comparison"",""the AI Alignment Newsletter"",""Concrete Problems in AI Safety by Amodei et al""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","Superintelligence (book),Life 3.0 (book),the FLI podcast,Unsolved Problems in ML Safety by Hendrycks et al,Rob Miles videos,AXRP - the AI X-risk Research Podcast,talks by AI alignment researchers,The Alignment Problem (book),Human Compatible (book),the annual AI Alignment Literature Review and Charity Comparison,the AI Alignment Newsletter,Concrete Problems in AI Safety by Amodei et al",,,,,,,,,,
7507802,44,,2022-07-10 04:40:13,2022-07-10 04:43:12,2.98,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""Concrete Problems in AI Safety by Amodei et al"",""talks by AI alignment researchers"",""AIRCS workshops"",""the annual AI Alignment Literature Review and Charity Comparison"",""the Embedded Agency sequence on the Alignment Forum"",""the Iterated Amplification sequence on the Alignment Forum"",""Superintelligence (book)"",""the AI Alignment Newsletter"",""conversations with AI alignment researchers at conferences""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am paid to work on technical AI alignment research,"[""I am paid to work on technical AI alignment research""]",,I am paid to work on technical AI alignment research,"AXRP - the AI X-risk Research Podcast,Concrete Problems in AI Safety by Amodei et al,talks by AI alignment researchers,AIRCS workshops,the annual AI Alignment Literature Review and Charity Comparison,the Embedded Agency sequence on the Alignment Forum,the Iterated Amplification sequence on the Alignment Forum,Superintelligence (book),the AI Alignment Newsletter,conversations with AI alignment researchers at conferences",4 | 2 | 2 | 3 | 4 | 1 | 2 | 0 | 2 | 4,conversations with AI alignment researchers at conferences,4 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0,3 | 2 | 2 | 0 | 1 | 1 | 1 | 0 | 1 | 3, |  |  |  |  |  |  | , | ,"","",3,
7508661,44,,2022-07-10 17:22:57,,0.07,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7509775,44,,2022-07-11 07:32:04,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7511861,44,,2022-07-12 06:09:32,,0.05,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7512181,44,,2022-07-12 10:51:42,,3200.62,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7512654,44,,2022-07-12 15:45:04,,1.97,8d2bvx6,0,,,"[""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]",I am interested in AI alignment research,"[""I am interested in AI alignment research""]",4,I am interested in AI alignment research,Superintelligence (book),,,,,,,,,,
7513979,44,,2022-07-13 02:24:42,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7513989,44,,2022-07-13 02:35:32,2022-07-13 02:41:47,6.23,t6togir,0,,,"[""talks by AI alignment researchers"",""Superintelligence (book)"",""AI Safety Camp"",""Life 3.0 (book)"",""the AI Alignment Newsletter"",""Human Compatible (book)"",""Concrete Problems in AI Safety by Amodei et al"",""the ARCHES agenda by Andrew Critch and David Krueger""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","[""I have heard of AI alignment"",""I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)""]",,"I help run an organization with an AI alignment mission (e.g. CHAI, MIRI, Anthropic)","talks by AI alignment researchers,Superintelligence (book),AI Safety Camp,Life 3.0 (book),the AI Alignment Newsletter,Human Compatible (book),Concrete Problems in AI Safety by Amodei et al,the ARCHES agenda by Andrew Critch and David Krueger",4 | 4 | 3 | 4 | 4 | 4 | 4 | 4 | 4,the ARCHES agenda by Andrew Critch and David Krueger,2 | 2 | 3 | 4 | 4 | 4 | 4 | 4 | 4,3 | 4 | 3 | 4 | 4 | 4 | 4 | 4 | 4,I will talk with someone deeply to find the suitable part of superintelligence for her/him |  |  |  | [REDACTED] | [REDACTED] |  | ,Tom Everitt,"",AI Alignment Newsletter,,
7515471,44,,2022-07-13 18:00:44,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7515699,44,,2022-07-13 19:42:10,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7517221,44,,2022-07-14 09:00:02,2022-07-14 09:03:19,3.27,t6togir,0,,,"[""conversations with AI alignment researchers at conferences"",""AGI Safety Fundamentals Course"",""the Machine Learning for Alignment Bootcamp"",""Superintelligence (book)"",""The Alignment Problem (book)"",""the AI Alignment Newsletter""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I have heard of AI alignment,I am interested in AI alignment research,I am trying to move into a technical AI alignment career,I spend some of my time doing AI alignment field/community-building,I spend some of my time facilitating technical AI alignment research in ways other than doing it directly,I spend some of my time publicly communicating about AI alignment","[""I have heard of AI alignment"",""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career"",""I spend some of my time doing AI alignment field/community-building"",""I spend some of my time facilitating technical AI alignment research in ways other than doing it directly"",""I spend some of my time publicly communicating about AI alignment""]",3,I spend some of my time publicly communicating about AI alignment,"conversations with AI alignment researchers at conferences,AGI Safety Fundamentals Course,the Machine Learning for Alignment Bootcamp,Superintelligence (book),The Alignment Problem (book),the AI Alignment Newsletter",2 | 3 | 4 | 3 | 3 | 2,the AI Alignment Newsletter,0 | 4 | 4 | 2 | 4 | 2,4 | 1 | 4 | 3 | 2 | 3, | Most significant boost to my alignment skills and interest |  |  | ,"","",AI alignment newsletter,,
7519157,44,,2022-07-14 20:35:16,2022-07-14 20:43:47,8.50,t6togir,0,,,"[""AXRP - the AI X-risk Research Podcast"",""AGI Safety Fundamentals Course"",""Concrete Problems in AI Safety by Amodei et al"",""Human Compatible (book)"",""Superintelligence (book)""]","[[""Not at all"",0],[""A little"",1],[""Moderately"",2],[""Very"",3],[""Extremely"",4]]","[[""0-20%"",0],[""20-40%"",1],[""40-60%"",2],[""60-80%"",3],[""80-100%"",4]]","I am interested in AI alignment research,I am trying to move into a technical AI alignment career","[""I am interested in AI alignment research"",""I am trying to move into a technical AI alignment career""]",1,I am trying to move into a technical AI alignment career,"AXRP - the AI X-risk Research Podcast,AGI Safety Fundamentals Course,Concrete Problems in AI Safety by Amodei et al,Human Compatible (book),Superintelligence (book)",1 | 4 | 3 | 2 | 3,Superintelligence (book),1 | 4 | 2 | 1 | 2,0 | 3 | 4 | 0 | 0,"I only listened to a couple of episodes and the main benefit is it gives a platform for alignment researchers to speak their mind. This is cool, but there are probably more information rich and structured ways to learn about ai alignment. Also, the X-risk perspective sometimes gets in the way of interesting scientific discussion imo. | Great course so far, and is actually designed to help people learn and spin up. Like that it helps consolidate a ""canon"" of alignment literature, which is useful to make progress in the field, although of course this has potential negative consequences as well. | I would expect an alignment researcher to have read this article, but in general I think anyone who is prepared enough should certainly read it. Attempts to make comprehensive lists are very useful, good consolidators of what we already know. |  | Good and entertaining introduction to the longtermist motivations for ai safety work. Certain beginning chapters regarding enhanced biological cognition are excellent as well.",,"","",,
7522313,44,,2022-07-16 04:48:05,,153.67,4t3j1q5,0,,,,,,,,,,,,,,,,,,,,
7522405,44,,2022-07-16 06:18:33,,0.00,rnjq0pm,0,,,,,,,,,,,,,,,,,,,,
7522849,44,,2022-07-16 11:38:20,,0.08,u166gt1,0,,,,,,"",[],,,,,,,,,,,,,
